<!DOCTYPE html>
<html lang="en">
<head>
        <title>Alex Minnaar's Blog - Machine Learning, Data Science and Software Engineering - NLP</title>
        <meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <link rel="shortcut icon" href="http://launchyard.com/images/favicon.png"/>
        <link rel="stylesheet" href="/theme/css/main.css" type="text/css" />

        <!--[if IE]>
                <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

        <!--[if lte IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="/css/ie.css"/>
                <script src="/js/IE8.js" type="text/javascript"></script><![endif]-->

        <!--[if lt IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="/css/ie6.css"/><![endif]-->
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.8/jquery.min.js" type="text/javascript"></script>


</head>

<body id="index" class="home">
	
  <!--      <header id="banner" class="body">
                <h1><a href="/"><img src="http://www.launchyard.com/images/logo.png" /></a></h1>
        </header> -->
<!-- /#banner -->
	      <div class="LaunchyardDetail"><p><a href="/"></a>
<a class="title" href="http://alexminnaar.github.io/">Alex Minnaar</a>
<br/>
Machine Learning at University College London. Natural language processing at VerticalScope.
<br/>
<br/>
<a href="mailto:minnaaralex@gmail.com">Email</a><br />
<a href="https://github.com/alexminnaar">Github</a><br/>
<a href="https://ca.linkedin.com/pub/alex-minnaar/56/a23/853">LinkedIn</a><br/>
</p>


<div id="recent_posts">
			<h3>Categories</h3>
			<div align="left">
			<ul>
				<li><a href="/tag/nlp.html">NLP & Topic Models</a></li>
				<li><a href="/tag/supervised-learning.html">Supervised Learning</a></li>
				<li><a href="/tag/bayesian-inference.html">Bayesian Inference</a></li>
				<li><a href="/tag/kaggle-competitions.html">Kaggle Competitions</a></li>
				<li><a href="/tag/software-engineering.html">Software Engineering</a></li>
			</ul>
			</div>
              <h3>Recent Posts</h3>
                <a href="word2vec-tutorial-part-ii-the-continuous-bag-of-words-model.html">Word2Vec Tutorial Part II: The Continuous Bag-of-Words Model</a><br /><br />
                <a href="word2vec-tutorial-part-i-the-skip-gram-model.html">Word2Vec Tutorial Part I: The Skip-Gram Model</a><br /><br />
                <a href="distributed-online-latent-dirichlet-allocation-with-apache-spark.html">Distributed Online Latent Dirichlet Allocation with Apache Spark</a><br /><br />
                <a href="deep-learning-basics-neural-networks-backpropagation-and-stochastic-gradient-descent.html">Deep Learning Basics: Neural Networks, Backpropagation and Stochastic Gradient Descent</a><br /><br />
                <a href="building-a-shoutbox-app-with-cassandra-and-nodejs.html">Building a Shoutbox App with Cassandra and Node.js</a><br /><br />
                <a href="/building-a-distributed-binary-search-tree-with-akka.html">Building a Distributed Binary Search Tree with Akka</a><br /><br />
                <a href="/introduction-to-the-multithreading-problem-and-the-akka-actor-solution.html">Introduction to the Multithreading Problem and the Akka Actor Solution  </a><br /><br />
                <a href="/scalaner-a-scala-wrapper-for-the-stanford-ner-tool-with-some-added-features.html">ScalaNER: A Scala Wrapper for the Stanford NER Tool with Some Added Features  </a><br /><br />
                <a href="/online-latent-dirichlet-allocation-the-best-option-for-topic-modeling-with-large-data-sets.html">Online Latent Dirichlet Allocation - The Best Option for Topic Modeling with Large Data Sets  </a><br /><br />
                <a href="/latent-dirichlet-allocation-in-scala-part-ii-the-code.html">Latent Dirichlet Allocation in Scala Part II - The Code </a><br /><br />
          </div>

</div>

        
        

    
            <aside id="featured">
                <div class="body">
                    <article>
                        <h1 class="entry-title"><a href="/word2vec-tutorial-part-ii-the-continuous-bag-of-words-model.html">Word2Vec Tutorial Part II: The Continuous Bag-of-Words Model</a></h1>
<div class="post-info">
	<ul>
        <li class="vcard author">
                 by&nbsp;<a class="url fn" href="/author/alex-minnaar.html">Alex Minnaar</a>
        </li>
        <li class="published" title="2015-05-18T00:00:00+02:00">
          on&nbsp;Mon 18 May 2015
        </li>

	</ul>
<p>Category: <a href="/tag/nlp.html">   NLP</a><a href="/tag/supervised-learning.html">   Supervised Learning</a></p>
</div><!-- /.post-info --><p>In the <a href="http://alexminnaar.com/word2vec-tutorial-part-i-the-skip-gram-model.html">previous post</a> the concept of word vectors was explained as was the derivation of the <em>skip-gram</em> model.  In this post we will explore the other <em>Word2Vec</em> model - the <em>continuous bag-of-words</em> (CBOW) model.  If you understand the <em>skip-gram</em> model then the <em>CBOW</em> model should be quite straight-forward because in many ways they are mirror images of each other.  For instance, if you look at the model diagram</p>
<p><img alt="skip-gram model" src="images/cbow.png" /> </p>
<p>it looks like the <em>skip-gram</em> model with the inputs and outputs reversed.  The input layer consists of the <em>one-hot</em> encoded input context words <span class="math">\(\{\mathbf{x_1},...,\mathbf{x_C}\}\)</span> for a word window of size <span class="math">\(C\)</span> and vocabulary of size <span class="math">\(V\)</span>.  The hidden layer is an N-dimensional vector <span class="math">\(\mathbf{h}\)</span>.  Finally, the output layer is output word <span class="math">\(\mathbf{y}\)</span> in the training example which is also <em>one-hot</em> encoded.  The <em>one-hot</em> encoded input vectors are connected to the hidden layer via a <span class="math">\(V \times N\)</span> weight matrix <span class="math">\(\mathbf{W}\)</span> and the hidden layer is connected to the output layer via a <span class="math">\(N \times V\)</span> wieght matrix <span class="math">\(\mathbf{W'}\)</span>.</p>
<h1>Forward Propagation</h1>
<p>We must first understand how the output is computed from the input (i.e. forward propagation).  The following assumes that we know the input and output weight matrices (I will explain how these are actually learned in the next section).  The first step is to evaluate the output of the hidden layer <span class="math">\(\mathbf{h}\)</span>.  This is computed by</p>
<p>
<div class="math">$$\mathbf{h}=\frac{1}{C} \mathbf{W} \cdot (\sum^C_{i=1} \mathbf{x_i})$$</div>
</p>
<p>which is the average of the input vectors weighted by the matrix <span class="math">\(\mathbf{W}\)</span>.  It is worth noting that this hidden layer output computation is one of the only differences between the <em>continuous bag-of-words</em> model and the <em>skip-gram</em> model (in terms of them being mirror images of course).  Next we compute the inputs to each node in the output layer</p>
<p>
<div class="math">$$u_j= \mathbf{v'_{w_j}}^T \cdot \mathbf{h}$$</div>
</p>
<p>where <span class="math">\(\mathbf{v'_{w_j}}\)</span> is the <span class="math">\(j^{th}\)</span> column of the output matrix <span class="math">\(\mathbf{W'}\)</span>.  And finally we compute the output of the output layer.  The output <span class="math">\(y_j\)</span> is obtained by passing the input <span class="math">\(u_j\)</span> throught the soft-max function.</p>
<p>
<div class="math">$$y_j=p(w_{y_j} | w_1,...,w_C)=\frac{\exp(u_j)}{\sum^V_{j'=1} \exp(u_j')}$$</div>
</p>
<p>Now that we know how forward propagation works we can learn the weight matrices <span class="math">\(\mathbf{W}\)</span> and <span class="math">\(\mathbf{W'}\)</span>.  </p>
<h1>Learning the Weight Matrices with Backpropagation</h1>
<p>In the process of learning the wieght matrices <span class="math">\(\mathbf{W}\)</span> and <span class="math">\(\mathbf{W'}\)</span>, we begin with randomly initialized values.  We then sequentially feed training examples into our model and observe the error which is some function of the difference between the expected output and the actual output.  We then compute the gradient of this error with respect to the elements of both weight matrices and correct them in the direction of this gradient.  This general optimization procedure is known as stochastic gradient descent (or sgd) but the method by which the gradients are derived is known as backpropagation.  </p>
<p>The first step is to define the loss function.  The objective is to maximize the conditional probability of the output word given the input context, therefore our loss function will be</p>
<p>
<div class="math">$$\begin{align}
E &amp;= -\log p(w_O | w_I) \\
&amp;= -u_{j*} - \log \sum_{j'=1}^V \exp(u_{j'})  \\
&amp;=- \mathbf{v_{w_O}}^T \cdot \mathbf{h} - \log \sum_{j'=1}^V \exp(\mathbf{v_{w_{j'}}}^T \cdot \mathbf{h}) 
\end{align}$$</div>
</p>
<p>Where <span class="math">\(j^*\)</span> is the index of the the actual output word.  The next step is to derive the update equation for the hidden-output layer weights <span class="math">\(\mathbf{W'}\)</span>, then derive the weights for the input-hidden layer weights <span class="math">\(\mathbf{W}\)</span></p>
<h2>Updating the hidden-output layer weights</h2>
<p>The first step is to compute the derivative of the loss function <span class="math">\(E\)</span> with respect to the input to the <span class="math">\(j^{th}\)</span> node in the output layer <span class="math">\(u_j\)</span>.</p>
<p>
<div class="math">$$\frac{\partial{E}}{\partial{u_j}}=y_j - t_j$$</div>
</p>
<p>where <span class="math">\(t_j=1\)</span> if <span class="math">\(j=j^*\)</span> otherwise <span class="math">\(t_j=0\)</span>.  This is simply the prediction error of node <span class="math">\(j\)</span> in the output layer.  Next we take the derivative of <span class="math">\(E\)</span> with respect to the output weight <span class="math">\(w'_{ij}\)</span> using the chain rule.</p>
<p>
<div class="math">$$\begin{align}
\frac{\partial{E}}{\partial{w'_{ij}}} &amp;= \frac{\partial{E}}{\partial{u_j}} \cdot \frac{\partial{u_j}}{\partial{w'_{ij}}} \\
&amp;= (y_j - t_j) \cdot h_i
\end{align}$$</div>
</p>
<p>Now that we have the gradient with respect to an arbitrary output weight <span class="math">\(w'_{ij}\)</span>, we can define the stochastic gradient descent equation.</p>
<p>
<div class="math">$$w_{ij}^{'(new)}=w_{ij}^{'(old)} - \eta \cdot (y_j - t_j) \cdot h_i$$</div>
</p>
<p>or
<div class="math">$$\mathbf{v'_{w_j}}^{(new)}=\mathbf{v'_{w_j}}^{(old)} - \eta \cdot (y_j - t_j) \cdot \mathbf{h}$$</div>
</p>
<p>where <span class="math">\(\eta&gt;0\)</span> is the learning rate. </p>
<h2>Updating the input-hidden layer weights</h2>
<p>Now let's try to derive a similar update equation for the input weights <span class="math">\(w_{ij}\)</span>.  The first step is to compute the derivative of <span class="math">\(E\)</span> with respect to an arbitrary hidden node <span class="math">\(h_i\)</span> (again using the chain rule).</p>
<p>
<div class="math">$$
\begin{align}
\frac{\partial{E}}{\partial{h_i}} &amp;= \sum^V_{j=1} \frac{\partial{E}}{\partial{u_j}} \cdot \frac{\partial{u_j}}{\partial{h_i}} \\
&amp;= \sum^V_{j=1} (y_j - t_j) \cdot w'_{ij}
\end{align}
$$</div>
</p>
<p>where the sum is do to the fact that the hidden layer node <span class="math">\(h_i\)</span> is connected to each node of the output layer and therefore each prediction error must be incorporated.  The next step is to compute the derivative of <span class="math">\(E\)</span> with respect to an arbitrary input weight <span class="math">\(w_{ki}\)</span>.</p>
<p>
<div class="math">$$
\begin{align}
\frac{\partial{E}}{\partial{w_{ki}}} &amp;= \frac{\partial{E}}{\partial{h_i}} \cdot \frac{\partial{h_i}}{\partial{w_{ki}}}\\
 &amp;= \sum^V_{j=1} (y_j - t_j) \cdot w'_{ij} \cdot \frac{1}{C} \cdot x_k \\
 &amp; = \frac{1}{C}(\mathbf{x} \cdot EH)
\end{align}
$$</div>
</p>
<p>Where <span class="math">\(EH\)</span> is an N-dimensional vector of elements <span class="math">\(\sum^V_{j=1} (y_j - t_j) \cdot w'_{ij}\)</span> from <span class="math">\(i=1,...,N\)</span>.  However, since the inputs <span class="math">\(\mathbf{x}\)</span> are <em>one-hot</em> encoded, only one row of the <span class="math">\(N \times V\)</span> matrix <span class="math">\(\frac{1}{C}(\mathbf{x} \cdot EH)\)</span> will be nonzero.  Thus the final stochastic gradient descent equation for the input weights is</p>
<p>
<div class="math">$$\mathbf{v'_{w_{I,c}}}^{(new)}=\mathbf{v'_{w_{I,c}}}^{(old)} - \eta \cdot \frac{1}{C} \cdot EH$$</div>
</p>
<p>where <span class="math">\(w_{I,c}\)</span> is the <span class="math">\(c^{th}\)</span> word in the input context.</p>
<h2>References</h2>
<ul>
<li><a href="http://alexminnaar.com/word2vec-tutorial-part-i-the-skip-gram-model.html">Word2Vec Tutorial Part I: The Skip-Gram Model</a></li>
<li><a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Distributed Representations of Words and Phrases and their Compositionality</a>, Mikolov et al.</li>
<li><a href="http://arxiv.org/abs/1103.0398">Natural Language Processing (almost) from Scratch</a>, Collobert et al.</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                    </article>
                </div>
            </aside><!-- /#featured -->
            
        
        

    
            <aside id="featured">
                <div class="body">
                    <article>
                        <h1 class="entry-title"><a href="/word2vec-tutorial-part-i-the-skip-gram-model.html">Word2Vec Tutorial Part I: The Skip-Gram Model</a></h1>
<div class="post-info">
	<ul>
        <li class="vcard author">
                 by&nbsp;<a class="url fn" href="/author/alex-minnaar.html">Alex Minnaar</a>
        </li>
        <li class="published" title="2015-04-12T00:00:00+02:00">
          on&nbsp;Sun 12 April 2015
        </li>

	</ul>
<p>Category: <a href="/tag/nlp.html">   NLP</a><a href="/tag/supervised-learning.html">   Supervised Learning</a></p>
</div><!-- /.post-info --><p>In many natural language processing tasks, words are often represented by their <em>tf-idf</em> scores.  While these scores give us some idea of a word's relative importance in a document, they do not give us any insight into its semantic meaning.  <em>Word2Vec</em> is the name given to a class of neural network models that, given an unlabelled training corpus, produce a vector for each word in the corpus that encodes its semantic information.  These vectors are usefull for two main reasons.</p>
<ol>
<li>We can measure the semantic similarity between two words are by calculating the cosine similarity between their corresponding word vectors.  </li>
<li>We can use these word vectors as features for various supervised NLP tasks such as document classification, named entity recognition, and sentiment analysis.  The semantic information that is contained in these vectors make them powerful features for these tasks.</li>
</ol>
<p>You may ask <em>"how do we know that these vectors effectively capture the semantic meanings of the words?"</em>.  The answer is because the vectors adhere surprisingly well to our intuition.  For instance, words that we know to be synonyms tend to have similar vectors in terms of cosine similarity and antonyms tend to have dissimilar vectors.  Even more surprisingly, word vectors tend to obey the laws of analogy.  For example, consider the analogy <em>"Woman is to queen as man is to king"</em>.  It turns out that</p>
<p>
<div class="math">$$v_{queen}-v_{woman}+v_{man} \approx v_{king}$$</div>
</p>
<p>where <span class="math">\(v_{queen}\)</span>,<span class="math">\(v_{woman}\)</span>,<span class="math">\(v_{man}\)</span>, and <span class="math">\(v_{king}\)</span> are the word vectors for <span class="math">\(queen\)</span>, <span class="math">\(woman\)</span>, <span class="math">\(man\)</span>, and <span class="math">\(king\)</span> respectively.  These observations strongly suggest that word vectors encode valuable semantic information about the words that they represent. </p>
<p>In this series of blog posts I will describe the two main <em>Word2Vec</em> models - the <em>skip-gram model</em> and the <em>continuous bag-of-words</em> model.</p>
<p>Both of these models are simple neural networks with one hidden layer.  The word vectors are learned via backpropagation and stochastic gradient descent both of which I descibed in my previous <a href="http://alexminnaar.com/deep-learning-basics-neural-networks-backpropagation-and-stochastic-gradient-descent.html">Deep Learning Basics</a> blog post.</p>
<h1>The Skip-Gram Model</h1>
<p>Before we define the <em>skip-gram</em> model, it would be instructive to understand the format of the training data that it accepts.  The input of the <em>skip-gram</em> model is a single word <span class="math">\(w_I\)</span> and the output is the words in <span class="math">\(w_I\)</span>'s context <span class="math">\(\{w_{O,1},...,w_{O,C}\}\)</span> defined by a word window of size <span class="math">\(C\)</span>.  For example, consider the sentence <em>"I drove my car to the store"</em>.  A potential training instance could be the word "car" as an input and the words {"I","drove","my","to","the","store"} as outputs.  All of these words are <em>one-hot</em> encoded meaning they are vectors of length <span class="math">\(V\)</span> (the size of the vocabulary) with a value of <span class="math">\(1\)</span> at the index corresponding to the word and zeros in all other indexes.  As you can see, we are essentially <em>creating</em> training examples from plain text which means that we can have a virtually unlimited number of training examples at our disposal.</p>
<h2>Forward Propagation</h2>
<p>Now let's define the <em>skip-gram</em> nerual network model as follows.</p>
<p><img alt="skip-gram model" src="images/skip-gram.png" /> </p>
<p>In the above model <span class="math">\(\mathbf{x}\)</span> represents the <em>one-hot</em> encoded vector corresponding to the input word in the training instance and <span class="math">\(\{\mathbf{y_1},...\mathbf{y_C}\}\)</span> are the <em>one-hot</em> encoded vectors corresponding to the output words in the training instance.  The <span class="math">\(V \times N\)</span> matrix <span class="math">\(\mathbf{W}\)</span> is the weight matrix between the input layer and hidden layer whose <span class="math">\(i^{th}\)</span> row represents the weights corresponding to the <span class="math">\(i^{th}\)</span> word in the vocabulary. This weight matrix <span class="math">\(\mathbf{W}\)</span> is what we are interested in learning because it contains the vector encodings of all of the words in our vocabulary (as its rows).  Each output word vector also has an associated <span class="math">\(N \times V\)</span> output matrix <span class="math">\(\mathbf{W'}\)</span>. There is also a hidden layer consisting of <span class="math">\(N\)</span> nodes (the exact size of <span class="math">\(N\)</span> is a training parameter).</p>
<p>From my <a href="http://alexminnaar.com/deep-learning-basics-neural-networks-backpropagation-and-stochastic-gradient-descent.html">previous blog post</a>, we know that the input to a unit in the hidden layer <span class="math">\(h_i\)</span> is simply the weighted sum of its inputs.  Since the input vector <span class="math">\(\mathbf{x}\)</span> is <em>one-hot</em> encoded, the weights coming from the nonzero element will be the only ones contributing to the hidden layer.  Therefore, for the input <span class="math">\(\mathbf{x}\)</span> with <span class="math">\(x_k=1\)</span> and <span class="math">\(x_{k'}=0\)</span> for all <span class="math">\(k' \neq k\)</span> the outputs of the hidden layer will be equivalent to the <span class="math">\(k^{th}\)</span> row of <span class="math">\(\mathbf{W}\)</span>.  Or mathematically,</p>
<p>
<div class="math">$$\mathbf{h}=\mathbf{x}^T \mathbf{W}=\mathbf{W}_{(k, .)} := \mathbf{v}_{w_I}$$</div>
</p>
<p>Notice that there is no activation function used here.  This is presumably because the inputs are bounded by the <em>one-hot</em> encoding.  In the same way, the inputs to each of the <span class="math">\(C \times V\)</span> output nodes is computed by the weighted sum of its inputs.  Therefore, the input to the <span class="math">\(j^{th}\)</span> node of the <span class="math">\(c^{th}\)</span> output word is</p>
<p>
<div class="math">$$u_{c,j}=\mathbf{v'}_{w_j}^T \mathbf{h}$$</div>
</p>
<p>However we can also observe that the output layers for each output word share the same weights therefore <span class="math">\(u_{c,j}=u_j\)</span>.  We can finally compute the output of the <span class="math">\(j^{th}\)</span> node of the <span class="math">\(c^{th}\)</span> output word via the <em>softmax</em> function which produces a multinomial distribution </p>
<p>
<div class="math">$$p(w_{c,j}=w_{O,c} | w_I)=y_{c,j}=\frac{exp(u_{c,j})}{\sum_{j'=1}^V exp(u_{j'})}$$</div>
</p>
<p>In plain english, this value is the probability that the output of the <span class="math">\(j^{th}\)</span> node of the <span class="math">\(c^{th}\)</span> output word is equal to the actual value of the <span class="math">\(j^{th}\)</span> index of the <span class="math">\(c^{th}\)</span> output vector (which is <em>one-hot</em> encoded).</p>
<h2>Learning the Weights with Backpropagation and Stochastic Gradient Descent</h2>
<p>Now that we know how inputs are propograted forward through the network to produce outputs, we can derive the error gradients necessary for the backpropagation algorithm which we will use to learn both <span class="math">\(\mathbf{W}\)</span> and <span class="math">\(\mathbf{W'}\)</span>.  The first step in deriving the gradients is defining a loss function.  This loss function will be</p>
<p>
<div class="math">$$\begin{align}
E &amp;= -\log p(w_{O,1}, w_{O,2}, ..., w_{O,C} | w_I) \\
&amp;= -\log \prod^C_{c=1} \frac{exp(u_{c,j^*_c})}{\sum^V_{j'=1} exp(u_j')} \\
&amp;= -\sum^C_{c=1}u_{j^*_c} +C \cdot \log \sum^V_{j'=1} exp(u_{j'})
\end{align}$$</div>
</p>
<p>which is simply the probability of the output words (the words in the input word's context) given the input word.  Here, <span class="math">\(j^*_c\)</span> is the index of the <span class="math">\(c^{th}\)</span> output word.  If we take the derivative with respect to <span class="math">\(u_{c,j}\)</span> we get</p>
<p>
<div class="math">$$\frac{\partial{E}}{\partial{u_{c,j}}}=y_{c,j}-t_{c,j}$$</div>
</p>
<p>where <span class="math">\(t_{c,j}=1\)</span> if the <span class="math">\(j^{th}\)</span> node of the <span class="math">\(c^{th}\)</span> true output word is equal to <span class="math">\(1\)</span> (from its <em>one-hot</em> encoding), otherwise <span class="math">\(t_{c,j}=1\)</span>.  This is the prediction error for node <span class="math">\(c,j\)</span> (or the <span class="math">\(j^{th}\)</span> node of the <span class="math">\(c^{th}\)</span> output word).</p>
<p>Now that we have the error derivative with respect to inputs of the final layer, we can derive the derivative with respect to the output matrix <span class="math">\(\mathbf{W'}\)</span>.  Here we use the chain rule </p>
<p>
<div class="math">$$\begin{align}
\frac{\partial E}{\partial w'_{ij}} &amp;=\sum^C_{c=1} \frac{\partial E}{\partial u_{c,j}} \cdot \frac{\partial u_{c,j}}{\partial w'_{ij}} \\
&amp;=\sum^C_{c=1} (y_{c,j}-t_{c,j}) \cdot h_i
\end{align}$$</div>
</p>
<p>Therefore the gradient descent update equation for the output matrix <span class="math">\(\mathbf{W'}\)</span> is</p>
<p>
<div class="math">$$w'^{(new)}_{ij} =w'^{(old)}_{ij}- \eta \cdot \sum^C_{c=1} (y_{c,j}-t_{c,j}) \cdot h_i$$</div>
</p>
<p>Now we can derive the update equation for the input-hidden layer weights in <span class="math">\(\mathbf{W}\)</span>.  Let's start by computing the error derivative with respect to the hidden layer.</p>
<p>
<div class="math">$$\begin{align}
\frac{\partial{E}}{\partial{h_i}} &amp;=\sum^V_{j=1} \frac{\partial{E}}{\partial{u_j}} \cdot \frac{\partial{u_j}}{\partial{h_i}}  \\
&amp;=\sum^V_{j=1} \sum^C_{c=1} (y_{c,j}-t_{c,j})  \cdot w'_{ij}
\end{align}$$</div>
</p>
<p>Now we are able to compute the derivative with respect to <span class="math">\(\mathbf{W}\)</span></p>
<p>
<div class="math">$$\begin{align}
\frac{\partial{E}}{\partial{w_{ki}}} &amp;=\frac{\partial{E}}{\partial{h_i}} \cdot \frac{\partial{h_i}}{\partial{w_{ki}}} \\
&amp;= \sum^V_{j=1} \sum^C_{c=1} (y_{c,j}-t_{c,j})  \cdot w'_{ij} \cdot x_k
\end{align}$$</div>
</p>
<p>and finally we arrive at our gradient descent equation for our input weights</p>
<p>
<div class="math">$$w^{(new)}_{ij} =w^{(old)}_{ij}- \eta \cdot \sum^V_{j=1} \sum^C_{c=1} (y_{c,j}-t_{c,j})  \cdot w'_{ij} \cdot x_j$$</div>
</p>
<p>As you can see, each gradient descent update requires a sum over the entire vocabulary <span class="math">\(V\)</span> which is computationally expensive.  In practice, computation techniques such as hierarchical softmax and negative sampling are used to make this computation more efficient.</p>
<h2>References</h2>
<ul>
<li><a href="word2vec-tutorial-part-ii-the-continuous-bag-of-words-model.html">Word2Vec Tutorial Part II: The Continuous Bag-of-Words Model</a></li>
<li><a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Distributed Representations of Words and Phrases and their Compositionality</a>, Mikolov et al.</li>
<li><a href="http://arxiv.org/abs/1103.0398">Natural Language Processing (almost) from Scratch</a>, Collobert et al.</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                    </article>
                </div>
            </aside><!-- /#featured -->
            
        
        

    
            <aside id="featured">
                <div class="body">
                    <article>
                        <h1 class="entry-title"><a href="/distributed-online-latent-dirichlet-allocation-with-apache-spark.html">Distributed Online Latent Dirichlet Allocation with Apache Spark</a></h1>
<div class="post-info">
	<ul>
        <li class="vcard author">
                 by&nbsp;<a class="url fn" href="/author/alex-minnaar.html">Alex Minnaar</a>
        </li>
        <li class="published" title="2015-03-20T00:00:00+01:00">
          on&nbsp;Fri 20 March 2015
        </li>

	</ul>
<p>Category: <a href="/tag/nlp.html">   NLP</a><a href="/tag/software-engineering.html">   Software Engineering</a></p>
</div><!-- /.post-info --><p>In the past, I have studied the online LDA algorithm from <a href="https://www.google.ca/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CDUQFjAA&amp;url=https%3A%2F%2Fwww.cs.princeton.edu%2F~blei%2Fpapers%2FHoffmanBleiBach2010b.pdf&amp;ei=pib7VP_ZIsewggS3pYAY&amp;usg=AFQjCNHLmU8Gk_P4usBj2QcGcaolw87w4w&amp;sig2=TVmpNtdTBqqPScHDqBGYcg&amp;bvm=bv.87611401,d.eXY">Hoffman et al.</a> in some depth resulting in <a href="http://alexminnaar.com/online-latent-dirichlet-allocation-the-best-option-for-topic-modeling-with-large-data-sets.html">this blog post</a> and corresponding <a href="https://github.com/alexminnaar/ScalaTopicModels">Scala code</a>.  Before we go further I will provide a general description of how the algorithm works.  In online LDA, minibatches of documents are sequentially processed to update a global topic/word matrix which defines the topics that have been learned.  The processing consists of two steps:</p>
<ul>
<li><strong>The E-Step:</strong>  Given the minibatch of documents, updates to the corresponding rows of the topic/word matrix are computed.</li>
<li><strong>The M-Step:</strong>  The updates from the E-Step are blended with the current topic/word matrix resulting in the updated topic/word matrix.</li>
</ul>
<p>This post details how I developed a distributed version of online LDA using the Apache Spark engine.  At first glance, it might seem redundant to build a distributed version of online LDA since one of the main advantages of the algorithm is its scalability (documents are streamed sequentially so they do not need to be kept in main memory).  While it is true that the original algorithm is scalable in terms of memory, using a distributed computing framework (such as Spark) can speed the algorithm up immensely.  Today, companies are demanding real-time or near real-time data processing which makes a Spark solution advantageous.  Furthermore, there are some cases - for example if you choose to learn a sufficiently large number of topics with a sufficiently large vocabulary size - where the original algorithm can in fact run into memory issues.  Hopefully I have shown that a distributed version of online LDA would be beneficial.</p>
<p>This blog post will be divided into a few sections.  First I will give a very broad overview of how Spark works.  Then I will outline how some processes of the original online LDA algorithm can be parallelized (again, for a more detailed outline of how the original online LDA algorithm works I encourage you to read <a href="http://alexminnaar.com/online-latent-dirichlet-allocation-the-best-option-for-topic-modeling-with-large-data-sets.html">my prevous blog post</a>).  Finally I will provide the code for the Spark implemenation as well as a demo.</p>
<h1>The Basics of Spark</h1>
<p>The beauty of Spark is in its simplicity.  It has abstracted away all of the complicated aspects of MapReduce programming and it leaves us with a simple interface with which to build our distributed processing jobs.  Here I will provide you with a very brief and incomplete overview of how Spark works (refer to the <a href="https://spark.apache.org/docs/latest/">Spark documentation</a> for more details). </p>
<p>In Spark, all distributed computations are done on RDDs (Resilient Distributed Datasets).  RDDs are linear data structures that are distributed across the nodes in your cluster.  The most common operation that you can perform on an RDD is a <code>map</code>.  The <code>map</code> function takes a function as an input an applies this function to each element of the RDD in parallel and returns another RDD containing the result.  Another common operation that is performed on RDDs is the <code>reduce</code> function.  <code>reduce</code> also takes a function as an input (that must be commutative and associative) and aggregates the elements of the RDD in based on that function.  There are many other very useful RDD operations (eg. <em>reduceByKey</em>, <em>join</em>, <em>zip</em> etc.) that are beyond the scope of this blog post.  To learn more read the <a href="https://spark.apache.org/docs/latest/programming-guide.html">Spark programming guide</a>.</p>
<p>It is also important to note that, aside from the user-friendly interface, Spark's main advantage is its speed.  Unlike previous MapReduce frameworks, Spark utilizes the RAM of the machines in the clusters.  When data is kept in memory, disk serialization/deserialzation is greatly reduced.  This, in turn, allows us to distribute iterative algorithms much faster (we no longer have to wait for the data to write to disk after each iteration).  Online LDA is a distributed algorithm which makes it a great fit for Spark.</p>
<h1>Parallelizing the Online LDA Algorithm</h1>
<p>Now let's use Spark to parallelize the online LDA algorith.  There are four aspects of the algorithm that could benefit from parallelzation - the global topic/word matrix, the minibatch, the E-Step, and the M-Step.  Let's start with the topic/word matrix.</p>
<h2>A Distributed Topic/Word Matrix</h2>
<p>The global topic/word matrix is the data structure that contains all of the topics that have been learned so far.  The rows of this matrix correspond to the words in the vocabulary and the columns correspond to the topics.  At first, it might seem like this data structure is small enough to exist locally in the driver.  After all, there are only so many words in the English language which bounds the number of rows and the number of topics should be less than the number of words in the vocabulary.  However, I have experienced out-of-memory errors when this matrix is too large.  This is likely not because the matrix itself is too large to fit in the driver, but because size of this matrix combined with everything else that is associated with the Spark application is too large.  In addition, distributing the topic/word matrix allows for more parallelism in other parts of the online LDA algorithm which results in an overall speedup.  Spark's MLlib library provides a distributed matrix data structure called <a href="https://spark.apache.org/docs/1.0.0/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix">indexedRowMatrix</a> which is an RDD of tuples containing the rows of the matrix and their corresponding indexes.  We will use this data structure to store our topic/word matrix.</p>
<h2>A Distributed Minibatch</h2>
<p>The minibatch is a collection of documents with which we update the LDA model.  In the original algorithm, the documents in the minibatch must be converted to <em>bag-of-words</em> format (i.e. a set of (wordId, frequency) tuples).  If the minibatch is made into an RDD (where each element of the RDD consists of a document in the minibatch), the <em>bag-of-words</em> conversion can be performed in parallel via a <em>map</em> and a <em>toBagOfWords</em> function as shown below.</p>
<div class="highlight"><pre><span class="k">val</span> <span class="n">bagOfWordsRDD</span><span class="k">:</span><span class="kt">RDD</span><span class="o">[</span><span class="kt">List</span><span class="o">[(</span><span class="kt">Int</span>,<span class="kt">Int</span><span class="o">)]]</span> <span class="k">=</span> <span class="n">minibatchRDD</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="n">toBagOfWords</span><span class="o">(</span><span class="n">x</span><span class="o">))</span>
</pre></div>


<p>Clearly, this is more efficient that converting each document to its <em>bag-of-words</em> format sequentially.  Furthermore, well shall see later that the minibatch must be an RDD in order to parallelize future computations that involve the minibatch.</p>
<h2>A Distributed E-Step</h2>
<p>The following is the original non-distributed implementation of the E-step for online LDA.  It is not important to understand exactly what the code is doing - the important part is observing the presence of the inner and outer loops (indicated in the comments).</p>
<div class="highlight"><pre> <span class="k">def</span> <span class="n">eStep</span><span class="o">(</span><span class="n">miniBatch</span><span class="k">:</span> <span class="kt">List</span><span class="o">[</span><span class="kt">List</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Int</span><span class="o">)]],</span> <span class="n">expELogBeta</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span><span class="k">:</span> <span class="o">(</span><span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="nc">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span> <span class="k">=</span> <span class="o">{</span>

    <span class="k">val</span> <span class="n">gamma</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">](</span><span class="n">miniBatch</span><span class="o">.</span><span class="n">length</span><span class="o">,</span> <span class="n">numTopics</span><span class="o">,</span> <span class="nc">Gamma</span><span class="o">(</span><span class="mf">100.0</span><span class="o">,</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="mf">100.0</span><span class="o">).</span><span class="n">sample</span><span class="o">(</span><span class="n">numTopics</span> <span class="o">*</span> <span class="n">miniBatch</span><span class="o">.</span><span class="n">length</span><span class="o">).</span><span class="n">toArray</span><span class="o">)</span>

    <span class="k">val</span> <span class="n">eLogTheta</span> <span class="k">=</span> <span class="n">dirichletExpectation</span><span class="o">(</span><span class="n">gamma</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">expELogTheta</span> <span class="k">=</span> <span class="n">exp</span><span class="o">(</span><span class="n">eLogTheta</span><span class="o">)</span>
    <span class="k">var</span> <span class="n">sstats</span> <span class="k">=</span> <span class="nc">DenseMatrix</span><span class="o">.</span><span class="n">zeros</span><span class="o">[</span><span class="kt">Double</span><span class="o">](</span><span class="n">numTopics</span><span class="o">,</span> <span class="n">numTerms</span><span class="o">)</span>

    <span class="k">for</span> <span class="o">((</span><span class="n">doc</span><span class="o">,</span> <span class="n">idx</span><span class="o">)</span> <span class="k">&lt;-</span> <span class="n">miniBatch</span><span class="o">.</span><span class="n">zipWithIndex</span><span class="o">)</span> <span class="o">{</span>  <span class="c1">// &lt;---------- Outer Loop</span>

      <span class="k">val</span> <span class="n">idCtList</span> <span class="k">=</span> <span class="n">doc</span><span class="o">.</span><span class="n">toList</span><span class="o">.</span><span class="n">sortBy</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">_1</span><span class="o">)</span>
      <span class="k">val</span> <span class="n">wordIDs</span> <span class="k">=</span> <span class="n">idCtList</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">_1</span><span class="o">)</span>
      <span class="k">val</span> <span class="n">cts</span> <span class="k">=</span> <span class="n">idCtList</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">_2</span><span class="o">.</span><span class="n">toDouble</span><span class="o">)</span>

      <span class="k">val</span> <span class="n">gammaD</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="k">=</span> <span class="n">gamma</span><span class="o">(</span><span class="n">idx</span><span class="o">,</span> <span class="o">::).</span><span class="n">t</span><span class="o">.</span><span class="n">toDenseMatrix</span>
      <span class="k">val</span> <span class="n">expELogThetaD</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="k">=</span> <span class="n">expELogTheta</span><span class="o">(</span><span class="n">idx</span><span class="o">,</span> <span class="o">::).</span><span class="n">t</span><span class="o">.</span><span class="n">toDenseMatrix</span>
      <span class="k">val</span> <span class="n">expELogBetaD</span> <span class="k">=</span> <span class="n">expELogBeta</span><span class="o">(</span><span class="mi">0</span> <span class="n">until</span> <span class="n">expELogBeta</span><span class="o">.</span><span class="n">rows</span><span class="o">,</span> <span class="n">wordIDs</span><span class="o">.</span><span class="n">toIndexedSeq</span><span class="o">).</span><span class="n">toDenseMatrix</span>
      <span class="k">val</span> <span class="n">phiNorm</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="k">=</span> <span class="n">expELogThetaD</span> <span class="o">*</span> <span class="n">expELogBetaD</span> <span class="o">+</span> <span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">100</span>
      <span class="k">val</span> <span class="n">docCounts</span> <span class="k">=</span> <span class="nc">DenseMatrix</span><span class="o">(</span><span class="n">cts</span><span class="o">.</span><span class="n">toArray</span><span class="o">)</span>

      <span class="c1">//Recursive loop to infer phiNorm, gammaD and exoElogThetaD parameters</span>
      <span class="k">def</span> <span class="n">gammaUpdate</span><span class="o">(</span><span class="n">pn</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="n">expETD</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
        <span class="k">val</span> <span class="n">term1</span> <span class="k">=</span> <span class="n">expETD</span> <span class="o">:*</span> <span class="o">(</span><span class="n">docCounts</span> <span class="o">/</span> <span class="n">pn</span><span class="o">)</span> <span class="o">*</span> <span class="n">expELogBetaD</span><span class="o">.</span><span class="n">t</span>
        <span class="n">term1</span><span class="o">(::,</span> <span class="o">*)</span> <span class="o">+</span> <span class="n">alpha</span>
      <span class="o">}</span>

      <span class="k">def</span> <span class="n">thetaUpdate</span><span class="o">(</span><span class="n">gD</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
        <span class="n">exp</span><span class="o">(</span><span class="n">dirichletExpectation</span><span class="o">(</span><span class="n">gD</span><span class="o">))</span>
      <span class="o">}</span>

      <span class="k">def</span> <span class="n">phiUpdate</span><span class="o">(</span><span class="n">expETD</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
        <span class="n">expETD</span> <span class="o">*</span> <span class="n">expELogBetaD</span> <span class="o">+</span> <span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">100</span>
      <span class="o">}</span>

      <span class="k">def</span> <span class="n">eStepIterator</span><span class="o">(</span><span class="n">phiNorm</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="n">expELogThetaD</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="n">gammaD</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span><span class="k">:</span> <span class="o">(</span><span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="nc">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="nc">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span> <span class="k">=</span> <span class="o">{</span>

        <span class="k">val</span> <span class="n">lastGamma</span> <span class="k">=</span> <span class="nc">DenseMatrix</span><span class="o">.</span><span class="n">zeros</span><span class="o">[</span><span class="kt">Double</span><span class="o">](</span><span class="n">gammaD</span><span class="o">.</span><span class="n">rows</span><span class="o">,</span> <span class="n">gammaD</span><span class="o">.</span><span class="n">cols</span><span class="o">)</span>

        <span class="k">def</span> <span class="n">loop</span><span class="o">(</span><span class="n">counter</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">newGamma</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="n">newTheta</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="n">newPhi</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="n">lastGamma</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span><span class="k">:</span> <span class="o">(</span><span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="nc">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="nc">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span> <span class="k">=</span> <span class="o">{</span>
          <span class="k">if</span> <span class="o">(((</span><span class="n">mean</span><span class="o">(</span><span class="n">abs</span><span class="o">(</span><span class="n">newGamma</span> <span class="o">-</span> <span class="n">lastGamma</span><span class="o">)))</span> <span class="o">&lt;</span> <span class="n">gammaThreshold</span><span class="o">)</span> <span class="o">||</span> <span class="o">(</span><span class="n">counter</span> <span class="o">&gt;</span> <span class="n">iterations</span><span class="o">))</span> <span class="o">{</span>
            <span class="o">(</span><span class="n">newGamma</span><span class="o">,</span> <span class="n">newPhi</span><span class="o">,</span> <span class="n">newTheta</span><span class="o">)</span>
          <span class="o">}</span>
          <span class="k">else</span> <span class="o">{</span>
            <span class="k">val</span> <span class="n">term1</span> <span class="k">=</span> <span class="n">gammaUpdate</span><span class="o">(</span><span class="n">newPhi</span><span class="o">,</span> <span class="n">newTheta</span><span class="o">)</span>
            <span class="k">val</span> <span class="n">term2</span> <span class="k">=</span> <span class="n">thetaUpdate</span><span class="o">(</span><span class="n">term1</span><span class="o">)</span>
            <span class="k">val</span> <span class="n">term3</span> <span class="k">=</span> <span class="n">phiUpdate</span><span class="o">(</span><span class="n">term2</span><span class="o">)</span>
            <span class="n">loop</span><span class="o">(</span><span class="n">counter</span> <span class="o">+</span> <span class="mi">1</span><span class="o">,</span> <span class="n">term1</span><span class="o">,</span> <span class="n">term2</span><span class="o">,</span> <span class="n">term3</span><span class="o">,</span> <span class="n">newGamma</span><span class="o">)</span>
          <span class="o">}</span>
        <span class="o">}</span>
        <span class="n">loop</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="n">gammaD</span><span class="o">,</span> <span class="n">expELogThetaD</span><span class="o">,</span> <span class="n">phiNorm</span><span class="o">,</span> <span class="n">lastGamma</span><span class="o">)</span>
      <span class="o">}</span>

      <span class="c1">//execute recursive loop function</span>
      <span class="k">val</span> <span class="o">(</span><span class="n">newGammaD</span><span class="o">,</span> <span class="n">newPhiNorm</span><span class="o">,</span> <span class="n">newExpELogThetaD</span><span class="o">)</span> <span class="k">=</span> <span class="n">eStepIterator</span><span class="o">(</span><span class="n">phiNorm</span><span class="o">,</span> <span class="n">expELogThetaD</span><span class="o">,</span> <span class="n">gammaD</span><span class="o">)</span>  <span class="c1">// &lt;---------- Inner Loop</span>
      <span class="n">gamma</span><span class="o">(</span><span class="n">idx</span><span class="o">,</span> <span class="o">::)</span> <span class="o">:=</span> <span class="n">newGammaD</span><span class="o">.</span><span class="n">toDenseVector</span><span class="o">.</span><span class="n">t</span>
      <span class="k">val</span> <span class="n">sstatTerm</span> <span class="k">=</span> <span class="n">newExpELogThetaD</span><span class="o">.</span><span class="n">t</span> <span class="o">*</span> <span class="o">(</span><span class="n">docCounts</span> <span class="o">/</span> <span class="n">newPhiNorm</span><span class="o">)</span>

      <span class="k">for</span> <span class="o">((</span><span class="n">i</span><span class="o">,</span> <span class="n">ct</span><span class="o">)</span> <span class="k">&lt;-</span> <span class="n">wordIDs</span><span class="o">.</span><span class="n">zipWithIndex</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">sstats</span><span class="o">(::,</span> <span class="n">i</span><span class="o">)</span> <span class="o">:+=</span> <span class="n">sstatTerm</span><span class="o">(::,</span> <span class="n">ct</span><span class="o">)</span>
      <span class="o">}</span>
    <span class="o">}</span>

    <span class="n">sstats</span> <span class="k">=</span> <span class="n">sstats</span> <span class="o">:*</span> <span class="n">expELogBeta</span>
    <span class="o">(</span><span class="n">gamma</span><span class="o">,</span> <span class="n">sstats</span><span class="o">)</span>
  <span class="o">}</span>
</pre></div>


<p>As you can see, there is an outer loop over all documents in the minibatch, and an inner (recursive) loop for each document.  In the distributed implementation, the inner loop is performed in parallel for every document in the minibatch RDD (using a <code>map</code> function) and then the results are combined (using a <code>reduce</code> function).  </p>
<p>So let's first focus on the <code>map</code> function.  This map function is applied to every document in the minibatch RDD in parallel so the function should take only one document as an input and be essentially the same as the inner loop of the above non-distributed version.  Unfortunately, this produces a big problem because this code also requires the global topic/word matrix (the <code>expELogBeta</code> variable) which is now an RDD.  <strong>Spark does not allow nested RDD computations!</strong>  That is, one cannot apply a map function to an RDD that involves another RDD (this will create a serialization error).  We have to be a bit more creative!</p>
<p>This problem can be solved by first observing that the E-Step does not necessarily require all of the rows of the topic/word matrix, only those corresponding to the words that are in the minibatch.   Therefore, let's have our <code>map</code> function take a list of 3-tuples that consist of the wordId, the word count, and the row of the topic/word matrix corresponding to the wordId. This way, we can compute the E-Step without working with the distributed topic/word matrix directly.  The code below shows this implementation.</p>
<div class="highlight"><pre><span class="k">def</span> <span class="n">eStep</span><span class="o">(</span><span class="n">expELogBeta</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[(</span><span class="kt">V</span>, <span class="kt">Int</span>, <span class="kt">Int</span><span class="o">)],</span> <span class="n">numTopics</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">alpha</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">gammaThreshold</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">iterations</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span><span class="k">:</span> <span class="o">(</span><span class="kt">Array</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Array</span><span class="o">[</span><span class="kt">Double</span><span class="o">])],</span> <span class="nc">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span> <span class="k">=</span> <span class="o">{</span>

    <span class="k">var</span> <span class="n">gammaD</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">](</span><span class="n">numTopics</span><span class="o">,</span> <span class="mi">1</span><span class="o">,</span> <span class="nc">Gamma</span><span class="o">(</span><span class="mf">100.0</span><span class="o">,</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="mf">100.0</span><span class="o">).</span><span class="n">sample</span><span class="o">(</span><span class="n">numTopics</span><span class="o">).</span><span class="n">toArray</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">eLogThetaD</span> <span class="k">=</span> <span class="n">dirichletExpectation</span><span class="o">(</span><span class="n">gammaD</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">expELogThetaD</span> <span class="k">=</span> <span class="n">exp</span><span class="o">(</span><span class="n">eLogThetaD</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">cts</span> <span class="k">=</span> <span class="n">expELogBeta</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">_3</span><span class="o">.</span><span class="n">toDouble</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">wordIDs</span> <span class="k">=</span> <span class="n">expELogBeta</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">expELogBetaD</span> <span class="k">=</span> <span class="n">expELogBeta</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">_1</span><span class="o">.</span><span class="n">toArray</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">expELogBetaDDM</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DenseMatrix</span><span class="o">(</span><span class="n">expELogBetaD</span><span class="o">.</span><span class="n">size</span><span class="o">,</span> <span class="n">numTopics</span><span class="o">,</span> <span class="n">expELogBetaD</span><span class="o">.</span><span class="n">flatten</span><span class="o">,</span> <span class="mi">0</span><span class="o">,</span> <span class="n">numTopics</span><span class="o">,</span> <span class="kc">true</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">phiNorm</span> <span class="k">=</span> <span class="n">expELogBetaDDM</span> <span class="o">*</span> <span class="n">expELogThetaD</span> <span class="o">+</span> <span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">100</span>
    <span class="k">val</span> <span class="n">docCounts</span> <span class="k">=</span> <span class="nc">DenseMatrix</span><span class="o">(</span><span class="n">cts</span><span class="o">)</span>

    <span class="k">def</span> <span class="n">gammaUpdate</span><span class="o">(</span><span class="n">pn</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="n">expETD</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
      <span class="k">val</span> <span class="n">term1</span> <span class="k">=</span> <span class="n">expETD</span> <span class="o">:*</span> <span class="o">(</span><span class="n">docCounts</span> <span class="o">/</span> <span class="n">pn</span><span class="o">.</span><span class="n">t</span><span class="o">)</span> <span class="o">*</span> <span class="n">expELogBetaDDM</span>
      <span class="n">term1</span><span class="o">(::,</span> <span class="o">*)</span> <span class="o">+</span> <span class="n">alpha</span>
    <span class="o">}</span>

    <span class="k">def</span> <span class="n">thetaUpdate</span><span class="o">(</span><span class="n">gD</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
      <span class="n">exp</span><span class="o">(</span><span class="n">dirichletExpectation</span><span class="o">(</span><span class="n">gD</span><span class="o">))</span>
    <span class="o">}</span>

    <span class="k">def</span> <span class="n">phiUpdate</span><span class="o">(</span><span class="n">expETD</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
      <span class="n">expELogBetaDDM</span> <span class="o">*</span> <span class="n">expETD</span> <span class="o">+</span> <span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">100</span>
    <span class="o">}</span>

    <span class="k">def</span> <span class="n">eStepIterator</span><span class="o">(</span><span class="n">phiNorm</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="n">expELogThetaD</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span>
                      <span class="n">gammaD</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span><span class="k">:</span> <span class="o">(</span><span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="nc">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="nc">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span> <span class="k">=</span> <span class="o">{</span>

      <span class="k">val</span> <span class="n">lastGamma</span> <span class="k">=</span> <span class="nc">DenseMatrix</span><span class="o">.</span><span class="n">zeros</span><span class="o">[</span><span class="kt">Double</span><span class="o">](</span><span class="n">gammaD</span><span class="o">.</span><span class="n">rows</span><span class="o">,</span> <span class="n">gammaD</span><span class="o">.</span><span class="n">cols</span><span class="o">)</span>

      <span class="k">def</span> <span class="n">loop</span><span class="o">(</span><span class="n">counter</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">newGamma</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="n">newTheta</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="n">newPhi</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span>
               <span class="n">lastGamma</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span><span class="k">:</span> <span class="o">(</span><span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="nc">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="nc">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span> <span class="k">=</span> <span class="o">{</span>
        <span class="k">if</span> <span class="o">(((</span><span class="n">mean</span><span class="o">(</span><span class="n">abs</span><span class="o">(</span><span class="n">newGamma</span> <span class="o">-</span> <span class="n">lastGamma</span><span class="o">)))</span> <span class="o">&lt;</span> <span class="n">gammaThreshold</span><span class="o">)</span> <span class="o">||</span> <span class="o">(</span><span class="n">counter</span> <span class="o">&gt;</span> <span class="n">iterations</span> <span class="o">-</span> <span class="mi">1</span><span class="o">))</span> <span class="o">{</span>
          <span class="o">(</span><span class="n">newGamma</span><span class="o">,</span> <span class="n">newPhi</span><span class="o">,</span> <span class="n">newTheta</span><span class="o">)</span>
        <span class="o">}</span>
        <span class="k">else</span> <span class="o">{</span>
          <span class="k">val</span> <span class="n">term1</span> <span class="k">=</span> <span class="n">gammaUpdate</span><span class="o">(</span><span class="n">newPhi</span><span class="o">,</span> <span class="n">newTheta</span><span class="o">)</span>
          <span class="k">val</span> <span class="n">term2</span> <span class="k">=</span> <span class="n">thetaUpdate</span><span class="o">(</span><span class="n">term1</span><span class="o">)</span>
          <span class="k">val</span> <span class="n">term3</span> <span class="k">=</span> <span class="n">phiUpdate</span><span class="o">(</span><span class="n">term2</span><span class="o">)</span>
          <span class="n">loop</span><span class="o">(</span><span class="n">counter</span> <span class="o">+</span> <span class="mi">1</span><span class="o">,</span> <span class="n">term1</span><span class="o">,</span> <span class="n">term2</span><span class="o">,</span> <span class="n">term3</span><span class="o">,</span> <span class="n">newGamma</span><span class="o">)</span>
        <span class="o">}</span>
      <span class="o">}</span>
      <span class="n">loop</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="n">gammaD</span><span class="o">,</span> <span class="n">expELogThetaD</span><span class="o">,</span> <span class="n">phiNorm</span><span class="o">,</span> <span class="n">lastGamma</span><span class="o">)</span>
    <span class="o">}</span>

    <span class="c1">//execute recursive loop function</span>
    <span class="k">val</span> <span class="o">(</span><span class="n">newGammaD</span><span class="o">,</span> <span class="n">newPhiNorm</span><span class="o">,</span> <span class="n">newExpELogThetaD</span><span class="o">)</span> <span class="k">=</span> <span class="n">eStepIterator</span><span class="o">(</span><span class="n">phiNorm</span><span class="o">,</span> <span class="n">expELogThetaD</span><span class="o">,</span> <span class="n">gammaD</span><span class="o">)</span>
    <span class="n">gammaD</span> <span class="k">=</span> <span class="n">newGammaD</span>
    <span class="k">val</span> <span class="n">sstatTerm</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="k">=</span> <span class="o">(</span><span class="n">docCounts</span> <span class="o">/</span> <span class="n">newPhiNorm</span><span class="o">.</span><span class="n">t</span><span class="o">).</span><span class="n">t</span> <span class="o">*</span> <span class="n">newExpELogThetaD</span><span class="o">.</span><span class="n">t</span>
    <span class="o">(</span><span class="n">sstatTerm</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">toArray</span><span class="o">.</span><span class="n">grouped</span><span class="o">(</span><span class="n">sstatTerm</span><span class="o">.</span><span class="n">cols</span><span class="o">).</span><span class="n">toArray</span><span class="o">.</span><span class="n">zip</span><span class="o">(</span><span class="n">wordIDs</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">x</span><span class="o">.</span><span class="n">_2</span><span class="o">,</span> <span class="n">x</span><span class="o">.</span><span class="n">_1</span><span class="o">)),</span> <span class="n">gammaD</span><span class="o">)</span>
  <span class="o">}</span>
</pre></div>


<p>However, currently our minibatch RDD only contains lists of <code>(wordId, frequency)</code> tuples (i.e. bag-of-words format).  We need to somehow combine our current minibatch RDD with the distributed topic/word matrix to get lists of <code>(wordId, frequency, row)</code> 3-tuples in order to be able to apply our new <code>map</code> function to it.  This can be done in the following way.</p>
<ul>
<li>Give each document in the minibatch RDD a unique Id via the <code>zipWithIndex</code> function followed by a <code>map</code> function.</li>
<li>Apply the <code>flatMap</code> function to the RDD to produce an RDD of <code>(docId, wordId, frequency)</code> 3-tuples.</li>
<li>Use the <code>join</code> function to join this RDD with the distributed topic/word matrix via the wordId key to produce an RDD of <code>(docId, wordId, frequency, row)</code> 4-tuples.</li>
<li>Finally use the <code>groupByKey</code> on the <code>docId</code> field to get our minibatch RDD in the correct <code>(row, wordId, frequency)</code> 3-tuple format.</li>
</ul>
<p>This process is shown in the following code.</p>
<div class="highlight"><pre><span class="k">val</span> <span class="n">rowIdCtRDD</span><span class="k">=</span><span class="n">bowRDD</span><span class="o">.</span><span class="n">zipWithIndex</span><span class="o">()</span>   <span class="c1">//give each document and Id</span>
        <span class="o">.</span><span class="n">map</span><span class="o">{</span><span class="k">case</span> <span class="o">(</span><span class="n">bow</span><span class="o">,</span><span class="n">docId</span><span class="o">)=&gt;(</span><span class="n">docId</span><span class="o">,</span><span class="n">bow</span><span class="o">)}</span>   
        <span class="o">.</span><span class="n">flatMap</span><span class="o">{</span><span class="k">case</span> <span class="o">(</span><span class="n">docId</span><span class="o">,</span> <span class="n">bow</span><span class="o">)</span><span class="k">=&gt;</span><span class="n">bow</span><span class="o">.</span><span class="n">map</span><span class="o">{</span><span class="k">case</span> <span class="o">(</span><span class="n">wordId</span><span class="o">,</span><span class="n">ct</span><span class="o">)=&gt;(</span><span class="n">docId</span><span class="o">,(</span><span class="n">wordId</span><span class="o">,</span><span class="n">ct</span><span class="o">))}}</span>  <span class="c1">// create desired tuple</span>
        <span class="o">.</span><span class="n">map</span><span class="o">{</span><span class="k">case</span> <span class="o">(</span><span class="n">docID</span><span class="o">,(</span><span class="n">wordID</span><span class="o">,</span><span class="n">ct</span><span class="o">))</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">wordID</span><span class="o">,(</span><span class="n">docID</span><span class="o">,</span><span class="n">ct</span><span class="o">))}</span>
        <span class="o">.</span><span class="n">join</span><span class="o">(</span>
          <span class="n">expELogBeta</span><span class="o">.</span><span class="n">rows</span><span class="o">.</span><span class="n">map</span><span class="o">{</span><span class="n">x</span><span class="o">=&gt;(</span><span class="n">x</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">toInt</span><span class="o">,</span><span class="n">x</span><span class="o">.</span><span class="n">vector</span><span class="o">)}</span>   <span class="c1">//join to get rows matching wordIds</span>
          <span class="o">)</span>
        <span class="o">.</span><span class="n">map</span><span class="o">{</span><span class="k">case</span> <span class="o">(</span><span class="n">wordId</span><span class="o">,((</span><span class="n">docId</span><span class="o">,</span><span class="n">ct</span><span class="o">),</span><span class="n">row</span><span class="o">))</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">docId</span><span class="o">,(</span><span class="n">row</span><span class="o">,</span><span class="n">wordId</span><span class="o">,</span><span class="n">ct</span><span class="o">))}</span>
        <span class="o">.</span><span class="n">groupByKey</span><span class="o">()</span>   <span class="c1">//group by document Id</span>
        <span class="o">.</span><span class="n">map</span><span class="o">{</span><span class="k">case</span> <span class="o">(</span><span class="n">docId</span><span class="o">,</span><span class="n">rowStats</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">rowStats</span><span class="o">.</span><span class="n">toArray</span><span class="o">}</span>
</pre></div>


<p>Now the E-Step can be performed on the minibatch RDD via a <code>map</code> operation followed by a <code>reduce</code> operation which simply sums the results by id.</p>
<div class="highlight"><pre><span class="k">val</span> <span class="n">eStepRDD</span><span class="k">=</span><span class="n">rowIdCtRDD</span>
        <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span><span class="k">=&gt;</span><span class="n">eStep</span><span class="o">(</span><span class="n">x</span><span class="o">,</span> <span class="n">numTopics</span><span class="o">,</span> <span class="n">alpha</span><span class="o">,</span> <span class="n">gammaThreshold</span><span class="o">,</span> <span class="n">iterations</span><span class="o">).</span><span class="n">_1</span><span class="o">)</span>   <span class="c1">//perform E-Step</span>
        <span class="o">.</span><span class="n">flatMap</span><span class="o">(</span><span class="n">z</span><span class="k">=&gt;</span><span class="n">z</span><span class="o">)</span>
        <span class="o">.</span><span class="n">reduceByKey</span><span class="o">(</span><span class="n">arraySum</span><span class="o">)</span>  <span class="c1">//sum results by rowId</span>
</pre></div>


<h2>A Distributed M-Step</h2>
<p>In the original non-distributed implementation the M-Step was easy.  The new global topc/word matrix was computed by a simple weighted sum of the previous topic\word matrix and the output of the E-Step for the current minibatch (which were both local matrices).  It is a bit more complicated with the distributed implementation because the topic/word matrix is distributed as is the result of the E-step.  Furthermore, the result of the E-step does not contain all rows of the topic/word matrix, only those corresponding to the words present in the current minibatch.  However, we can still compute the same weighted sum with a <code>leftOuterJoin</code> based on the indexes of the rows of the topic/word matrix and the result of the E-step followed by a <em>map</em> that sums the joined rows.  This is shown in the following code.</p>
<div class="highlight"><pre><span class="k">val</span> <span class="n">mStepRDD</span><span class="k">=</span><span class="n">sstatsRM</span><span class="o">.</span><span class="n">rows</span>
        <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span><span class="o">=&gt;(</span><span class="n">x</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">toInt</span><span class="o">,</span><span class="n">x</span><span class="o">.</span><span class="n">vector</span><span class="o">.</span><span class="n">toArray</span><span class="o">))</span>   <span class="c1">//get matrix rows</span>
        <span class="o">.</span><span class="n">leftOuterJoin</span><span class="o">(</span>  <span class="c1">//join with E-step result by rowId</span>
          <span class="n">eStepRDD</span>
            <span class="o">.</span><span class="n">map</span><span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">rowId</span><span class="o">,</span><span class="n">row</span><span class="o">)</span><span class="k">=&gt;</span>
            <span class="o">(</span><span class="n">rowId</span><span class="o">,</span><span class="n">blend</span><span class="o">(</span><span class="n">rho</span><span class="o">,</span><span class="n">row</span><span class="o">,</span><span class="n">mbSize</span><span class="o">.</span><span class="n">toInt</span><span class="o">,</span> <span class="n">threadsSeen</span><span class="o">))}</span>  
        <span class="o">)</span>
        <span class="o">.</span><span class="n">map</span><span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">rowId</span><span class="o">,(</span><span class="n">oldRow</span><span class="o">,</span> <span class="n">newRow</span><span class="o">))</span><span class="k">=&gt;</span>
          <span class="o">(</span><span class="n">rowId</span><span class="o">,</span><span class="n">optionArraySum</span><span class="o">(</span><span class="n">oldRow</span><span class="o">,</span><span class="n">newRow</span><span class="o">))}</span>  <span class="c1">//sum matching rows</span>
        <span class="o">.</span><span class="n">join</span><span class="o">(</span>
          <span class="n">expELogBeta</span><span class="o">.</span><span class="n">rows</span>
            <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span><span class="o">=&gt;(</span><span class="n">x</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">toInt</span><span class="o">,</span><span class="n">x</span><span class="o">.</span><span class="n">vector</span><span class="o">.</span><span class="n">toArray</span><span class="o">))</span>
        <span class="o">)</span>
        <span class="o">.</span><span class="n">map</span><span class="o">{</span><span class="k">case</span> <span class="o">(</span><span class="n">rowId</span><span class="o">,(</span><span class="n">oldRow</span><span class="o">,</span> <span class="n">newRow</span><span class="o">))</span> <span class="k">=&gt;</span>
        <span class="o">(</span><span class="n">rowId</span><span class="o">,</span><span class="n">arrayElMultiply</span><span class="o">(</span><span class="n">oldRow</span><span class="o">,</span><span class="n">newRow</span><span class="o">))}</span>  <span class="c1">//also elementwise multiply with previous matrix</span>
</pre></div>


<p>As you can see, this is exactly the same as performing a weighted sum of two matrices, it just takes a bit more work when they are both distributed.</p>
<h1>The Full Algorithm</h1>
<p>Now we are at a point at which we can describe the full algorithm.  For each minibatch of documents, the following steps are taken.</p>
<ol>
<li>The minibatch RDD is transformed into its bag-of-words form.</li>
<li>The minibatch RDD is joined with the topic/word matrix RDD to get the rows corresponding to the words in the minibatch.</li>
<li>The E-Step function is applied to the minibatch RDD.</li>
<li>The M-Step function is applied to the output RDD of the previous step.</li>
<li>The rows of the topic/word matrix RDD corresponding to the words in the minibatch are updated.</li>
</ol>
<p>These steps are repeated for each minibatch.  The following diagram illustrates this process.</p>
<p><img alt="full algorithm" src="images/full_algorithm.png" /> </p>
<h1>Code Demo</h1>
<p>Now let's try this algorithm on a real dataset of documents.  Let's use the NIPS dataset that was used in the code demo from my previous post on online LDA.  In order to actually implement this algorithm, we must have a way to to iterate over minibatches.  The minibatch iterator should be implemented by the user since it is dependent on how the documents are stored. The following is the code which implements online LDA in Spark (with the minibatch iterator and Spark context code omitted).</p>
<div class="highlight"><pre><span class="k">while</span> <span class="o">(</span><span class="n">mbIterator</span><span class="o">.</span><span class="n">hasNext</span><span class="o">)</span> <span class="o">{</span>

    <span class="k">val</span> <span class="n">mb</span> <span class="k">=</span> <span class="n">mbIterator</span><span class="o">.</span><span class="n">next</span>

    <span class="k">val</span> <span class="n">mbSize</span> <span class="k">=</span> <span class="n">mb</span><span class="o">.</span><span class="n">count</span><span class="o">()</span>

    <span class="n">threadsSeen</span> <span class="o">+=</span> <span class="n">mbSize</span><span class="o">.</span><span class="n">toInt</span>
    <span class="n">numUpdates</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">rho</span> <span class="k">=</span> <span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="o">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">numUpdates</span><span class="o">,</span> <span class="o">-</span><span class="n">decay</span><span class="o">)</span>

    <span class="c1">//raw text to bag-of-words</span>
    <span class="k">val</span> <span class="n">bowRDD</span> <span class="k">=</span> <span class="n">mb</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="n">toBagOfWords</span><span class="o">(</span><span class="n">x</span><span class="o">,</span> <span class="n">vocab</span><span class="o">))</span>

    <span class="c1">//preprocess sstats matrix</span>
    <span class="k">val</span> <span class="n">sstatsPlusEta</span> <span class="k">=</span> <span class="nc">RmElementwiseAdd</span><span class="o">(</span><span class="n">sstatsRM</span><span class="o">,</span> <span class="n">eta</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">expELogBeta</span> <span class="k">=</span> <span class="n">matrixExp</span><span class="o">(</span><span class="n">dirichletExpectation</span><span class="o">(</span><span class="n">sstatsPlusEta</span><span class="o">))</span>

    <span class="c1">//preprocess minibatch RDD</span>
    <span class="k">val</span> <span class="n">rowIdCtRDD</span> <span class="k">=</span> <span class="n">bowRDD</span><span class="o">.</span><span class="n">zipWithIndex</span><span class="o">()</span>
      <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">bow</span><span class="o">,</span> <span class="n">docId</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">docId</span><span class="o">,</span> <span class="n">bow</span><span class="o">)}</span>
      <span class="o">.</span><span class="n">flatMap</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">docId</span><span class="o">,</span> <span class="n">bow</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">bow</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">wordId</span><span class="o">,</span> <span class="n">ct</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">docId</span><span class="o">,</span> <span class="o">(</span><span class="n">wordId</span><span class="o">,</span> <span class="n">ct</span><span class="o">))}}</span>
      <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">docID</span><span class="o">,</span> <span class="o">(</span><span class="n">wordID</span><span class="o">,</span> <span class="n">ct</span><span class="o">))</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">wordID</span><span class="o">,</span> <span class="o">(</span><span class="n">docID</span><span class="o">,</span> <span class="n">ct</span><span class="o">))}</span>
      <span class="o">.</span><span class="n">join</span><span class="o">(</span>
        <span class="n">expELogBeta</span><span class="o">.</span><span class="n">rows</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">x</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">x</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">toInt</span><span class="o">,</span> <span class="n">x</span><span class="o">.</span><span class="n">vector</span><span class="o">)}</span>
      <span class="o">)</span>
      <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">wordId</span><span class="o">,</span> <span class="o">((</span><span class="n">docId</span><span class="o">,</span> <span class="n">ct</span><span class="o">),</span> <span class="n">row</span><span class="o">))</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">docId</span><span class="o">,</span> <span class="o">(</span><span class="n">row</span><span class="o">,</span> <span class="n">wordId</span><span class="o">,</span> <span class="n">ct</span><span class="o">))}</span>
      <span class="o">.</span><span class="n">groupByKey</span><span class="o">()</span>
      <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">docId</span><span class="o">,</span> <span class="n">rowStats</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">rowStats</span><span class="o">.</span><span class="n">toArray</span><span class="o">}</span>

    <span class="c1">//perform E-Step</span>
    <span class="k">val</span> <span class="n">eStepRDD</span> <span class="k">=</span> <span class="n">rowIdCtRDD</span>
      <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="n">eStep</span><span class="o">(</span><span class="n">x</span><span class="o">,</span> <span class="n">numTopics</span><span class="o">,</span> <span class="n">alpha</span><span class="o">,</span> <span class="n">gammaThreshold</span><span class="o">,</span> <span class="n">iterations</span><span class="o">).</span><span class="n">_1</span><span class="o">)</span>
      <span class="o">.</span><span class="n">flatMap</span><span class="o">(</span><span class="n">z</span> <span class="k">=&gt;</span> <span class="n">z</span><span class="o">)</span>
      <span class="o">.</span><span class="n">reduceByKey</span><span class="o">(</span><span class="n">arraySum</span><span class="o">)</span>

    <span class="c1">//perform M-Step</span>
    <span class="k">val</span> <span class="n">mStepRDD</span> <span class="k">=</span> <span class="n">sstatsRM</span><span class="o">.</span><span class="n">rows</span>
      <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">x</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">toInt</span><span class="o">,</span> <span class="n">x</span><span class="o">.</span><span class="n">vector</span><span class="o">.</span><span class="n">toArray</span><span class="o">))</span>
      <span class="o">.</span><span class="n">leftOuterJoin</span><span class="o">(</span>
        <span class="n">eStepRDD</span>
          <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">rowId</span><span class="o">,</span> <span class="n">row</span><span class="o">)</span> <span class="k">=&gt;</span>
          <span class="o">(</span><span class="n">rowId</span><span class="o">,</span> <span class="n">blend</span><span class="o">(</span><span class="n">rho</span><span class="o">,</span> <span class="n">row</span><span class="o">,</span> <span class="n">mbSize</span><span class="o">.</span><span class="n">toInt</span><span class="o">,</span> <span class="n">threadsSeen</span><span class="o">))</span>
        <span class="o">}</span>
      <span class="o">)</span>
      <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">rowId</span><span class="o">,</span> <span class="o">(</span><span class="n">oldRow</span><span class="o">,</span> <span class="n">newRow</span><span class="o">))</span> <span class="k">=&gt;</span>
      <span class="o">(</span><span class="n">rowId</span><span class="o">,</span> <span class="n">optionArraySum</span><span class="o">(</span><span class="n">oldRow</span><span class="o">,</span> <span class="n">newRow</span><span class="o">))</span>
    <span class="o">}</span>
      <span class="o">.</span><span class="n">join</span><span class="o">(</span>
        <span class="n">expELogBeta</span><span class="o">.</span><span class="n">rows</span>
          <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">x</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">toInt</span><span class="o">,</span> <span class="n">x</span><span class="o">.</span><span class="n">vector</span><span class="o">.</span><span class="n">toArray</span><span class="o">))</span>
      <span class="o">)</span>
      <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">rowId</span><span class="o">,</span> <span class="o">(</span><span class="n">oldRow</span><span class="o">,</span> <span class="n">newRow</span><span class="o">))</span> <span class="k">=&gt;</span>
      <span class="o">(</span><span class="n">rowId</span><span class="o">,</span> <span class="n">arrayElMultiply</span><span class="o">(</span><span class="n">oldRow</span><span class="o">,</span> <span class="n">newRow</span><span class="o">))</span>
    <span class="o">}</span>

    <span class="c1">//update sstats matrix</span>
    <span class="n">sstatsRM</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">IndexedRowMatrix</span><span class="o">(</span>
      <span class="n">mStepRDD</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="nc">IndexedRow</span><span class="o">(</span><span class="n">x</span><span class="o">.</span><span class="n">_1</span><span class="o">.</span><span class="n">toLong</span><span class="o">,</span> <span class="nc">Vectors</span><span class="o">.</span><span class="n">dense</span><span class="o">(</span><span class="n">x</span><span class="o">.</span><span class="n">_2</span><span class="o">)))</span>
    <span class="o">)</span>
  <span class="o">}</span>

  <span class="c1">//print learned topics</span>
  <span class="n">showTopics</span><span class="o">(</span><span class="mi">10</span><span class="o">,</span> <span class="n">sstatsRM</span><span class="o">,</span> <span class="n">id2Word</span><span class="o">)</span>
</pre></div>


<p>The above code produces the following topics</p>
<div class="highlight"><pre><span class="nx">Topic</span> <span class="mi">0</span><span class="o">:</span> <span class="nx">List</span><span class="p">((</span><span class="nx">activity</span><span class="p">,</span><span class="mf">0.032092510865724705</span><span class="p">),</span> <span class="p">(</span><span class="nx">voltage</span><span class="p">,</span><span class="mf">0.028580820005928525</span><span class="p">),</span> <span class="p">(</span><span class="nx">neurons</span><span class="p">,</span><span class="mf">0.025338836825008867</span><span class="p">),</span> <span class="p">(</span><span class="nx">cortex</span><span class="p">,</span><span class="mf">0.024546295981522796</span><span class="p">),</span> <span class="p">(</span><span class="nx">rhythmic</span><span class="p">,</span><span class="mf">0.023029320016109932</span><span class="p">),</span> <span class="p">(</span><span class="nx">wta</span><span class="p">,</span><span class="mf">0.02047031045772205</span><span class="p">),</span> <span class="p">(</span><span class="nx">tones</span><span class="p">,</span><span class="mf">0.02044761938141161</span><span class="p">),</span> <span class="p">(</span><span class="nx">obs</span><span class="p">,</span><span class="mf">0.016662670108027338</span><span class="p">),</span> <span class="p">(</span><span class="nx">analog</span><span class="p">,</span><span class="mf">0.01639142696901859</span><span class="p">),</span> <span class="p">(</span><span class="nx">cortical</span><span class="p">,</span><span class="mf">0.016364893763539945</span><span class="p">))</span>

<span class="nx">Topic</span> <span class="mi">1</span><span class="o">:</span> <span class="nx">List</span><span class="p">((</span><span class="nx">inference</span><span class="p">,</span><span class="mf">0.06811446156014736</span><span class="p">),</span> <span class="p">(</span><span class="nx">models</span><span class="p">,</span><span class="mf">0.038610276907281436</span><span class="p">),</span> <span class="p">(</span><span class="nx">data</span><span class="p">,</span><span class="mf">0.03699453925479673</span><span class="p">),</span> <span class="p">(</span><span class="nx">prior</span><span class="p">,</span><span class="mf">0.033185332552837114</span><span class="p">),</span> <span class="p">(</span><span class="nx">posterior</span><span class="p">,</span><span class="mf">0.032290300391455466</span><span class="p">),</span> <span class="p">(</span><span class="nx">probability</span><span class="p">,</span><span class="mf">0.025078686881619315</span><span class="p">),</span> <span class="p">(</span><span class="nx">parent</span><span class="p">,</span><span class="mf">0.02041949893883364</span><span class="p">),</span> <span class="p">(</span><span class="nx">parameters</span><span class="p">,</span><span class="mf">0.018027714272498996</span><span class="p">),</span> <span class="p">(</span><span class="nx">gaussian</span><span class="p">,</span><span class="mf">0.017631357263122132</span><span class="p">),</span> <span class="p">(</span><span class="nx">log</span><span class="p">,</span><span class="mf">0.017515618349787442</span><span class="p">))</span>

<span class="nx">Topic</span> <span class="mi">2</span><span class="o">:</span> <span class="nx">List</span><span class="p">((</span><span class="nx">state</span><span class="p">,</span><span class="mf">0.04652001892330229</span><span class="p">),</span> <span class="p">(</span><span class="nx">policy</span><span class="p">,</span><span class="mf">0.04113501660902407</span><span class="p">),</span> <span class="p">(</span><span class="nx">eligibility</span><span class="p">,</span><span class="mf">0.03905536641647694</span><span class="p">),</span> <span class="p">(</span><span class="nx">sarsa</span><span class="p">,</span><span class="mf">0.03905536631097504</span><span class="p">),</span> <span class="p">(</span><span class="nx">truncated</span><span class="p">,</span><span class="mf">0.03326530692419029</span><span class="p">),</span> <span class="p">(</span><span class="nx">traces</span><span class="p">,</span><span class="mf">0.03142425104787902</span><span class="p">),</span> <span class="p">(</span><span class="nx">memoryless</span><span class="p">,</span><span class="mf">0.027722772212976254</span><span class="p">),</span> <span class="p">(</span><span class="nx">policies</span><span class="p">,</span><span class="mf">0.024839780749834842</span><span class="p">),</span> <span class="p">(</span><span class="nx">agent</span><span class="p">,</span><span class="mf">0.02303785156504393</span><span class="p">),</span> <span class="p">(</span><span class="nx">pomdps</span><span class="p">,</span><span class="mf">0.02301863583267124</span><span class="p">))</span>

<span class="nx">Topic</span> <span class="mi">3</span><span class="o">:</span> <span class="nx">List</span><span class="p">((</span><span class="nx">algorithm</span><span class="p">,</span><span class="mf">0.02135927293979804</span><span class="p">),</span> <span class="p">(</span><span class="nx">learning</span><span class="p">,</span><span class="mf">0.010459512976094452</span><span class="p">),</span> <span class="p">(</span><span class="nx">error</span><span class="p">,</span><span class="mf">0.008990052150829781</span><span class="p">),</span> <span class="p">(</span><span class="nx">weight</span><span class="p">,</span><span class="mf">0.0073065564236364155</span><span class="p">),</span> <span class="p">(</span><span class="kd">function</span><span class="p">,</span><span class="mf">0.007146485129287556</span><span class="p">),</span> <span class="p">(</span><span class="nx">vector</span><span class="p">,</span><span class="mf">0.006762208984454446</span><span class="p">),</span> <span class="p">(</span><span class="nx">number</span><span class="p">,</span><span class="mf">0.006051699485084758</span><span class="p">),</span> <span class="p">(</span><span class="o">-</span><span class="nx">rcb</span><span class="o">-</span><span class="p">,</span><span class="mf">0.005855829811131096</span><span class="p">),</span> <span class="p">(</span><span class="nx">results</span><span class="p">,</span><span class="mf">0.005837170436069406</span><span class="p">),</span> <span class="p">(</span><span class="nx">probability</span><span class="p">,</span><span class="mf">0.005788740947910415</span><span class="p">))</span>

<span class="nx">Topic</span> <span class="mi">4</span><span class="o">:</span> <span class="nx">List</span><span class="p">((</span><span class="nx">attentional</span><span class="p">,</span><span class="mf">0.031205396268822388</span><span class="p">),</span> <span class="p">(</span><span class="nx">image</span><span class="p">,</span><span class="mf">0.030771091189727498</span><span class="p">),</span> <span class="p">(</span><span class="nx">location</span><span class="p">,</span><span class="mf">0.02025190023001232</span><span class="p">),</span> <span class="p">(</span><span class="nx">images</span><span class="p">,</span><span class="mf">0.01848811029404064</span><span class="p">),</span> <span class="p">(</span><span class="nx">target</span><span class="p">,</span><span class="mf">0.01788585445419973</span><span class="p">),</span> <span class="p">(</span><span class="nx">streams</span><span class="p">,</span><span class="mf">0.014740100512122468</span><span class="p">),</span> <span class="p">(</span><span class="nx">field</span><span class="p">,</span><span class="mf">0.014659463334666768</span><span class="p">),</span> <span class="p">(</span><span class="nx">tones</span><span class="p">,</span><span class="mf">0.014100436801905392</span><span class="p">),</span> <span class="p">(</span><span class="nx">bottom</span><span class="o">-</span><span class="nx">up</span><span class="p">,</span><span class="mf">0.013031110714897202</span><span class="p">),</span> <span class="p">(</span><span class="nx">dts</span><span class="p">,</span><span class="mf">0.012683978720498826</span><span class="p">))</span>
</pre></div>


<p>As you can see, these topics seem quite coherent and roughly correspond to different fields of machine learning.</p>
<h1>References</h1>
<ul>
<li><a href="https://github.com/alexminnaar/SparkOnlineLDA">Github repo</a> containing example code.</li>
<li><a href="http://alexminnaar.com/online-latent-dirichlet-allocation-the-best-option-for-topic-modeling-with-large-data-sets.html">Previous post</a> on online LDA.</li>
<li><a href="http://spark.apache.org/docs/1.2.1/programming-guide.html">Spark programming guide</a>.</li>
</ul>
                    </article>
                </div>
            </aside><!-- /#featured -->
            
        
        

    
            <aside id="featured">
                <div class="body">
                    <article>
                        <h1 class="entry-title"><a href="/scalaner-a-scala-wrapper-for-the-stanford-ner-tool-with-some-added-features.html">ScalaNER: A Scala Wrapper for the Stanford NER Tool with Some Added Features</a></h1>
<div class="post-info">
	<ul>
        <li class="vcard author">
                 by&nbsp;<a class="url fn" href="/author/alex-minnaar.html">Alex Minnaar</a>
        </li>
        <li class="published" title="2014-11-11T00:00:00+01:00">
          on&nbsp;Tue 11 November 2014
        </li>

	</ul>
<p>Category: <a href="/tag/nlp.html">   NLP</a><a href="/tag/software-engineering.html">   Software Engineering</a></p>
</div><!-- /.post-info --><p>The <a href="http://nlp.stanford.edu/software/CRF-NER.shtml">Stanford NER (named entity recognizer) tool</a> is a widely-used, general purpose named entity recognition tool that Stanford has made available as part of its CoreNLP Java library.  It performs named entity recognition via a CRF-based sequence model which has been known to give near state-of-the-art performance results which makes it a popular choice for open-source NER tools.</p>
<p>Having said that, I have used this tool in the past and I was left wanting more functionality.  From its <a href="http://nlp.stanford.edu/software/crf-faq.shtml">FAQ section</a>, you can see that most of its functionality (i.e. training and testing a NER model) is designed to be performed using the command line.  But what if you want to use a pre-trained NER model as part of a real-time text processing pipeline?  For example, you are processing a string of text and you want to apply your NER model to the text and then do something with the tokens corresponding to classified named entities.  There is no clear way to do this with the Stanford NER tool.</p>
<p>I have also found that the Stanford NER tool is lacking in its model validation functionality.  Just like any classification model, I want to be able to perform cross-validation tests on my training data so that I can be confident in its generalized performance.  Again, there is no clear way of doing this.  You can test your model on a test set and obtain the precision, recall and F1 values but unfortunately these values are just shown in standard output and there is no way to persist them.  Consequently, if you wanted to perform 50-fold cross-validation on your dataset you would have to visually read each of the 50 sets of performance metrics off the screen and then manually average them to get your desired result (or export standard output and parse it).  Obviously no one wants to do this.</p>
<p><a href="https://github.com/alexminnaar/ScalaNER">ScalaNER</a> attempts to solve these problems by offering the following additional functionality to the Stanford NER tool.</p>
<ul>
<li>Programmatically apply a pre-trained NER model to a string of text and output the labelled result.</li>
<li>Programmatically train an NER model.</li>
<li>Easy model validation.  Specifically cross-validation.</li>
</ul>
<h2>ScalaNER Demo</h2>
<p>The following code samples demonstrate this new functionality.  First of all, it should be noted that the training data sets must be in the same format that the Stanford NER tool accepts.  That is, each line must contain a tab-separated token/label pair.  Entity labels can be any string but non-entity labels must be "O".  For example, training data for a person name entity might look like</p>
<div class="highlight"><pre>The    O
US    O
president    O
is    O
Barrack    NAME
Obama    NAME
</pre></div>


<p>where named entities are labelled as "NAME" and non-entity tokens are labelled as "O".</p>
<h3>Train an NER Model</h3>
<p>First we will demonstrate how to train a NER model given a training dataset in the format explained above.  The code is very simple - in fact it is only one line.  It uses a Scala object called <code>NERModel</code>.  To train an NER model you simply call this object's <code>trainClassifier</code> method which takes two arguments, the location of the training data file (it must be a text file) and the filename and location where the the trained NER model will be saved.</p>
<div class="highlight"><pre><span class="nc">NERModel</span><span class="o">.</span><span class="n">trainClassifier</span><span class="o">(</span><span class="s">&quot;my/data/location.txt&quot;</span><span class="o">,</span> <span class="s">&quot;save/my/model/here.ser.gz&quot;</span><span class="o">)</span>
</pre></div>


<h3>Apply an NER Model</h3>
<p>Then once you have trained your NER model you will probably want to apply this model to some new text.  To do this we use the <code>ApplyModel</code> class which takes the location of the trained model as a constructor.  Once this class has been instantiated, we call its <code>runNER</code> method which takes a string as an input argument.  This input string is the text from which you want to extract the named entities.  The result is an indexed sequence of <code>LabeledToken</code> objects which contain a token field and a label field.  The token fields contain the tokens in the input string and the label fields contain the named entities that the tokens have been assigned to.</p>
<div class="highlight"><pre><span class="k">val</span> <span class="n">classifier</span><span class="k">=new</span> <span class="nc">ApplyModel</span><span class="o">(</span><span class="s">&quot;my/pretrained/model.ser.gz&quot;</span><span class="o">)</span>

<span class="k">val</span> <span class="n">results</span><span class="k">=</span><span class="n">classifier</span><span class="o">.</span><span class="n">runNER</span><span class="o">(</span><span class="s">&quot;Find named entities in this new sentence.&quot;</span><span class="o">)</span>
</pre></div>


<h3>Performing Cross-Validation on an NER Model</h3>
<p>To perform cross-validation we use the CrossValidation class which takes the number of folds and training data location as constructors.  Then we call the <code>runCrossValidation</code> method with an input parameter that is the location of the directory where the training and validation sets will be written.  The result is a vector whose elements correspond to the number of folds.  Each element is a map whose keys represent the unique entity types in that fold and values represent the precision, recall and F1-score of the corresponding entity type.</p>
<div class="highlight"><pre><span class="k">val</span> <span class="n">testInstance</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">CrossValidation</span><span class="o">(</span><span class="mi">5</span><span class="o">,</span> <span class="s">&quot;location/of/training/data.txt&quot;</span><span class="o">)</span>

<span class="k">val</span> <span class="n">xvalResults</span><span class="k">=</span><span class="n">classifier</span><span class="o">.</span><span class="n">runCrossValidation</span><span class="o">(</span><span class="s">&quot;directory/to/write/xvalidation/data/&quot;</span><span class="o">)</span>
</pre></div>


<p>Next let's look at a real-world example.</p>
<h2>Example: Identifying Protein Names</h2>
<p>Suppose that you wanted to train an NER model to identify protein named in bio-medical literature.  We will use the BioNLP dataset that has already been transformed into the correct Stanford NER format which can be found in the <a href="https://github.com/alexminnaar/ScalaNER/tree/master/data">ScalaNER github repo</a>.</p>
<p>First let's try training an NER model with this data and running it on a sample string of text to determine if it contains any protein names.</p>
<div class="highlight"><pre><span class="nc">NERModel</span><span class="o">.</span><span class="n">trainClassifier</span><span class="o">(</span><span class="s">&quot;data/bionlp.txt&quot;</span><span class="o">,</span> <span class="s">&quot;/bioNlpModel.ser.gz&quot;</span><span class="o">)</span>

<span class="k">val</span> <span class="n">classifier</span><span class="k">=new</span> <span class="nc">ApplyModel</span><span class="o">(</span><span class="s">&quot;/bioNlpModel.ser.gz&quot;</span><span class="o">)</span>

<span class="k">val</span> <span class="n">results</span><span class="k">=</span><span class="n">classifier</span><span class="o">.</span><span class="n">runNER</span><span class="o">(</span><span class="s">&quot;Leukotriene B4 stimulates c-fos and c-jun gene transcription and AP-1 binding activity in human monocytes.&quot;</span><span class="o">)</span>

<span class="n">println</span><span class="o">(</span><span class="n">results</span><span class="o">)</span>
</pre></div>


<p>Which gives the following output</p>
<p><code>Vector(LabeledToken(Leukotriene,O), LabeledToken(B4,O), LabeledToken(stimulates,O), LabeledToken(c-fos,protein), LabeledToken(and,O), LabeledToken(c-jun,protein), LabeledToken(gene,O), LabeledToken(transcription,O), LabeledToken(and,O), LabeledToken(AP-1,O), LabeledToken(binding,O), LabeledToken(activity,O), LabeledToken(in,O), LabeledToken(human,O), LabeledToken(monocytes,O), LabeledToken(.,O))</code></p>
<p>As you can see, the trained model assigns the correct <em>protein</em> label to the tokens "c-fos" and "c-jun" and all other tokens are assigned the <em>O</em> label indicating that they are not named entities.</p>
<p>Next, let's perform 5-fold cross-validation on the entire dataset to get a good idea of its generalized performance.  This can be done in the following code where we specify the folder "data/xval" to be location where the 5 training and validation sets will be written.</p>
<div class="highlight"><pre><span class="k">val</span> <span class="n">cv</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">CrossValidation</span><span class="o">(</span><span class="mi">5</span><span class="o">,</span> <span class="s">&quot;data/bionlp.txt&quot;</span><span class="o">)</span>

<span class="k">val</span> <span class="n">testResults</span> <span class="k">=</span> <span class="n">cv</span><span class="o">.</span><span class="n">runCrossValidation</span><span class="o">(</span><span class="s">&quot;data/xval&quot;</span><span class="o">)</span>

<span class="n">println</span><span class="o">(</span><span class="n">testResults</span><span class="o">)</span>
</pre></div>


<p>Which gives the following output</p>
<p><code>Vector(Map(protein -&gt; Performance(0.680461329715061,0.9862340216322517,0.8052990766760337), O -&gt; Performance(0.999634483838964,0.9878479836941098,0.9937062846316554)), Map(O -&gt; Performance(0.9991162403826159,0.9858425237240318,0.9924350003872866), protein -&gt; Performance(0.5766871165644172,0.9567430025445293,0.7196172248803828)), Map(O -&gt; Performance(0.9986442092089483,0.9858183409260546,0.9921898273472612), protein -&gt; Performance(0.6125175808720112,0.9436619718309859,0.7428571428571428)), Map(O -&gt; Performance(0.9994266652767643,0.9878419452887538,0.9936005389019872), protein -&gt; Performance(0.6638176638176638,0.9769392033542977,0.7905004240882104)), Map(O -&gt; Performance(0.9988831168831169,0.9877484974572354,0.9932846036624738), protein -&gt; Performance(0.6261755485893417,0.9489311163895487,0.7544853635505192)))</code></p>
<p>The above output shows the precision, recall and F1-scores for each entity type (in this case protein and O) and each of the 5 folds.  So the F1-scores associated with identifying <em>protein</em> named entities are 0.8052, 0.7196, 0.7428, 0.7905, and 0.7544 for an average F1-score of 0.7625.</p>
<h2>References</h2>
<ul>
<li><a href="http://nlp.stanford.edu/software/CRF-NER.shtml">Stanford Named Entity Recognizer</a></li>
<li><a href="https://github.com/alexminnaar/ScalaNER">ScalaNER Github Repo</a></li>
</ul>
                    </article>
                </div>
            </aside><!-- /#featured -->
            
        
        

    
            <aside id="featured">
                <div class="body">
                    <article>
                        <h1 class="entry-title"><a href="/online-latent-dirichlet-allocation-the-best-option-for-topic-modeling-with-large-data-sets.html">Online Latent Dirichlet Allocation - The Best Option for Topic Modeling with Large Data Sets</a></h1>
<div class="post-info">
	<ul>
        <li class="vcard author">
                 by&nbsp;<a class="url fn" href="/author/alex-minnaar.html">Alex Minnaar</a>
        </li>
        <li class="published" title="2014-10-14T00:00:00+02:00">
          on&nbsp;Tue 14 October 2014
        </li>

	</ul>
<p>Category: <a href="/tag/bayesian-inference.html">   Bayesian Inference</a><a href="/tag/nlp.html">   NLP</a></p>
</div><!-- /.post-info --><p>By now it has become very clear that Latent Dirichlet Allocation (LDA) has a variety of valuable, real-world use cases.  However, most real-world use cases involve large volumes of data which can be problematic for LDA.  This is because both of the traditional implementations of LDA (variational inference and collapsed Gibbs sampling) require the entire corpus (or some encoding of it) to be loaded into main memory.  Obviously, if you are working with a single machine and a data set that is sufficiently large, this can be infeasible.  One solution is to parallelize the algorithm and scale out until you have the required resources.  However, this presents an entire new set of problems - acquiring a cluster of machines, modifying your LDA code such that it can work in a MapReduce framework, etc.  A much better solution would be to segment your large data set into small batches and sequentially read each of these batches into main memory and update your LDA model as you go in an online fashion.  This way you are only keeping a small fraction of your large data set in main memory at any given time.  Furthermore, consider a scenario where your corpus is constantly growing such as an online discussion forum.  As your corpus grows you want to see how the topics are changing.  With traditional variational inference you would have to rerun the entire batch algorithm with the old data and the new data but it would be much more efficient to simply update your model with only the new data.  In their paper <a href="https://www.google.ca/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0CDEQFjAA&amp;url=https%3A%2F%2Fwww.cs.princeton.edu%2F~blei%2Fpapers%2FHoffmanBleiBach2010b.pdf&amp;ei=b4bvU-2_K433yQSbh4LoCw&amp;usg=AFQjCNHLmU8Gk_P4usBj2QcGcaolw87w4w&amp;sig2=cbFKirN-0jWvPj-mCqRP_g&amp;bvm=bv.73231344,d.aWw">Online Learning for Latent Dirichlet Allocation</a>, Blei et al. present an algorithm for achieving this kind of functionality.  This blog post aims to give a summary of this paper and also show some results from my own Scala implementation.</p>
<h2>Variational Inference vs Stochastic Variational Inference</h2>
<p>Let's start off with a very general graphical model that includes observations, local hidden variables, and global hidden variables.</p>
<p><img alt="general graphical model" src="images/general_graphical_model.png" /> </p>
<p>In the above graphical model, there are <span class="math">\(N\)</span> observations <span class="math">\(x_{1:N}\)</span> and one local hidden variable for each observation <span class="math">\(z_{1:N}\)</span>.  There are also global hidden variables <span class="math">\(\beta\)</span> with a known prior <span class="math">\(\alpha\)</span>.  I will first compare variation inference with stochastic variational inference in the context of this graphical model because it can be generalized to a variety of different graphical models with local and global hidden variables e.g. the LDA model, Gaussian mixture models, hidden Markov models and many more.</p>
<p>The joint distribution of this graphical model is</p>
<p><span class="math">\(p(x,z,\beta | \alpha)=p(\beta | \alpha)\prod_{n=1}^Np(x_n,z_n | \beta)\)</span></p>
<p>Also, our assumption that <span class="math">\(\beta\)</span> are the global parameters and <span class="math">\(z_n\)</span> are the local parameters is formalized by the fact that the observation <span class="math">\(x_n\)</span> and the corresponding local variable <span class="math">\(z_n\)</span> are conditionally independent given the global variables <span class="math">\(\beta\)</span> i.e.</p>
<p><span class="math">\(p(x_n,z_n|x_{-n},z_{-n},\beta,\alpha)=p(x_n,z_n|\beta,\alpha)\)</span></p>
<p>Another assumption that we make in this model is that the conditional distributions of the hidden variables given the observations and other hidden variables (also called the complete conditionals) are in the exponential family which means they take the general form</p>
<p><span class="math">\(p(\beta | x,z,\alpha)=h(\beta)\exp(\eta_g (x,z,\alpha)^Tt(\beta)-a_g(\eta_g(x,z,\alpha)))\)</span></p>
<p><span class="math">\(p(z_{nj}|x_n,z_{n,-j},\beta)=h(z_{nj})\exp(\eta_l(x_n,z_{n,-j},\beta)^Tt(z_{nj})-\alpha_l(\eta_l(x_n,z_{n,-j},\beta)))\)</span></p>
<p>Where <span class="math">\(h(.)\)</span> is called the base measure, <span class="math">\(a(.)\)</span> is called the log-normalizer, <span class="math">\(\eta(.)\)</span> is called the natural parameter, and <span class="math">\(t(.)\)</span> is called the sufficient statistics.  Furthermore, we also assume that the prior distribution over <span class="math">\(\beta\)</span> is part of the exponential family.</p>
<p><span class="math">\(p(\beta)=h(\beta)\exp(\alpha^Tt(\beta)-a_g(\alpha))\)</span></p>
<p>The main goal is to compute the posterior distribution of the hidden variables given the observed variables</p>
<p>
<div class="math">$$p(\beta,z|x)=\frac{p(x,z,\beta)}{\int p(x,z,\beta)dz d \beta}$$</div>
</p>
<p>Unfortunately, the denominator <span class="math">\(\int p(x,z,\beta)dz d \beta\)</span> is intractable to compute so we must use approximate inference techniques such as variational inference.</p>
<h3>Variational Inference</h3>
<p>Variational inference turns the inference problem into an optimization problem.  A new distribution over the hidden variables <span class="math">\(q(z,\beta)\)</span> (called the variational distribution) is introduced.  This new distribution has properties such that it can be efficiently computed.  The variational distribution is a function of a set of free parameters that are optimized such that the variational distribution is as close as possible to the actual target posterior distribution where closeness is measured in terms of KL divergence.  Minimizing the KL divergence between the variational distribution and the target posterior is equivalent to maximizing the evidence lower bound (ELBO) (proof not shown here) which is</p>
<p>
<div class="math">$$\mathscr{L}(q)=E_q[\log p(x,z,\beta)]-E_q[\log q(z,\beta)]$$</div>
</p>
<p>As stated earlier, the variational distribution has the property that it can be efficiently computed.  This is done by making each hidden variable independent of each other.</p>
<p><span class="math">\(q(z,\beta)=q(\beta|\lambda)\prod_{n=1}^N \prod_{j=1}^J q(z_{nj}|\phi_{nj})\)</span></p>
<p>Furthermore, each hidden variable is governed by its own variational parameter so the variational parameters <span class="math">\(\lambda\)</span> governs the global variables <span class="math">\(\beta\)</span> and the variational parameters <span class="math">\(\phi_n\)</span> govern the local variables <span class="math">\(z_n\)</span>.  <span class="math">\(q(\beta | \lambda)\)</span> and <span class="math">\(q(z_{nj} | \phi_{nj})\)</span> take the same form as the complete conditionals <span class="math">\(p(\beta | x,z,\alpha)\)</span> and <span class="math">\(p(z_{nj}|x_n,z_{n,-j},\beta)\)</span>, but the natural parameters are now <span class="math">\(\lambda\)</span> and <span class="math">\(\phi_{nj}\)</span>, respectively to give</p>
<p><span class="math">\(q(\beta | \lambda)=h(\beta)\exp(\lambda^Tt(\beta)-a_g(\lambda))\)</span></p>
<p><span class="math">\(q(z_{nj}|\phi_{nj})=h(z_{nj})\exp(\phi_{nj}^Tt(z_{nj})-a_l(\phi_{nj}))\)</span></p>
<p>We maximize the ELBO objective function with a coordinate ascent procedure.  We find its gradient with respect to the global variational parameter <span class="math">\(\lambda\)</span> and find the value of <span class="math">\(\lambda\)</span> that sets the gradient to zero.  We do the same thing for the local parameters <span class="math">\(\phi_{n}\)</span>.  We iterate between these updates until we converge to the maximum of the ELBO.  The updates are given without proof, but the general procedure is to write the ELBO in terms of parameter of interest (either <span class="math">\(\lambda\)</span> or <span class="math">\(\phi_n\)</span>) then take the gradient and set it to zero.</p>
<p>
<div class="math">$$\lambda=E_q[\eta_g(x,z,\alpha)]$$</div>
</p>
<p>
<div class="math">$$\phi_{nj}=E_q[\eta_l(x_n,z_{n,-j},\beta)]$$</div>
</p>
<p>Therefore the updates of each variational parameter are equal to the expected value of the natural parameters of the complete conditionals.  The complete coordinate ascent algorithm is given below.</p>
<p><img alt="variational inference" src="images/variational_inference.png" /> </p>
<p>As you can see, in the local parameter update (steps 3 and 4), we have to iterate over every data point in the data set which is computationally expensive and (as we will see later) not necessary.</p>
<h3>Stochastic Variational Inference</h3>
<p>Stochastic variational inference uses a stochastic optimization technique to sequentially maximize the ELBO using unbiased samples from the data set.  Updates are performed with the following formula</p>
<p><span class="math">\(\lambda^{(t)}=\lambda^{(t-1)}+\rho_tb_t(\lambda^{(t-1)})\)</span></p>
<p>where <span class="math">\(b_t\)</span> is a noisy (but unbiased) gradient of the objective function obtained from a subsample of the data set.  If the step size <span class="math">\(\rho_t\)</span> satisfies the following constraints</p>
<p><span class="math">\(\sum \rho_t=\infty\)</span>,   <span class="math">\(\sum \rho_t^2 &lt; \infty\)</span></p>
<p>then it is guaranteed to converge to the global optimum <span class="math">\(\lambda^*\)</span> if the objective function is convex, or a local optima if it is not convex.  Now let's look at how this noisy gradient can be computed for a single data point.  First we write the ELBO in terms of a global term and a sum of local terms.</p>
<p><span class="math">\(\mathcal{L}(\lambda)=E_q[\log p(\beta)]-E_q[\log q(\beta)]+\sum^N_{n=1}\max(E_q[\log p(x_n,z_n | \beta)]-E_q[\log q(z_n)])\)</span></p>
<p>Consider a randomly chosen data point index <span class="math">\(I\)</span> sampled from <span class="math">\(Unif(1,...,N)\)</span>. For this data point <span class="math">\(x_{I}\)</span> let us define</p>
<p><span class="math">\(\mathcal{L}_I(\lambda)=E_q[\log p(\beta)]-E_q[\log q(\beta)]+N \max(E_q[\log p(x_I,z_I | \beta)]-E_q[\log q(z_I)])\)</span></p>
<p>This is equivalent to the original ELBO if the entire data set was made up of <span class="math">\(x_{I}\)</span>.  There are two important facts that one must understand about <span class="math">\(\mathcal{L}_I(\lambda)\)</span></p>
<p>The expectation of <span class="math">\(\mathcal{L}_I(\lambda)\)</span> with respect to the data point <span class="math">\(x_{I}\)</span> is equivalent to the original ELBO.
As a consequence, the gradient of <span class="math">\(\mathcal{L}_I(\lambda)\)</span> can be thought of as a noisy gradient of the original ELBO because it is unbiased.
However, we do not want to take the usual gradient of <span class="math">\(\mathcal{L}_I(\lambda)\)</span>.  Instead we want to take the natural gradient.  The usual gradient assumes that the parameter space is Euclidean but it turns out that it is better to assume that it has a Riemannian metric structure (in the context of minimizing KL divergence) which is what the natural gradient does.  A full explanation of the natural gradient is beyond the scope of this blog post but this paper by Amari gives a good overview.  The The natural gradient of  <span class="math">\(\mathcal{L}_I(\lambda)\)</span> is</p>
<p><span class="math">\(\nabla \mathcal{L}_i=E_q[\eta_g(x_i^{(N)},z_i^{(N)},\alpha)]-\lambda\)</span></p>
<p>and setting this gradient to zero gives the update</p>
<p>
<div class="math">$$\lambda=E_q[\eta_g(x_i^{(N)},z_i^{(N)},\alpha)]$$</div>
</p>
<p>The full algorithm is shown below</p>
<p><img alt="stochastic variational" src="images/stochastic_variational.png" /> </p>
<p>The procedure consists of sampling a single data point then finding the optimal local parameters for that data point then updating the global  variational parameters under the assumption that the entire dataset consisted of <span class="math">\(N\)</span> replicas of that data point.  Then this "intermediate" global variational parameter is combined with the previous "overall" global parameter via a weighted average to produce a new "overall" global parameter.  In this way, the global parameters can be updated after each sample is seen, rather than once after each iteration over the entire data set as in traditional variational inference (however, in pactice "mini-batches" of data points are used rather than a single point).  Now let's finally see how we can apply stochastic variational inference to Latent Dirichlet Allocation to get a scalable online learning algorithm.</p>
<h2>Stochastic Variational Inference and LDA</h2>
<p>Topic models aim to uncover the hidden thematic coherent topics that exist in a corpus of documents.  The most popular topic model is Latent Dirichlet Allocation (LDA).  In LDA, documents are thought of as distributions over topics and the topics themselves are distributions over words.  The graphical model for LDA can be thought of as a special case of the general graphical model shown earlier.  In the case of LDA, the global parameters are the topic distributions <span class="math">\(\beta\)</span>  which all documents depend on and the local parameters are the document-topic proportions <span class="math">\(\theta\)</span> which are independent between documents and <span class="math">\(Z\)</span> the topic assignments for each word in the document. So, in this context, "local" refers to document-specific variables and "global" refers to corpus-specific variables. The observed variables are the words that appear in each document (in bag-of-words format).  LDA is explained in greater detail in my previous blog post on the subject but here is the graphical model as a reminder.</p>
<p><img alt="lda graphical model" src="images/smoothed_lda.png" /> </p>
<p>In terms of notation, let's assume there are <span class="math">\(N\)</span> unique words in the vocabulary, <span class="math">\(D\)</span> documents in the corpus, and <span class="math">\(K\)</span> topics.  The next step is defining the complete conditionals for the LDA model (i.e. the distributions of each variable given all of the other variables both hidden and observed).  The complete conditionals for the local topic assignments <span class="math">\(Z\)</span>, the local topic proportions <span class="math">\(\theta\)</span>, and global topic distributions <span class="math">\(\beta\)</span> are</p>
<p><span class="math">\(p(z_{dn} | \theta_d,\phi_{1:K},w_{dn}) \propto \exp(\log \theta_{dk} + \log \beta_{k,w_{dn}})\)</span></p>
<p><span class="math">\(p(\theta_d | z_d)=Dirichlet(\alpha+\sum^N_{n=1}z_{dn})\)</span></p>
<p><span class="math">\(p(\beta_k | z,w)=Dirichlet(\eta +\sum^D_{d=1} \sum^N_{n=1}z^k_{dn}w_{dn})\)</span></p>
<p>where <span class="math">\(d\)</span> is the document index in the corpus, <span class="math">\(n\)</span> is the word index in the vocabulary, and <span class="math">\(k\)</span> is the topic index.  As you can see, the complete conditionals of the local variables only depend on other local variables from the same local context (i.e. the same document) and the global variables, they do not depend on local variables from other documents.  As per mean-field variational inference, the variational distributions for these variables take the same form as their complete conditionals, that is</p>
<p><span class="math">\(q(z_{dn})=Multinomial(\phi_{dn})\)</span></p>
<p><span class="math">\(q(\theta_d)=Dirichlet(\gamma_d)\)</span></p>
<p><span class="math">\(q(\beta_k)=Dirichlet(\lambda_k)\)</span></p>
<p>Next we find the updates for each of these variational parameters by taking the expectation of the natural parameters of the complete conditionals which are</p>
<p>
<div class="math">$$\phi^k_{dn}=\exp(\Psi(\gamma_{dk})+\Psi(\lambda_{k,w_{dn}})-\Psi(\sum_v \lambda_{kv}))$$</div>
</p>
<p>
<div class="math">$$\gamma_d=\alpha + \sum^N_{n=1}\phi_{dn}$$</div>
</p>
<p>
<div class="math">$$\lambda_k=\beta+\sum^D_{d=1} \sum^N_{n=1}\phi^k_{dn}w_{dn}$$</div>
</p>
<p>Now let's use the procedure mapped out in the stochastic variational inference algorithm which is to first randomly sample a document from the corpus then update the local variational parameters <span class="math">\(\phi\)</span> (the topic assignments for each word) and <span class="math">\(\gamma\)</span> (the topic proportions for the document) for this document.  Then we update the variational parameters for the global topic distribution for that sampled document <span class="math">\(\lambda\)</span>.  Then we merge the global parameter for the sampled document with the overall global parameter (with an update weighted by <span class="math">\(\rho_t\)</span>).  We repeat this procedure until we think that convergence has occurred.  This procedure is better illustrated below.</p>
<p><img alt="stochastic variational inference LDA" src="images/svi_lda.png" /> </p>
<h2>Experimental Results</h2>
<p>Scala code that implements stochastic variational inference for LDA can be found in this github repo (warning! experimental so use at your own risk).  In this experiment, we are using the NIPS dataset which consists of the abstracts of 1,736 NIPS papers however this algorithm could handle a much larger data set than this.  The following Scala code shows how we run this experiment.</p>
<div class="highlight"><pre><span class="c1">//set location of directory containing data</span>
 <span class="k">val</span> <span class="n">docDirectory</span> <span class="k">=</span> <span class="s">&quot;NIPS_dataset/&quot;</span>

<span class="c1">//create vocabulary from data with minimum frequency cutoff of 10</span>
 <span class="k">val</span> <span class="n">testVocab</span> <span class="k">=</span> <span class="nc">CountVocab</span><span class="o">(</span><span class="n">docDirectory</span><span class="o">,</span> <span class="mi">10</span><span class="o">).</span><span class="n">getVocabulary</span>

<span class="c1">//create a corpus object that can be streamed into online LDA model</span>
 <span class="k">val</span> <span class="n">testCorpus</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">StreamingCorpus</span><span class="o">(</span><span class="n">testVocab</span><span class="o">,</span> <span class="mi">5</span><span class="o">,</span> <span class="n">docDirectory</span><span class="o">)</span>

<span class="c1">//create online LDA model object with 5 topics, a decay of 0.5 and 1736 documents</span>
<span class="k">val</span> <span class="n">oldaTest</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">OnlineLDA</span><span class="o">(</span><span class="n">testCorpus</span><span class="o">,</span> <span class="mi">5</span><span class="o">,</span> <span class="mf">0.5</span><span class="o">,</span> <span class="mi">1736</span><span class="o">)</span>

<span class="c1">//learn the model</span>
 <span class="n">oldaTest</span><span class="o">.</span><span class="n">inference</span><span class="o">()</span>

<span class="c1">//show the topics by displaying the 10 most probable words in each topic</span>
 <span class="n">oldaTest</span><span class="o">.</span><span class="n">printTopics</span><span class="o">(</span><span class="mi">10</span><span class="o">)</span>
</pre></div>


<p>The resulting topics are shown below</p>
<div class="highlight"><pre>Topic 1: state, learning, reinforcement, policy, action, control, actions, states, controller, robot

Topic 2: recognition, speech, training, word, classifier, classification, classifiers, tree, words, hmm

Topic 3: cells, neurons, cell, neuron, visual, response, figure, synaptic, model, activity

Topic 4: network, learning, model, neural, data, networks, input, set, figure

Topic 5: disparity, binding, similarity, protein, structural, clause, instruction, energy, structure, spin
</pre></div>


<p>If you have some background knowledge in the machine learning domain you can see that these five topics are both distinct and thematically coherent.  The topics appear to describe five different fields of machine learning. Topic 1 seems to describe <a href="http://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a>, Topic 2 seems to describe <a href="http://en.wikipedia.org/wiki/Speech_recognition">speech recognition</a>, Topic 3 seems to describe <a href="http://en.wikipedia.org/wiki/Computational_neuroscience">neuroscience</a> in general, Topic 4 seems to describe <a href="http://en.wikipedia.org/wiki/Artificial_neural_network">neural networks</a>, and finally Topic 5 seems to describe the field of <a href="http://en.wikipedia.org/wiki/Bioinformatics">bioinformatics</a>.  Some of the topics contain redundant words but this can be reduced by preprocessing the vocabulary (eg. word stemming).</p>
<h2>TL;DR</h2>
<ul>
<li>In traditional variational inference for LDA (and variational inference in general) we must iterate through the entire data set before we can update the global parameters.  This is slow and memory intensive.  It also turns out that it is unnecessary because the entire dataset contains redundant information - instead we can iteratively update our global parameters based on small samples from the data set.  This is much less memory intensive.</li>
<li>In terms of LDA, this means that we can iteratively update our model by learning sequentially from small mini-batches of documents taken from the corpus.  This means that at any given time, we only need to keep a small mini-batch of documents in memory which means that we can scale our LDA model to an arbitrarily large corpus!</li>
<li>Take a look at my <a href="https://github.com/alexminnaar/topic-models">experimental Scala</a> code that implements this.</li>
</ul>
<h2>References</h2>
<ul>
<li><a href="https://www.google.ca/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0CB8QFjAA&amp;url=https%3A%2F%2Fwww.cs.princeton.edu%2F~blei%2Fpapers%2FHoffmanBleiBach2010b.pdf&amp;ei=Py88VJfcPIaeyASvkIDoCg&amp;usg=AFQjCNHLmU8Gk_P4usBj2QcGcaolw87w4w&amp;sig2=9dw5H6mIdznjjumz2nCY1g&amp;bvm=bv.77161500,d.aWw">Online Learning for Latent Dirichlet Allocation.  Hoffman, Blei, Bach.</a></li>
<li><a href="https://www.google.ca/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;cad=rja&amp;uact=8&amp;ved=0CCQQFjAA&amp;url=http%3A%2F%2Fwww.cs.princeton.edu%2F~blei%2Fpapers%2FHoffmanBleiWangPaisley2013.pdf&amp;ei=ci88VIuwKof5yAS2i4II&amp;usg=AFQjCNFDdMC1UFSKbIMPm8PSFWsuqnl4qg&amp;sig2=wxaruB5M5pLC5Tc_3QgxrA&amp;bvm=bv.77161500,d.aWw">Stochastic Variational Inference. Hoffman, Blei, Wang, Paisley.</a></li>
<li><a href="http://msc-ai-thesis-christiaan-meijer.googlecode.com/svn-history/trunk/Literature/Amari%20-%20Natural%20Gradient%20works%20efficiently%20in%20learning.pdf">Natural Gradient Works Efficiently in Learning. Amari.</a></li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                    </article>
 
<div class="paginator">
    <div class="navButton">Page 1 / 3</div>
        <div class="navButton"><a href="/tag/nlp2.html" >Next</a></div>
</div>
                </div>
            </aside><!-- /#featured -->
            
        
        <section id="extras" >
       
        
        </section><!-- /#extras -->
	
        <footer id="contentinfo" >
                <address id="about" class="vcard ">
                Proudly powered by <a href="http://getpelican.com/" target="_blank">Pelican</a>, which takes
                great advantage of <a href="http://python.org" target="_blank">Python</a>. &copy; <a class="url fn" href="http://launchyard.com">LaunchYard</a>
		
                </address><!-- /#about -->
		

                
        </footer><!-- /#contentinfo -->

<script type="text/javascript">
    var disqus_shortname = 'alexminnaar';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
</body>
</html>