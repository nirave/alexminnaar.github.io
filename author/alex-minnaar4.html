<!DOCTYPE html>
<html lang="en">
<head>
        <title>Alex Minnaar's Blog - Machine Learning, Data Science and Software Engineering - Alex Minnaar</title>
        <meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <link rel="shortcut icon" href="http://launchyard.com/images/favicon.png"/>
        <link rel="stylesheet" href="/theme/css/main.css" type="text/css" />

        <!--[if IE]>
                <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

        <!--[if lte IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="/css/ie.css"/>
                <script src="/js/IE8.js" type="text/javascript"></script><![endif]-->

        <!--[if lt IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="/css/ie6.css"/><![endif]-->
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.8/jquery.min.js" type="text/javascript"></script>


</head>

<body id="index" class="home">
	
  <!--      <header id="banner" class="body">
                <h1><a href="/"><img src="http://www.launchyard.com/images/logo.png" /></a></h1>
        </header> -->
<!-- /#banner -->
	      <div class="LaunchyardDetail"><p><a href="/"></a>
<a class="title" href="http://alexminnaar.github.io/">Alex Minnaar</a>
<br/>
Machine Learning at University College London. Natural language processing at VerticalScope.
<br/>
<br/>
<a href="mailto:minnaaralex@gmail.com">Email</a><br />
<a href="https://github.com/alexminnaar">Github</a><br/>
<a href="https://ca.linkedin.com/pub/alex-minnaar/56/a23/853">LinkedIn</a><br/>
</p>


<div id="recent_posts">
			<h3>Categories</h3>
			<div align="left">
			<ul>
				<li><a href="/tag/nlp.html">NLP & Topic Models</a></li>
				<li><a href="/tag/supervised-learning.html">Supervised Learning</a></li>
				<li><a href="/tag/bayesian-inference.html">Bayesian Inference</a></li>
				<li><a href="/tag/kaggle-competitions.html">Kaggle Competitions</a></li>
				<li><a href="/tag/software-engineering.html">Software Engineering</a></li>
			</ul>
			</div>
              <h3>Recent Posts</h3>
                <a href="word2vec-tutorial-part-i-the-skip-gram-model.html">Word2Vec Tutorial Part I: The Skip-Gram Model</a><br /><br />
                <a href="distributed-online-latent-dirichlet-allocation-with-apache-spark.html">Distributed Online Latent Dirichlet Allocation with Apache Spark</a><br /><br />
                <a href="deep-learning-basics-neural-networks-backpropagation-and-stochastic-gradient-descent.html">Deep Learning Basics: Neural Networks, Backpropagation and Stochastic Gradient Descent</a><br /><br />
                <a href="building-a-shoutbox-app-with-cassandra-and-nodejs.html">Building a Shoutbox App with Cassandra and Node.js</a><br /><br />
                <a href="/building-a-distributed-binary-search-tree-with-akka.html">Building a Distributed Binary Search Tree with Akka</a><br /><br />
                <a href="/introduction-to-the-multithreading-problem-and-the-akka-actor-solution.html">Introduction to the Multithreading Problem and the Akka Actor Solution  </a><br /><br />
                <a href="/scalaner-a-scala-wrapper-for-the-stanford-ner-tool-with-some-added-features.html">ScalaNER: A Scala Wrapper for the Stanford NER Tool with Some Added Features  </a><br /><br />
                <a href="/online-latent-dirichlet-allocation-the-best-option-for-topic-modeling-with-large-data-sets.html">Online Latent Dirichlet Allocation - The Best Option for Topic Modeling with Large Data Sets  </a><br /><br />
                <a href="/latent-dirichlet-allocation-in-scala-part-ii-the-code.html">Latent Dirichlet Allocation in Scala Part II - The Code </a><br /><br />
          </div>

</div>

        
        

    
            <aside id="featured">
                <div class="body">
                    <article>
                        <h1 class="entry-title"><a href="/facebook-recruiting-iii-keyword-extraction-part-5.html">Facebook Recruiting III Keyword Extraction - Part 5</a></h1>
<div class="post-info">
	<ul>
        <li class="vcard author">
                 by&nbsp;<a class="url fn" href="/author/alex-minnaar.html">Alex Minnaar</a>
        </li>
        <li class="published" title="2013-10-05T00:00:00+02:00">
          on&nbsp;Sat 05 October 2013
        </li>

	</ul>
<p>Category: <a href="/tag/kaggle-competitions.html">   Kaggle-Competitions</a><a href="/tag/nlp.html">   NLP</a></p>
</div><!-- /.post-info --><h2>Implementing the association rule algorithm</h2>
<p>As stated earlier, words in post titles and words in post bodies are fundamentally different with respect to their influence on the tags that are assigned to that post.  So for this reason, two sets of association rules will be generated - title-tag association rules and body-tag association rules.</p>
<p>Since the dataset is so large, steps must be taken to make sure that we don't run out of RAM. We can do this by importing the dataset line-by-line such that only one line of the dataset is held in main memory at any one time before it is exported to another csv file.  The association rule algorithm can be implemented in the following steps.</p>
<ol>
<li><strong>Find All Combinations:</strong>  The first step is to find all combinations of words (title or body words depending on which set of rules you are finding) and tags that appear in the same posts.  The list of all combinations will be huge! Mine came to about 13GB which would definitely generate a memory error if you were to hold it all in main memory, so each combination must be exported sequentially into another csv file.</li>
<li><strong>Count All Combinations:</strong>  Now you must import that huge list of all combinations and count the number of times each distinct combination appears in the list.  The counts must be held in main memory, but since you are only holding the counts of each distinct combination, it will be much smaller than 13GB (mine came to about 1.5GB).  Now you have <span class="math">\(|Co(A,B)|\)</span> for all words <span class="math">\(A\)</span> and tags <span class="math">\(B\)</span>!</li>
<li><strong>Count Overall Word Occurrences:</strong> Next, in order to calculate the required probabilities, we need to count the number of occurrences of each word in the set of post titles (or bodies).</li>
<li><strong>Calculate Probabilities and Save as Dictionary:</strong>  Now, since we have the word counts and the co-occurrence counts, we can calculate the required probability <span class="math">\(P(B|A)\)</span> for each combination.</li>
</ol>
<h3>Code snippets</h3>
<p>Here are a few snippets of python code to give us a better understanding of how the above steps can be implemented.  The following snippet finds all combinations of words and tags</p>
<div class="highlight"><pre><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">r&quot;input_file.csv&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">r</span><span class="p">,</span> <span class="nb">open</span><span class="p">(</span><span class="s">r&quot;output_file.csv&quot;</span><span class="p">,</span> <span class="s">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">w</span><span class="p">:</span>
     <span class="n">rdr</span> <span class="o">=</span> <span class="n">csv</span><span class="o">.</span><span class="n">reader</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
     <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">rdr</span><span class="p">:</span>
          <span class="n">a</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
          <span class="n">b</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
          <span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">product</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">split</span><span class="p">(),</span> <span class="n">b</span><span class="o">.</span><span class="n">split</span><span class="p">()):</span>
               <span class="n">w</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s">&quot;{},{}</span><span class="se">\n</span><span class="s">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>
</pre></div>


<p>where a is a string containing the title (or body) of a post and b is a string containing the list of tags for that post.  Also, the product() function is part of the python itertools package which would need to be imported.
This next snippet shows how the combinations are then counted.</p>
<div class="highlight"><pre><span class="n">counter</span><span class="o">=</span><span class="p">{}</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">&quot;output_file.csv&quot;</span><span class="p">,</span><span class="s">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file_name</span><span class="p">:</span>
     <span class="n">reader</span><span class="o">=</span><span class="n">csv</span><span class="o">.</span><span class="n">reader</span><span class="p">(</span><span class="n">file_name</span><span class="p">)</span>
     <span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">reader</span><span class="p">:</span>
          <span class="n">pair</span><span class="o">=</span><span class="n">row</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">+</span><span class="s">&#39; &#39;</span><span class="o">+</span><span class="n">row</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
          <span class="k">if</span> <span class="n">pair</span> <span class="ow">in</span> <span class="n">counter</span><span class="p">:</span>
               <span class="n">counter</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span><span class="o">+=</span><span class="mi">1</span>
          <span class="k">else</span><span class="p">:</span>
               <span class="n">counter</span><span class="p">[</span><span class="n">pair</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span>
</pre></div>


<p>where counter is a dictionary whose key is a word/tag pair and its value is the count of the number of times they co-occur.  All of the other steps in the algorithm can be implemented by modifying these two code snippets in some way.</p>
<h3>Some results</h3>
<p>Here are some examples of probability distributions <span class="math">\(P(B|A)\)</span> for different words <span class="math">\(A\)</span>.</p>
<p>The first of the following plots shows the top 10 most likely tags when the word "C++" appears in a post title and the second plot shows the top 10 most likely tags when "C++" appears in a post body.</p>
<p><img alt="alt text" src="images/cplusplus3.png" title="C++ title" /> 
<img alt="alt text" src="images/cplusplus_body1.png" title="C++ body" /></p>
<p>As one might expect, the most likely tag (by a large margin) is "c++".  It is also interesting to see that the likelihoods are significantly larger for when "C++" appears in the title.  This makes sense intuitively because titles are short and succinct and therefore each title word should generally be more descriptive of the overall post than a word in the post body.</p>
<p>The next set of plot is for the word "sql".</p>
<p><img alt="alt text" src="images/sql.png" title="SQL title" /> 
<img alt="alt text" src="images/sql_body.png" title="SQL body" /></p>
<p>What is interesting about these tag likelihoods is that there is not a clear winner.  The tags "sql" and "sql-server" are both quite likely.</p>
<p>The next set of plot is for the more obscure word "geodesic".</p>
<p><img alt="alt text" src="images/geodesic3.png" title="Geodesic title" /> 
<img alt="alt text" src="images/geodesic_body.png" title="Geodesic body" /></p>
<p>There are several interesting differences with these plots.  The first is that the actual post word "geodesic" is not in the top ten most likely tags (it is probably not even in the set of possible tags).  Another difference is that the tag likelihoods are actually larger when "geodesic" appears in the post body.  This is probably because "geodesic" is a more obscure word than "C++" and "sql".</p>
<p>The next step is to use these association rules to make predictions.  Stay tuned...</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                    </article>
                </div>
            </aside><!-- /#featured -->
            
        
        

    
            <aside id="featured">
                <div class="body">
                    <article>
                        <h1 class="entry-title"><a href="/facebook-recruiting-iii-keyword-extraction-part-4.html">Facebook Recruiting III Keyword Extraction - Part 4</a></h1>
<div class="post-info">
	<ul>
        <li class="vcard author">
                 by&nbsp;<a class="url fn" href="/author/alex-minnaar.html">Alex Minnaar</a>
        </li>
        <li class="published" title="2013-09-26T00:00:00+02:00">
          on&nbsp;Thu 26 September 2013
        </li>

	</ul>
<p>Category: <a href="/tag/kaggle-competitions.html">   Kaggle-Competitions</a><a href="/tag/nlp.html">   NLP</a></p>
</div><!-- /.post-info --><h2>Finding Association Rules</h2>
<p>The idea is to develop rules that assign tags to certain words in the post titles and bodies.  For example,</p>
<p><span class="math">\(eclpise\rightarrow java
\\derivative \rightarrow calculus
\\matplotlib \rightarrow python\)</span></p>
<p>where the words on the left side of the arrow are words in the title/body and words on the right side of the arrow are tags. Once these rules are found, they can be applied to the post words in the test set and the tag word predictions can be found trivially. But first we must come up with a method to generate these rules. This can be done probabilistically using word counts from the training set.</p>
<h3>Using probabilities to generate association rules</h3>
<p>The question that needs to be answered is given that word A appears in a post, what is the probability that tag B will also appear in that post? To get this probability, you need to count the number of posts in which word A appears and the also the number of posts in which both word A and tag B appear.  The desired probability can then be calculated as</p>
<p><span class="math">\(P(B|A)=\frac{|Co(A,B)|}{|A|}\)</span></p>
<p>where <span class="math">\(|Co(A,B)|\)</span> is the number of posts where word A and tag B co-occur and <span class="math">\(|A|\)</span> is the number of posts where word A occurs.  So, if <span class="math">\(P(B|A)\)</span> is above a certain threshold, we can then generate the association rule <span class="math">\(A \rightarrow B\)</span>. However, this probability is only a point estimate and does not give us any information regarding how certain we are about the estimate.  For example, it is possible to have one case where <span class="math">\(|Co(A_1,B_1)|=9\)</span> and <span class="math">\(|A_1|=10\)</span> thus <span class="math">\(P(B_1|A_1)=90\%\)</span> and a second case where <span class="math">\(|Co(A_2,B_2)|=89\)</span> and <span class="math">\(|A_2|=100\)</span> thus <span class="math">\(P(B_2|A_2)=89\%\)</span>. And since <span class="math">\(P(B_1|A_1)&gt;P(B_2|A_2)\)</span>, you might think that <span class="math">\(A_1 \rightarrow B_1\)</span> is a stronger association rule than <span class="math">\(A_2 \rightarrow B_2\)</span>. However, it is very possible that this assumption is incorrect because we cannot be certain that <span class="math">\(P(B_1|A_1)\)</span> is the true probability because the sample size is so small. Whereas for <span class="math">\(P(B_2|A_2)\)</span> we can be far more certain. So for this reason we must evaluate the association rule based on two criteria - the probability <span class="math">\(P(B|A)\)</span> and also the support which is <span class="math">\(|Co(A,b)|\)</span>.  We should only consider association rules that have a support above a certain threshold value.</p>
<h3>Potential problems with this method</h3>
<ul>
<li><strong>Is it scalable?</strong>  The training data has ~42 thousand unique tag words and many more unique post words.  Can this association rule algorithm be performed on a single machine given the size of the data?</li>
<li><strong>What are the threshold values?</strong>  How do we determine how large the probabilities <span class="math">\(P(B|A)\)</span> and support need to be in order to warrant an association rule?  i.e. what is the optimal balance between overfitting and underfitting.</li>
<li><strong>What about the likelihood of co-occurring tags?</strong>  In this method we are only generating association rules between post words and tags, but there is no doubt that certain tags are more likely to occur together than others.  We have that information in the training set.  Shouldn't we use that information too?
These are all valid questions that we must think about before any kind of submission can be made...</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                    </article>
                </div>
            </aside><!-- /#featured -->
            
        
        

    
            <aside id="featured">
                <div class="body">
                    <article>
                        <h1 class="entry-title"><a href="/facebook-recruiting-iii-keyword-extraction-part-3.html">Facebook Recruiting III Keyword Extraction - Part 3</a></h1>
<div class="post-info">
	<ul>
        <li class="vcard author">
                 by&nbsp;<a class="url fn" href="/author/alex-minnaar.html">Alex Minnaar</a>
        </li>
        <li class="published" title="2013-09-21T00:00:00+02:00">
          on&nbsp;Sat 21 September 2013
        </li>

	</ul>
<p>Category: <a href="/tag/kaggle-competitions.html">   Kaggle-Competitions</a><a href="/tag/nlp.html">   NLP</a></p>
</div><!-- /.post-info --><h2>Is this a Clustering Problem?</h2>
<p>At first glance this problem looks like a clustering problem.  You can think of the tags as the cluster centroids and based on the words in the post title/body you can determine which clusters (tags words) each post is closest to.  In this way you can assign posts to tags in the test set.</p>
<p>The problem with this is that there is no guarantee that the training set holds all of the tag words that appear in the test set.  This is confirmed by this post in the Kaggle forum.  So if there are new tags in the test set, then perhaps this is more of an inference problem than a clustering problem.  However, we should not rely solely on a forum post so let's come up with some evidence to give us an idea of how many unseen tags will be in the test set.</p>
<h2>Time for an Experiment!</h2>
<p>We can use the training set to give us an idea of how many unseen tags will be present in the test set.  The training set has ~6 million examples and the test set has ~2 million examples.  That's a ratio of about 3:1.  So the experiment will be to divide the training set into the same 3:1 ratio and we shall see how many tags appear in the small portion but not the large portion.  So the large portion will have ~4.5 million examples and the small portion will have ~1.5 million examples.  The following are some of the results of this experiment.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>examples in portion A</td>
<td>4500000</td>
</tr>
<tr>
<td>examples in portion B</td>
<td>1534194</td>
</tr>
<tr>
<td>unique tags in A</td>
<td>41251</td>
</tr>
<tr>
<td>unique tags in B</td>
<td>37179</td>
</tr>
<tr>
<td>tags in B but not in A</td>
<td>797</td>
</tr>
<tr>
<td>occurrences of these tags in B</td>
<td>972</td>
</tr>
<tr>
<td>average number of occurrences of each of these tags in B</td>
<td>1.22</td>
</tr>
</tbody>
</table>
<p>So from the above results, only ~2% of the tags in B are not in A.  And these tags also occur very rarely in B, (only about once for each tag).  Therefore, we can conclude that the vast majority of tags that appear in B also appear in A.  We can also therefore assume that the vast majority of tags that appear in the test set also appear in the training set.  So we can confidently assume that we can treat this as a clustering problem!</p>
                    </article>
                </div>
            </aside><!-- /#featured -->
            
        
        

    
            <aside id="featured">
                <div class="body">
                    <article>
                        <h1 class="entry-title"><a href="/facebook-recruiting-iii-keyword-extraction-part-2.html">Facebook Recruiting III Keyword Extraction - Part 2</a></h1>
<div class="post-info">
	<ul>
        <li class="vcard author">
                 by&nbsp;<a class="url fn" href="/author/alex-minnaar.html">Alex Minnaar</a>
        </li>
        <li class="published" title="2013-09-20T00:00:00+02:00">
          on&nbsp;Fri 20 September 2013
        </li>

	</ul>
<p>Category: <a href="/tag/kaggle-competitions.html">   Kaggle-Competitions</a><a href="/tag/nlp.html">   NLP</a></p>
</div><!-- /.post-info --><p>Before any serious analysis can be done, we must make sure that the dataset is in the optimal form to do so.  The raw data has two problems associated with it.</p>
<ol>
<li>The file is huge!  Once unzipped the training set (called Train.csv) is 7.3 GB.  Depending on your machine this could be too large to hold in main memory.  Even then, the sheer size of the data could make analysis restrictive.</li>
<li>The file is full of irrelevant information (essentially noise).  The purpose of the competition is to build a model that can transform the input (post titles and bodies) into tags.  However, words like "the", "at", "a", etc. will very likely not have any influence in any model.
The good news is that these two problems are related to each other.  Removing irrelevant information will also reduce the file size.  This can be done through a data preprocessing phase.</li>
</ol>
<h2>Data Preprocessing</h2>
<p>So we need to remove the irrelevant information from the title and body columns in the training set.  Post titles and post bodies are inherently different in many ways so we will deal with them both separately.</p>
<h3>Post Titles</h3>
<p>We will first deal with the post titles.  The first thing we must figure out is how to decide which words are irrelevant and which words are important.</p>
<p>Which words are important (and which are not)?</p>
<p>Common sense tells us that stop-words (i.e. words like  "the", "at", "a", etc.) are not important, so there is no question that these words should be removed.  We also know that words that are also tag-words are very important.  For example, if the word "c++" appears in a post title, it is very likely that "c++" will also appear as a tag in that post.</p>
<p>So now we know which words are important and which are not.  But what about the words in between? (i.e. words that are not stop-words but also are not tag-words).  It is difficult to say how important these words are.  If we remove all words except tag-words, the file will become a lot smaller (good), however we might be removing too much information which would hurt the performance of any model we create.</p>
<p>At this point it is probably best to leave these words in.  Perhaps we can consider removing them later if the file is still too large to handle.  The following table shows the result of removing stop-words.  Note:  the original file contained only the post titles and ids.</p>
<table>
<thead>
<tr>
<th>Action</th>
<th>File Size</th>
</tr>
</thead>
<tbody>
<tr>
<td>nothing (original file)</td>
<td>367.3 MB</td>
</tr>
<tr>
<td>removing stop-words #1</td>
<td>317.3 MB</td>
</tr>
<tr>
<td>removing stop-words #2</td>
<td>308.0 MB</td>
</tr>
</tbody>
</table>
<p>where removing stop-words #1 is removing the usual stop-words and removing stop-words #2 is removing stop-words that are more specific to question asking (i.e. "how", "what", "why", etc.).  So we have removed about 60 MB from the post titles without loosing information.  Hooray!</p>
<h3>Post Bodies</h3>
<p>The post bodies are much more difficult to deal with for the following reasons.</p>
<p>They make up the bulk of the training set.  The file containing only post bodies and ids is 6.7 GB.
The post bodies are in HTML format complete with HTML tags that make it very messy.
If that weren't enough, the bodies contain both English sentences and code and we must find a way to differentiate between the two.
In order to reduce the size of this large dataset we have to make some simplifying assumptions.  There is really no way around this if you want to get the data set into a manageable size.</p>
<h3>Simplifying assumptions</h3>
<p>The first simplifying assumptions that we will make is that all of the information that is needed in order to build our model is contained in the text part of the post body and the code part is superfluous.  I think that this is a fair assumption, and even if it weren't true I do not think it would be plausible to extract meaningful information from the code portion of the body that would help us with the model.</p>
<p>Therefore, we will remove all of the code from the post bodies.  This is a relatively straight-forward task because all of the code is enclosed in "code" tags.  You could use a Python package like BeautifulSoup to extract everything in the "code" tags.  The link text enclosed in "a" tags should also be removed for the same reason.</p>
<table>
<thead>
<tr>
<th>Action</th>
<th>File Size</th>
</tr>
</thead>
<tbody>
<tr>
<td>nothing (original file)</td>
<td>6.7 GB</td>
</tr>
<tr>
<td>remove everything in "code" and "a" tags</td>
<td>3.29 GB</td>
</tr>
<tr>
<td>removing all other tags</td>
<td>3.19 GB</td>
</tr>
</tbody>
</table>
<p>As you can see removing the code and link text has cut the size of the post bodies in half!  This is will be a huge benefit for future computation.</p>
                    </article>
                </div>
            </aside><!-- /#featured -->
            
        
        

    
            <aside id="featured">
                <div class="body">
                    <article>
                        <h1 class="entry-title"><a href="/facebook-recruiting-iii-keyword-extraction-part-1.html">Facebook Recruiting III Keyword Extraction - Part 1</a></h1>
<div class="post-info">
	<ul>
        <li class="vcard author">
                 by&nbsp;<a class="url fn" href="/author/alex-minnaar.html">Alex Minnaar</a>
        </li>
        <li class="published" title="2013-09-14T00:00:00+02:00">
          on&nbsp;Sat 14 September 2013
        </li>

	</ul>
<p>Category: <a href="/tag/kaggle-competitions.html">   Kaggle-Competitions</a><a href="/tag/nlp.html">   NLP</a></p>
</div><!-- /.post-info --><p>This is my first post about my foray into the Facebook recruiting competition from Kaggle.  The basic idea of this competition is that competitors are given a training set of posts from stackoverflow and their corresponding tags.  From this training set competitors must build some sort of model to infer tags from posts (in the form of post titles and post bodies).  Then this model is applied to a test set and its accuracy is measured based on its tag predictions. Full details can be found <a href="http://www.kaggle.com/c/facebook-recruiting-iii-keyword-extraction">here</a>.</p>
<p>After parsing and counting the individual tags.  The top ten most frequently occurring tags are the following.</p>
<table>
<thead>
<tr>
<th>Tag</th>
<th>Frequency</th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>c#</td>
<td>463526</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>java</td>
<td>392451</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>php</td>
<td>463526</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>javascript</td>
<td>365623</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>android</td>
<td>320622</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>jquery</td>
<td>305614</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>c++</td>
<td>199280</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>python</td>
<td>199280</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>iphone</td>
<td>183573</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>asp.net</td>
<td>177334</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<p>The most popular tags tend to be programming languages which is what one should expect.</p>
<h2>Something Strange!</h2>
<p>The list of the most popular tags is not strange at all.  What is strange is the fact that in the leaderboard for this competition there is a benchmark for the submission of all predictions set to 'javascript'.  This is strange because 'javascript' is not even the most popular tag.  It's not even the second most popular tag.  It is the fourth most popular tag.  So this begs the question why make the benchmark all predictions 'javascript' instead of all predictions 'c#' ?  After all,  'c#' is the most popular tag.  I don't know why...</p>
<h2>So Let's Test it!</h2>
<p>Why don't we use our first submission as setting all predictions to 'c#'.  If the tag distribution of the test set is the same as the training set (we must assume that it is), then this submission should do better than the 'javascript' submission.  The 'javascript' submission attained a score of 0.03148.  The result for the all 'c#' submission is... 0.03857</p>
<p>Yay! This at least confirms our suspicion that the most popular tags in the training set are also the most popular terms in the test set.  However, this is still a relatively low score.  A more rigorous solution needs to be implemented in order to achieve a higher score.  So stay tuned for part 2</p>
                    </article>
 
<div class="paginator">
            <div class="navButton"><a href="/author/alex-minnaar3.html">Prev</a></div>
    <div class="navButton">Page 4 / 4</div>
</div>
                </div>
            </aside><!-- /#featured -->
            
        
        <section id="extras" >
       
        
        </section><!-- /#extras -->
	
        <footer id="contentinfo" >
                <address id="about" class="vcard ">
                Proudly powered by <a href="http://getpelican.com/" target="_blank">Pelican</a>, which takes
                great advantage of <a href="http://python.org" target="_blank">Python</a>. &copy; <a class="url fn" href="http://launchyard.com">LaunchYard</a>
		
                </address><!-- /#about -->
		

                
        </footer><!-- /#contentinfo -->

<script type="text/javascript">
    var disqus_shortname = 'alexminnaar';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
</body>
</html>