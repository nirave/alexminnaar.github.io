<!DOCTYPE html>
<html lang="en">
<head>
        <title>Alex Minnaar's Blog - Machine Learning, Data Science and Software Engineering - Alex Minnaar</title>
        <meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <link rel="shortcut icon" href="http://launchyard.com/images/favicon.png"/>
        <link rel="stylesheet" href="/theme/css/main.css" type="text/css" />

        <!--[if IE]>
                <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

        <!--[if lte IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="/css/ie.css"/>
                <script src="/js/IE8.js" type="text/javascript"></script><![endif]-->

        <!--[if lt IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="/css/ie6.css"/><![endif]-->
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.8/jquery.min.js" type="text/javascript"></script>


</head>

<body id="index" class="home">
	
  <!--      <header id="banner" class="body">
                <h1><a href="/"><img src="http://www.launchyard.com/images/logo.png" /></a></h1>
        </header> -->
<!-- /#banner -->
	      <div class="LaunchyardDetail"><p><a href="/"></a>
<a class="title" href="http://alexminnaar.github.io/">Alex Minnaar</a>
<br/>
Machine Learning at University College London. Natural language processing at VerticalScope.
<br/>
<br/>
<a href="mailto:minnaaralex@gmail.com">Email</a><br />
<a href="https://github.com/alexminnaar">Github</a><br/>
<a href="https://ca.linkedin.com/pub/alex-minnaar/56/a23/853">LinkedIn</a><br/>
</p>


<div id="recent_posts">
			<h3>Categories</h3>
			<div align="left">
			<ul>
				<li><a href="/tag/nlp.html">NLP & Topic Models</a></li>
				<li><a href="/tag/supervised-learning.html">Supervised Learning</a></li>
				<li><a href="/tag/bayesian-inference.html">Bayesian Inference</a></li>
				<li><a href="/tag/kaggle-competitions.html">Kaggle Competitions</a></li>
				<li><a href="/tag/software-engineering.html">Software Engineering</a></li>
			</ul>
			</div>
              <h3>Recent Posts</h3>
                <a href="building-a-shoutbox-app-with-cassandra-and-nodejs.html">Building a Shoutbox App with Cassandra and Node.js</a><br /><br />
                <a href="/building-a-distributed-binary-search-tree-with-akka.html">Building a Distributed Binary Search Tree with Akka</a><br /><br />
                <a href="/introduction-to-the-multithreading-problem-and-the-akka-actor-solution.html">Introduction to the Multithreading Problem and the Akka Actor Solution  </a><br /><br />
                <a href="/scalaner-a-scala-wrapper-for-the-stanford-ner-tool-with-some-added-features.html">ScalaNER: A Scala Wrapper for the Stanford NER Tool with Some Added Features  </a><br /><br />
                <a href="/online-latent-dirichlet-allocation-the-best-option-for-topic-modeling-with-large-data-sets.html">Online Latent Dirichlet Allocation - The Best Option for Topic Modeling with Large Data Sets  </a><br /><br />
                <a href="/latent-dirichlet-allocation-in-scala-part-ii-the-code.html">Latent Dirichlet Allocation in Scala Part II - The Code </a><br /><br />
          </div>

</div>

        
        

    
            <aside id="featured">
                <div class="body">
                    <article>
                        <h1 class="entry-title"><a href="/latent-dirichlet-allocation-in-scala-part-ii-the-code.html">Latent Dirichlet Allocation in Scala Part II - The Code</a></h1>
<div class="post-info">
	<ul>
        <li class="vcard author">
                 by&nbsp;<a class="url fn" href="/author/alex-minnaar.html">Alex Minnaar</a>
        </li>
        <li class="published" title="2014-08-10T00:00:00+02:00">
          on&nbsp;Sun 10 August 2014
        </li>

	</ul>
<p>Category: <a href="/tag/bayesian-inference.html">   Bayesian Inference</a><a href="/tag/nlp.html">   NLP</a></p>
</div><!-- /.post-info --><p>The theory behind Latent Dirichlet Allocation was outlined in the the <a href="/latent-dirichlet-allocation-in-scala-part-i-the-theory.html">previous blog post</a>.  Now the goal is to translate this theory into a fully-fledged Scala application.  There are two main entities in the LDA algorithm</p>
<ol>
<li><strong>The Corpus:</strong>  This is the collection of documents.  Functionality is needed to accept documents from the user, create a vocabulary, perform text preprocessing, maintain document-level and corpus-level word counts, and returning the desired output back to the user.</li>
<li><strong>Inference:</strong>  The core aspect of this algorithm is the collapsed Gibbs sampling inference step.  This must be implemented efficiently and correctly.</li>
</ol>
<h2>The Corpus</h2>
<h3>Getting a vocabulary</h3>
<p>The first task that the corpus class must undertake is getting a vocabulary from the given documents.  Stop words must be removed and we also want to remove low-frequency words (these words would most likely not show up in any topics anyway so it is best to remove them for memory-management reasons).  The low-frequency cut-off threshold should be supplied by the user.  In order to determine the low-frequency words, a corpus-wide word count must be performed.  Also, it is good practice to remove stop words and words that are either too short or too long.  The following <code>CountVocab</code> class performs the required word count, then removes words that are too infrequent, are part of a stop word list, or are not within the allowable length bounds.</p>
<div class="highlight"><pre><span class="k">class</span> <span class="nc">CountVocab</span><span class="o">(</span><span class="n">filePath</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">minCount</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="k">extends</span> <span class="nc">Vocabulary</span> <span class="o">{</span>

  <span class="k">val</span> <span class="n">stopWords</span> <span class="k">=</span> <span class="nc">Source</span><span class="o">.</span><span class="n">fromURL</span><span class="o">(</span><span class="n">getClass</span><span class="o">.</span><span class="n">getResource</span><span class="o">(</span><span class="s">&quot;/english_stops_words.txt&quot;</span><span class="o">)).</span><span class="n">mkString</span><span class="o">.</span><span class="n">split</span><span class="o">(</span><span class="s">&quot;\n&quot;</span><span class="o">).</span><span class="n">toSet</span>

  <span class="k">def</span> <span class="n">getVocabulary</span><span class="k">:</span> <span class="kt">HashMap</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>

    <span class="k">var</span> <span class="n">vocabulary</span><span class="k">:</span> <span class="kt">HashMap</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="nc">HashMap</span><span class="o">.</span><span class="n">empty</span>
    <span class="k">var</span> <span class="n">wordCounter</span><span class="k">:</span> <span class="kt">HashMap</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="nc">HashMap</span><span class="o">.</span><span class="n">empty</span>

    <span class="k">def</span> <span class="n">countWords</span><span class="o">(</span><span class="n">docFile</span><span class="k">:</span> <span class="kt">File</span><span class="o">)</span> <span class="o">{</span>

      <span class="k">val</span> <span class="n">tokens</span> <span class="k">=</span> <span class="nc">StanfordTokenizer</span><span class="o">.</span><span class="n">tokenizeFile</span><span class="o">(</span><span class="n">docFile</span><span class="o">)</span>

      <span class="k">for</span> <span class="o">(</span><span class="n">token</span> <span class="k">&lt;-</span> <span class="n">tokens</span><span class="o">)</span> <span class="o">{</span>

        <span class="k">if</span> <span class="o">(</span><span class="n">wordCounter</span><span class="o">.</span><span class="n">contains</span><span class="o">(</span><span class="n">token</span><span class="o">))</span> <span class="o">{</span>
          <span class="n">wordCounter</span> <span class="o">+=</span> <span class="o">(</span><span class="n">token</span> <span class="o">-&gt;</span> <span class="o">(</span><span class="n">wordCounter</span><span class="o">(</span><span class="n">token</span><span class="o">)</span> <span class="o">+</span> <span class="mi">1</span><span class="o">))</span>
        <span class="o">}</span>
        <span class="k">else</span> <span class="k">if</span> <span class="o">(!</span><span class="n">stopWords</span><span class="o">.</span><span class="n">contains</span><span class="o">(</span><span class="n">token</span><span class="o">)</span> <span class="o">&amp;&amp;</span> <span class="o">!</span><span class="n">token</span><span class="o">.</span><span class="n">matches</span><span class="o">(</span><span class="s">&quot;\\p{Punct}&quot;</span><span class="o">)</span> <span class="o">&amp;&amp;</span> <span class="n">token</span><span class="o">.</span><span class="n">length</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="o">&amp;&amp;</span> <span class="n">token</span><span class="o">.</span><span class="n">length</span> <span class="o">&lt;</span> <span class="mi">15</span> <span class="o">&amp;&amp;</span> <span class="n">token</span> <span class="o">!=</span> <span class="s">&quot;-lrb-&quot;</span> <span class="o">&amp;&amp;</span> <span class="n">token</span> <span class="o">!=</span> <span class="s">&quot;-rrb-&quot;</span><span class="o">)</span> <span class="o">{</span>
          <span class="n">wordCounter</span> <span class="o">+=</span> <span class="o">(</span><span class="n">token</span> <span class="o">-&gt;</span> <span class="mi">1</span><span class="o">)</span>
        <span class="o">}</span>
      <span class="o">}</span>
    <span class="o">}</span>

    <span class="k">new</span> <span class="nc">File</span><span class="o">(</span><span class="n">filePath</span><span class="o">).</span><span class="n">listFiles</span><span class="o">.</span><span class="n">toIterator</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">isFile</span><span class="o">).</span><span class="n">toList</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">docFile</span> <span class="k">=&gt;</span> <span class="n">countWords</span><span class="o">(</span><span class="n">docFile</span><span class="o">))</span>

    <span class="k">for</span> <span class="o">((</span><span class="n">w</span><span class="o">,</span> <span class="n">freq</span><span class="o">)</span> <span class="k">&lt;-</span> <span class="n">wordCounter</span><span class="o">)</span> <span class="o">{</span>
      <span class="k">if</span> <span class="o">(</span><span class="n">freq</span> <span class="o">&gt;=</span> <span class="n">minCount</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">vocabulary</span> <span class="o">+=</span> <span class="o">(</span><span class="n">w</span> <span class="o">-&gt;</span> <span class="n">vocabulary</span><span class="o">.</span><span class="n">size</span><span class="o">)</span>
      <span class="o">}</span>
    <span class="o">}</span>

    <span class="n">vocabulary</span>
  <span class="o">}</span>
<span class="o">}</span>
</pre></div>


<p>The above function takes a file path as an input parameter that points to the directory containing the documents (in text file format) on which you are going to perform LDA.  It also takes a frequency threshold parameter, below which a word is deemed too infrequent to be useful.  There is a nested function that performs a word count on the important words (tokenized with the Stanford coreNLP tokenizer), then we iterate through the counted words and keep the ones above the frequency threshold.  Also, we create a hashmap where every word we keep is mapped to a unique integer ID which will be used later.</p>
<h3>Keeping Track of the Topic Assignment Counts</h3>
<p>From <a href="/latent-dirichlet-allocation-in-scala-part-i-the-theory.html">part I</a>, we learned that the probabilities that we are interested in are dependent upon the topics that are assigned to each word in the corpus.  During the Gibbs sampling procedure, the topic assignments are constantly being updated and the conditional distribution that is being sampled from also needs to be updated to reflect the new topic assignments.  Therefore, we need a way of keeping tack of these topic assignment counts.  This will be done with two matrices (using the <a href="https://github.com/scalanlp/breeze">Breeze</a> linear algebra library) - a topic/word matrix that counts how many times each word is assigned to each topic, and a document/topic matrix that counts how many words each topic is assigned to in each document.  Furthermore, we need to initialize the Gibbs sampling procedure by randomly assigning each word to a topic.  This is all done in the following <code>CollapsedLDACorpus</code> class.</p>
<div class="highlight"><pre><span class="k">class</span> <span class="nc">CollapsedLDACorpus</span><span class="o">(</span><span class="n">vocab</span><span class="k">:</span> <span class="kt">HashMap</span><span class="o">[</span><span class="kt">String</span>, <span class="kt">Int</span><span class="o">],</span> <span class="n">numTopics</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">docDirectory</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span> <span class="k">extends</span> <span class="nc">Corpus</span> <span class="o">{</span>

  <span class="k">val</span> <span class="n">numDocs</span> <span class="k">=</span> <span class="nc">DocUtils</span><span class="o">.</span><span class="n">numDocs</span><span class="o">(</span><span class="n">docDirectory</span><span class="o">)</span>
  <span class="k">val</span> <span class="n">vocabulary</span> <span class="k">=</span> <span class="n">vocab</span>
  <span class="k">var</span> <span class="n">docTopicMatrix</span> <span class="k">=</span> <span class="nc">DenseMatrix</span><span class="o">.</span><span class="n">zeros</span><span class="o">[</span><span class="kt">Double</span><span class="o">](</span><span class="n">numDocs</span><span class="o">,</span> <span class="n">numTopics</span><span class="o">)</span>
  <span class="k">var</span> <span class="n">topicWordMatrix</span> <span class="k">=</span> <span class="nc">DenseMatrix</span><span class="o">.</span><span class="n">zeros</span><span class="o">[</span><span class="kt">Double</span><span class="o">](</span><span class="n">numTopics</span><span class="o">,</span> <span class="n">vocabulary</span><span class="o">.</span><span class="n">size</span><span class="o">)</span>
  <span class="k">var</span> <span class="n">words</span><span class="k">:</span> <span class="kt">Vector</span><span class="o">[</span><span class="kt">Word</span><span class="o">]</span> <span class="k">=</span> <span class="nc">Vector</span><span class="o">.</span><span class="n">empty</span>

  <span class="k">val</span> <span class="n">randomTopicGenerator</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">Random</span>
  <span class="k">var</span> <span class="n">docIndex</span> <span class="k">=</span> <span class="mi">0</span>

  <span class="k">def</span> <span class="n">processDoc</span><span class="o">(</span><span class="n">contents</span><span class="k">:</span> <span class="kt">String</span><span class="o">)</span> <span class="k">=</span> <span class="o">{</span>

    <span class="k">val</span> <span class="n">tokens</span> <span class="k">=</span> <span class="nc">StanfordTokenizer</span><span class="o">.</span><span class="n">tokenizeString</span><span class="o">(</span><span class="n">contents</span><span class="o">)</span>

    <span class="k">for</span> <span class="o">(</span><span class="n">token</span> <span class="k">&lt;-</span> <span class="n">tokens</span><span class="o">)</span> <span class="o">{</span>

      <span class="k">val</span> <span class="n">randTopic</span> <span class="k">=</span> <span class="n">randomTopicGenerator</span><span class="o">.</span><span class="n">nextInt</span><span class="o">(</span><span class="n">numTopics</span><span class="o">)</span>

      <span class="k">if</span> <span class="o">(</span><span class="n">vocabulary</span><span class="o">.</span><span class="n">contains</span><span class="o">(</span><span class="n">token</span><span class="o">))</span> <span class="o">{</span>

        <span class="c1">//Assign the word to a random topic</span>
        <span class="n">words</span> <span class="o">:+=</span> <span class="nc">Word</span><span class="o">(</span><span class="n">token</span><span class="o">,</span> <span class="n">docIndex</span><span class="o">,</span> <span class="n">randTopic</span><span class="o">)</span>
        <span class="n">docTopicMatrix</span><span class="o">(</span><span class="n">docIndex</span><span class="o">,</span> <span class="n">randTopic</span><span class="o">)</span> <span class="o">+=</span> <span class="mf">1.0</span>
        <span class="n">topicWordMatrix</span><span class="o">(</span><span class="n">randTopic</span><span class="o">,</span> <span class="n">vocabulary</span><span class="o">(</span><span class="n">token</span><span class="o">))</span> <span class="o">+=</span> <span class="mf">1.0</span>
      <span class="o">}</span>
    <span class="o">}</span>
    <span class="n">docIndex</span> <span class="o">+=</span> <span class="mi">1</span>
  <span class="o">}</span>

  <span class="k">def</span> <span class="n">initialize</span> <span class="k">=</span> <span class="o">{</span>
    <span class="k">new</span> <span class="nc">File</span><span class="o">(</span><span class="n">docDirectory</span><span class="o">).</span><span class="n">listFiles</span><span class="o">.</span><span class="n">toIterator</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">getName</span><span class="o">.</span><span class="n">endsWith</span><span class="o">(</span><span class="s">&quot;.txt&quot;</span><span class="o">)).</span><span class="n">toList</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">docFile</span> <span class="k">=&gt;</span> <span class="n">processDoc</span><span class="o">(</span><span class="n">fromFile</span><span class="o">(</span><span class="n">docFile</span><span class="o">).</span><span class="n">getLines</span><span class="o">().</span><span class="n">mkString</span><span class="o">))</span>
  <span class="o">}</span>

  <span class="k">def</span> <span class="n">reverseVocab</span><span class="k">:</span> <span class="kt">HashMap</span><span class="o">[</span><span class="kt">Int</span>, <span class="kt">String</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
    <span class="n">vocabulary</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span> <span class="n">swap</span><span class="o">)</span>
  <span class="o">}</span>
<span class="o">}</span>
</pre></div>


<p>There is also an <code>processDoc</code> function that assigns each word in the document (that is in the vocabulary) to a random topic and increments the corresponding entries <code>docTopicMatrix</code> and <code>topicWordMatrix</code>.  The topic assignments are assigned within objects of a case class called <code>Word</code> that maintains the state of each word.</p>
<div class="highlight"><pre><span class="k">case</span> <span class="k">class</span> <span class="nc">Word</span><span class="o">(</span><span class="n">token</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">doc</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="k">var</span> <span class="n">topic</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span>
</pre></div>


<p>The state of a word is its assigned topic, the document that it appears in, and the actual string value of the word itself.</p>
<h2>Inference</h2>
<p>The collapsed Gibbs sampling inference algorithm was described in detail in <a href="/latent-dirichlet-allocation-in-scala-part-i-the-theory.html">part I of this blog post</a>.  In short, topic assignments are repeatedly sampled from a conditional distribution and after enough samples have been performed, it is assumed that the samples are taken from the posterior distribution over topic assignments.  Then, the <span class="math">\(\theta\)</span> and <span class="math">\(\phi\)</span> probabilities can be computed from these inferred topic assignments.  The following <code>collapsedGibbs</code> class performs this these tasks.</p>
<div class="highlight"><pre><span class="k">class</span> <span class="nc">CollapsedGibbs</span><span class="o">(</span><span class="n">corpus</span><span class="k">:</span> <span class="kt">CollapsedLDACorpus</span><span class="o">,</span> <span class="n">docDirectory</span><span class="k">:</span> <span class="kt">String</span><span class="o">,</span> <span class="n">vocabThreshold</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">K</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">alpha</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">beta</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span> <span class="k">extends</span> <span class="nc">TopicModel</span> <span class="o">{</span>

  <span class="cm">/**</span>
<span class="cm">   * For a given word, calculate the conditional distribution over topic assignments to be sampled from.</span>
<span class="cm">   * @param word word whose topic will be inferred from the Gibb&#39;s sampler.</span>
<span class="cm">   * @return distribution over topics for the word input.</span>
<span class="cm">   */</span>
  <span class="k">private</span><span class="o">[</span><span class="kt">this</span><span class="o">]</span> <span class="k">def</span> <span class="n">gibbsDistribution</span><span class="o">(</span><span class="n">word</span><span class="k">:</span> <span class="kt">Word</span><span class="o">)</span><span class="k">:</span> <span class="kt">Multinomial</span><span class="o">[</span><span class="kt">DenseVector</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span>, <span class="kt">Int</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>

    <span class="k">val</span> <span class="n">docTopicRow</span><span class="k">:</span> <span class="kt">DenseVector</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="k">=</span> <span class="n">corpus</span><span class="o">.</span><span class="n">docTopicMatrix</span><span class="o">(</span><span class="n">word</span><span class="o">.</span><span class="n">doc</span><span class="o">,</span> <span class="o">::).</span><span class="n">t</span>
    <span class="k">val</span> <span class="n">topicWordCol</span><span class="k">:</span> <span class="kt">DenseVector</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="k">=</span> <span class="n">corpus</span><span class="o">.</span><span class="n">topicWordMatrix</span><span class="o">(::,</span> <span class="n">corpus</span><span class="o">.</span><span class="n">vocabulary</span><span class="o">(</span><span class="n">word</span><span class="o">.</span><span class="n">token</span><span class="o">))</span>
    <span class="k">val</span> <span class="n">topicSums</span><span class="k">:</span> <span class="kt">DenseVector</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="k">=</span> <span class="n">sum</span><span class="o">(</span><span class="n">corpus</span><span class="o">.</span><span class="n">topicWordMatrix</span><span class="o">,</span> <span class="nc">Axis</span><span class="o">.</span><span class="n">_1</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">params</span> <span class="k">=</span> <span class="o">(</span><span class="n">docTopicRow</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">)</span> <span class="o">:*</span> <span class="o">(</span><span class="n">topicWordCol</span> <span class="o">+</span> <span class="n">beta</span><span class="o">)</span> <span class="o">/</span> <span class="o">(</span><span class="n">topicSums</span> <span class="o">+</span> <span class="n">corpus</span><span class="o">.</span><span class="n">vocabulary</span><span class="o">.</span><span class="n">size</span> <span class="o">*</span> <span class="n">beta</span><span class="o">)</span>

    <span class="c1">//normalize parameters</span>
    <span class="k">val</span> <span class="n">normalizingConstant</span> <span class="k">=</span> <span class="n">sum</span><span class="o">(</span><span class="n">params</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">normalizedParams</span> <span class="k">=</span> <span class="n">params</span> <span class="o">:/</span> <span class="n">normalizingConstant</span>

    <span class="nc">Multinomial</span><span class="o">(</span><span class="n">normalizedParams</span><span class="o">)</span>
  <span class="o">}</span>

  <span class="cm">/**</span>
<span class="cm">   * Gibbs sampler for LDA</span>
<span class="cm">   * @param numIter number of iterations that Gibbs sampler will be run</span>
<span class="cm">   */</span>
  <span class="k">private</span><span class="o">[</span><span class="kt">this</span><span class="o">]</span> <span class="k">def</span> <span class="n">gibbsSample</span><span class="o">(</span><span class="n">numIter</span><span class="k">:</span> <span class="kt">Int</span> <span class="o">=</span> <span class="mi">200</span><span class="o">)</span> <span class="o">{</span>

    <span class="k">for</span> <span class="o">(</span><span class="n">iter</span> <span class="k">&lt;-</span> <span class="mi">0</span> <span class="n">to</span> <span class="n">numIter</span><span class="o">)</span> <span class="o">{</span>

      <span class="n">println</span><span class="o">(</span><span class="n">iter</span><span class="o">)</span>

      <span class="k">for</span> <span class="o">(</span><span class="n">word</span> <span class="k">&lt;-</span> <span class="n">corpus</span><span class="o">.</span><span class="n">words</span><span class="o">)</span> <span class="o">{</span>

        <span class="k">val</span> <span class="n">multinomialDist</span> <span class="k">=</span> <span class="n">gibbsDistribution</span><span class="o">(</span><span class="n">word</span><span class="o">)</span>

        <span class="k">val</span> <span class="n">oldTopic</span> <span class="k">=</span> <span class="n">word</span><span class="o">.</span><span class="n">topic</span>

        <span class="c1">//reassign word to topic determined by sample</span>
        <span class="n">word</span><span class="o">.</span><span class="n">topic</span> <span class="k">=</span> <span class="n">multinomialDist</span><span class="o">.</span><span class="n">draw</span><span class="o">()</span>

        <span class="c1">//If topic assignment has changed, must also change count matrices</span>
        <span class="k">if</span> <span class="o">(</span><span class="n">oldTopic</span> <span class="o">!=</span> <span class="n">word</span><span class="o">.</span><span class="n">topic</span><span class="o">)</span> <span class="o">{</span>

          <span class="c1">//increment counts to due to reassignment to new topic</span>
          <span class="n">corpus</span><span class="o">.</span><span class="n">topicWordMatrix</span><span class="o">(</span><span class="n">word</span><span class="o">.</span><span class="n">topic</span><span class="o">,</span> <span class="n">corpus</span><span class="o">.</span><span class="n">vocabulary</span><span class="o">(</span><span class="n">word</span><span class="o">.</span><span class="n">token</span><span class="o">))</span> <span class="o">+=</span> <span class="mf">1.0</span>
          <span class="n">corpus</span><span class="o">.</span><span class="n">docTopicMatrix</span><span class="o">(</span><span class="n">word</span><span class="o">.</span><span class="n">doc</span><span class="o">,</span> <span class="n">word</span><span class="o">.</span><span class="n">topic</span><span class="o">)</span> <span class="o">+=</span> <span class="mf">1.0</span>

          <span class="c1">//decrement counts of old topic assignment that has been changed</span>
          <span class="n">corpus</span><span class="o">.</span><span class="n">topicWordMatrix</span><span class="o">(</span><span class="n">oldTopic</span><span class="o">,</span> <span class="n">corpus</span><span class="o">.</span><span class="n">vocabulary</span><span class="o">(</span><span class="n">word</span><span class="o">.</span><span class="n">token</span><span class="o">))</span> <span class="o">-=</span> <span class="mf">1.0</span>
          <span class="n">corpus</span><span class="o">.</span><span class="n">docTopicMatrix</span><span class="o">(</span><span class="n">word</span><span class="o">.</span><span class="n">doc</span><span class="o">,</span> <span class="n">oldTopic</span><span class="o">)</span> <span class="o">-=</span> <span class="mf">1.0</span>

        <span class="o">}</span>
      <span class="o">}</span>
    <span class="o">}</span>
  <span class="o">}</span>

  <span class="cm">/**</span>
<span class="cm">   * Calculate theta matrix directly from doc/topic counts.  Overwrite counts matrix to save memory.</span>
<span class="cm">   */</span>
  <span class="k">private</span><span class="o">[</span><span class="kt">this</span><span class="o">]</span> <span class="k">def</span> <span class="n">getTheta</span> <span class="o">{</span>

    <span class="c1">//we turn the counts matrix into a probability matrix</span>
    <span class="k">for</span> <span class="o">(</span><span class="n">doc</span> <span class="k">&lt;-</span> <span class="mi">0</span> <span class="n">to</span> <span class="n">corpus</span><span class="o">.</span><span class="n">numDocs</span> <span class="o">-</span> <span class="mi">1</span><span class="o">)</span> <span class="o">{</span>

      <span class="k">val</span> <span class="n">countToProb</span><span class="k">:</span> <span class="kt">DenseVector</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="k">=</span> <span class="o">((</span><span class="n">corpus</span><span class="o">.</span><span class="n">docTopicMatrix</span><span class="o">(</span><span class="n">doc</span><span class="o">,</span> <span class="o">::)</span> <span class="o">+</span> <span class="n">alpha</span><span class="o">)</span> <span class="o">/</span> <span class="o">(</span><span class="n">sum</span><span class="o">(</span><span class="n">corpus</span><span class="o">.</span><span class="n">docTopicMatrix</span><span class="o">(</span><span class="n">doc</span><span class="o">,</span> <span class="o">::).</span><span class="n">t</span><span class="o">)</span> <span class="o">+</span> <span class="n">K</span> <span class="o">*</span> <span class="n">alpha</span><span class="o">)).</span><span class="n">t</span>
      <span class="n">corpus</span><span class="o">.</span><span class="n">docTopicMatrix</span><span class="o">(</span><span class="n">doc</span><span class="o">,</span> <span class="o">::)</span> <span class="o">:=</span> <span class="n">countToProb</span><span class="o">.</span><span class="n">t</span>
    <span class="o">}</span>
  <span class="o">}</span>

  <span class="cm">/**</span>
<span class="cm">   * Calculate phi matric directly from topic/word counts.  Overwrite counts matrix to save memory.</span>
<span class="cm">   */</span>
  <span class="k">private</span><span class="o">[</span><span class="kt">this</span><span class="o">]</span> <span class="k">def</span> <span class="n">getPhi</span> <span class="o">{</span>

    <span class="c1">//we turn the counts matrix into a probability matrix</span>
    <span class="k">for</span> <span class="o">(</span><span class="n">topic</span> <span class="k">&lt;-</span> <span class="mi">0</span> <span class="n">to</span> <span class="n">K</span> <span class="o">-</span> <span class="mi">1</span><span class="o">)</span> <span class="o">{</span>

      <span class="k">val</span> <span class="n">countToProb</span><span class="k">:</span> <span class="kt">DenseVector</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="k">=</span> <span class="o">((</span><span class="n">corpus</span><span class="o">.</span><span class="n">topicWordMatrix</span><span class="o">(</span><span class="n">topic</span><span class="o">,</span> <span class="o">::)</span> <span class="o">+</span> <span class="n">beta</span><span class="o">)</span> <span class="o">/</span> <span class="o">(</span><span class="n">sum</span><span class="o">(</span><span class="n">corpus</span><span class="o">.</span><span class="n">topicWordMatrix</span><span class="o">(</span><span class="n">topic</span><span class="o">,</span> <span class="o">::).</span><span class="n">t</span><span class="o">)</span> <span class="o">+</span> <span class="n">corpus</span><span class="o">.</span><span class="n">vocabulary</span><span class="o">.</span><span class="n">size</span> <span class="o">*</span> <span class="n">beta</span><span class="o">)).</span><span class="n">t</span>
      <span class="n">corpus</span><span class="o">.</span><span class="n">topicWordMatrix</span><span class="o">(</span><span class="n">topic</span><span class="o">,</span> <span class="o">::)</span> <span class="o">:=</span> <span class="n">countToProb</span><span class="o">.</span><span class="n">t</span>
    <span class="o">}</span>
  <span class="o">}</span>

  <span class="cm">/**</span>
<span class="cm">   * Perform all inference steps - gibbs sampling, calculating theta matrix, calculating phi matrix.</span>
<span class="cm">   */</span>
  <span class="k">def</span> <span class="n">inference</span> <span class="o">{</span>
    <span class="n">gibbsSample</span><span class="o">()</span>
    <span class="n">getTheta</span>
    <span class="n">getPhi</span>
  <span class="o">}</span>

  <span class="cm">/**</span>
<span class="cm">   * Print topics found by LDA algorithm.</span>
<span class="cm">   * @param numWords Determines how many words to display for each topic.</span>
<span class="cm">   */</span>
  <span class="k">def</span> <span class="n">printTopics</span><span class="o">(</span><span class="n">numWords</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span> <span class="o">{</span>

    <span class="c1">//want to actually show the words, so we need to extract strings from ids</span>
    <span class="k">val</span> <span class="n">revV</span> <span class="k">=</span> <span class="n">corpus</span><span class="o">.</span><span class="n">reverseVocab</span>

    <span class="k">for</span> <span class="o">(</span><span class="n">topic</span> <span class="k">&lt;-</span> <span class="mi">0</span> <span class="n">to</span> <span class="n">K</span> <span class="o">-</span> <span class="mi">1</span><span class="o">)</span> <span class="o">{</span>

      <span class="c1">//tie probability to column index, then sort by probabiltiy, take the top numWords, map column index to corresponding word</span>
      <span class="n">println</span><span class="o">(</span><span class="s">&quot;Topic #&quot;</span> <span class="o">+</span> <span class="n">topic</span> <span class="o">+</span> <span class="s">&quot;:  &quot;</span> <span class="o">+</span> <span class="n">corpus</span><span class="o">.</span><span class="n">topicWordMatrix</span><span class="o">(</span><span class="n">topic</span><span class="o">,</span> <span class="o">::).</span><span class="n">t</span><span class="o">.</span><span class="n">toArray</span><span class="o">.</span><span class="n">zipWithIndex</span><span class="o">.</span><span class="n">sortBy</span><span class="o">(-</span><span class="k">_</span><span class="o">.</span><span class="n">_1</span><span class="o">).</span><span class="n">take</span><span class="o">(</span><span class="n">numWords</span><span class="o">).</span><span class="n">toList</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="n">revV</span><span class="o">(</span><span class="n">x</span><span class="o">.</span><span class="n">_2</span><span class="o">)))</span>

    <span class="o">}</span>
  <span class="o">}</span>

  <span class="cm">/**</span>
<span class="cm">   * For a given document, display most likely topics occurring in it.</span>
<span class="cm">   * @param docIndex index of document to be analyzed.</span>
<span class="cm">   * @param probCutoff Determines how likely a topic has to be for it to be displayed.</span>
<span class="cm">   */</span>
  <span class="k">def</span> <span class="n">printTopicProps</span><span class="o">(</span><span class="n">docIndex</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">probCutoff</span><span class="k">:</span> <span class="kt">Double</span><span class="o">)</span> <span class="o">{</span>

    <span class="c1">//tie probability to column index, filter probabilities by probCutoff</span>
    <span class="n">println</span><span class="o">(</span><span class="n">corpus</span><span class="o">.</span><span class="n">docTopicMatrix</span><span class="o">(</span><span class="n">docIndex</span><span class="o">,</span> <span class="o">::).</span><span class="n">t</span><span class="o">.</span><span class="n">toArray</span><span class="o">.</span><span class="n">zipWithIndex</span><span class="o">.</span><span class="n">filter</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">_1</span> <span class="o">&gt;</span> <span class="n">probCutoff</span><span class="o">).</span><span class="n">toList</span><span class="o">)</span>

  <span class="o">}</span>
<span class="o">}</span>
</pre></div>


<p>The full inference procedure is performed in the <code>inference</code> method. First the Gibbs sampling procedure is performed.  This is done by iterating over each word in the corpus and during each iteration, the conditional probability distribution over all topics for that word is calculated with the following equation from <a href="/latent-dirichlet-allocation-in-scala-part-i-the-theory.html">part I</a>,</p>
<p>
<div class="math">$$P(z_i=j|z_{-i},w) \propto \frac{n_{-i,j}^{w_i}+\beta}{n_{-i,j}+W\beta}\frac{n_{-i,j}^{d_i}+\alpha}{n_{-i}^{d_i}+K \alpha}$$</div>
</p>
<p>This is done in the method called <code>gibbsDistribution</code>.  The distribution is returned as a Multinomial distribution (see the scaladocs for <a href="http://www.scalanlp.org/api/breeze/#breeze.stats.distributions.package">Breeze Multinomial distribution</a>).  Then a new topic is sampled from this multinomial distribution and assigned to the word object.  And finally, the doc/topic and topic/word counts are updated to reflect this new assignment (where the word ID from vocabulary is used to associate words with columns in the topic/word matrix).  This sampling procedure runs for 10,000 iterations (so that we can be sure that MCMC convergence has occurred).  The next step in <code>inference</code> is to calculate the <span class="math">\(\theta\)</span> and <span class="math">\(\phi\)</span> probability matrices.  This is done using the count matrices and the following equations from part I,</p>
<p><span class="math">\(\theta_{i,k}=\frac{n_i^k+\alpha_k}{\sum_{k=1}^Kn_i^k+\alpha_k}\)</span> and <span class="math">\(\phi_{k,w}=\frac{n_w^k+\beta_w}{\sum_{w=1}^Wn_w^k+\beta_w}\)</span></p>
<p>The count matrices are transformed into probability matrices in place to save on memory.  The <code>printTopics</code> method prints the topics found from the <span class="math">\(\theta\)</span> matrix in terms of the most likely words for each topic.  The <code>printTopicProps</code> method prints the most likely topics for a particular document within the corpus from the <span class="math">\(\phi\)</span> matrix.</p>
<p>Hopefully this series of blog posts has shed some light on some of the mysteries of Latent Dirichlet Allocation.  The above code snippets were taken from my Scala implementation that you can find at my <a href="https://github.com/alexminnaar/ScalaTopicModels">ScalaTopicModels github repo</a>.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                    </article>
                </div>
            </aside><!-- /#featured -->
            
        
        

    
            <aside id="featured">
                <div class="body">
                    <article>
                        <h1 class="entry-title"><a href="/latent-dirichlet-allocation-in-scala-part-i-the-theory.html">Latent Dirichlet Allocation in Scala Part I - The Theory</a></h1>
<div class="post-info">
	<ul>
        <li class="vcard author">
                 by&nbsp;<a class="url fn" href="/author/alex-minnaar.html">Alex Minnaar</a>
        </li>
        <li class="published" title="2014-06-22T00:00:00+02:00">
          on&nbsp;Sun 22 June 2014
        </li>

	</ul>
<p>Category: <a href="/tag/bayesian-inference.html">   Bayesian Inference</a><a href="/tag/nlp.html">   NLP</a></p>
</div><!-- /.post-info --><p>I wanted to write a two-part blog post on Latent Dirichlet Allocation.  When I was first learning about this algorithm I became somewhat frustrated because it seemed that many learning resources either explained the theory behind Latent Dirichlet Allocation or provided code but none actually explained the connection between the two.  In other words, it was difficult for me to identify the various aspects of the theory behind Latent Dirichlet Allocation algorithm in the code itself.  So the goal of these two blog posts is to both explain the theory behind Latent Dirichlet Allocation and specifically how that theory can be transformed into implementable code.  In this case, I will be using the Scala programming language because that is what I am currently working with professionally.</p>
<p>This first blog entry will focus on the theory behind LDA and the <a href="/latent-dirichlet-allocation-in-scala-part-ii-the-code.html">next post</a> will focus on its transformation into implementable code.</p>
<h2>Why Learn about Latent Dirichlet Allocation (LDA)?</h2>
<p>There are many reasons to want to learn about LDA such as</p>
<ol>
<li>What it does is very useful.  Given a set of documents, it assigns to each topic a distribution over words and to each document a distribution over topics in a completely unsupervised way.  So essentially, it is a way to figure out what each document in a set of documents is about without having to actually read them!  For that reason, it is a great tool to use for a multitude of text categorization tasks.</li>
<li>It is a great practical example of Bayesian inference.  If you are looking to improve your understand Bayesian inference, understanding LDA would definitely help with this.</li>
</ol>
<h2>Probability Distributions involved in LDA</h2>
<p>The two probability distributions used in LDA are the multinomial distribution and the Dirichlet distribution.</p>
<h3>The Multinomial Distribution</h3>
<p>The multinomial distribution models the histogram of outcomes of an experiment with <span class="math">\(k\)</span> possible outcomes that is performed <span class="math">\(T\)</span> times.  The parameters of this distribution are the probabilities of each of the <span class="math">\(k\)</span> possible outcomes occurring in a single trial, <span class="math">\(p_1,...,p_k\)</span>.  This is a discrete, multi-variate distribution with probability mass function</p>
<p><span class="math">\(P(x_1,...,x_k |T,p_1,...,p_k)=\frac{T!}{\prod_{i=1}^kx_i!}p_i^{x_i}\)</span> where <span class="math">\(x_i \geq 0\)</span></p>
<p>A good example of how the multinomial distribution is used is to think of rolling a dice several times.  Say you want to know the probability of rolling 3 fours, 2 sixes, and 4 ones in 9 total rolls given that you know the probability of landing on each side of the dice.  The multinomial distribution models this probability.</p>
<h3>The Dirichlet Distribution</h3>
<p>It is obvious from its name that the Dirichlet distribution is involved in Latent Dirichlet Allocation.  The Dirichlet distribution is sometimes difficult to understand because it deals with distributions over the probability simplex.  What this means is that the Dirichlet distribution is a distribution over discrete probability distributions.  Its probability mass function is</p>
<p><span class="math">\(P(P=\{p_i\}|\alpha_i)=\frac{\prod_i \Gamma(\alpha_i)}{\Gamma(\sum_i \alpha_i)}\prod_ip_i^{\alpha_i-1}\)</span>  where <span class="math">\(\sum_ip_i=1\)</span> and <span class="math">\(p_i \geq 0\)</span></p>
<p>The key to understanding the Dirichlet distribution is realizing that instead of it being a distribution over events or counts (as most probability distributions are), it is a distribution over discrete probability distributions (eg. multinomials).</p>
<p>Another important note is that the Dirichlet distribution is the conjugate prior to the multinomial distribution.  This means that the posterior distribution for a multinomial likelihood with a Dirichlet prior over its parameters is also a Dirichlet distribution.  In fact, the posterior is the following Dirichlet distribution</p>
<p><span class="math">\(P(p_1,...,p_k|x_1,...x_k)=\frac{\prod_i \Gamma(\alpha_i+x_i)}{\Gamma(N+\sum_i \alpha_i)}\prod_ip_i^{\alpha_i+x_i-1}\)</span></p>
<p>The proof is not shown here but it can be derived by multiplying the multinomial likelihood with the Dirichlet prior in a straight-forward way.</p>
<h2>The LDA Generative Model</h2>
<p>LDA uses what is called a generative model as a means to explain how the observed words in a corpus are generated from an underlying latent structure.  The following picture shows how this generative model works.</p>
<p><img alt="alt text" src="images/smoothed_lda.png" title="lda graphical model" /> </p>
<p>This may look complicated at first but every part of this model can be explained. Node <span class="math">\(w\)</span> represents the observed words in the corpus.  It is shaded to indicate that it is observed and not a latent variable.  But before we explore the other nodes in the model, the intuition behind the generative model should be explained.  LDA makes the assumption that a document in a corpus is a distribution over topics and a topic is a distribution over words.  So given a distribution over topics for a particular document, the actual words that appear in the document are generated by first sampling a topic from the distribution over topics then sampling a word from the sampled topic which is itself a distribution over words.  This is a somewhat natural way to think about documents.  A document can contain several different topics with varying proportions and some words are more associated with some topics than others.</p>
<p>Now let's get back into the generative model.  Instead of starting with <span class="math">\(w\)</span> the observed word node, let's start at the outside.  Both <span class="math">\(\beta\)</span> and <span class="math">\(\alpha\)</span> are each parameters of two separate Dirichlet distributions which are represented by the two outer nodes in the generative model.  These two Dirichlet distributions are priors for two multinomial distributions which are parameterized by <span class="math">\(\theta\)</span> and <span class="math">\(\phi\)</span> where <span class="math">\(Multinomial(\theta)\)</span> is the distribution over topics for a given document and <span class="math">\(Multinomial(\phi)\)</span> is the distribution over words for a given topic.  And since there are <span class="math">\(K\)</span> topics and <span class="math">\(M\)</span> documents, there are <span class="math">\(M\)</span> <span class="math">\(\theta\)</span> vectors, each of which are <span class="math">\(K\)</span> elements in length.  Similarly, since there are <span class="math">\(N\)</span> unique words in the corpus, there are <span class="math">\(K\)</span> <span class="math">\(\phi\)</span> vectors, each of which are <span class="math">\(N\)</span> elements in length.  A good way to think of these parameters is as two matrices.</p>
<p><img alt="alt text" src="images/theta.png" title="theta" /> </p>
<p><img alt="alt text" src="images/phi.png" title="phi" /> </p>
<p>Where each row is a parameterization of a multinomial distribution.  It also probably important to note that in the previous section the multinomial distribution was said to have a parameter <span class="math">\(T\)</span> representing the number of trials.  In the case of LDA, <span class="math">\(T=1\)</span> (sometimes this is called a categorical distribution rather than a multinomial distribution).  And finally, <span class="math">\(Z\)</span> is the topic assigned to word <span class="math">\(w\)</span>.</p>
<p>So, in summary, the generative process is the following.</p>
<ol>
<li>All of the Multinomical parameter vectors for all documents <span class="math">\(\theta\)</span> are each sampled from <span class="math">\(Dir(\alpha)\)</span>.</li>
<li>The other Mutlinomial parameter vectors for all topics <span class="math">\(\phi\)</span> are each sampled from <span class="math">\(Dir(\beta)\)</span>.</li>
<li>The for each word index <span class="math">\(n\)</span> in document <span class="math">\(i\)</span> where <span class="math">\(i \in \{1,...,M\}\)</span>.<ol>
<li>The topic assignment for <span class="math">\(n\)</span>, <span class="math">\(z_{n,i}\)</span> is sampled from the Multinomial distribution with parameter <span class="math">\(\theta_i\)</span> corresponding to document <span class="math">\(i\)</span>.</li>
<li>The word with index <span class="math">\(n\)</span> is sampled from the Multinomial distributinon with parameter determined from the topic that was sampled in the last step <span class="math">\(\phi_{z_n,i}\)</span>.</li>
</ol>
</li>
</ol>
<p>And given the above graphical model, it is straight-forward to show that the joint distribution is</p>
<p><span class="math">\(P(\theta,\phi,Z,W|\alpha,\beta)=\prod_{j=1}^KP(\phi_j|\beta)\prod_{i=1}^MP(\theta_i|\alpha)\prod_{n=1}^NP(z_{i,n}|\theta_i)P(w_{i,n}|z_{i,n},\phi_{z_{i,n}})\)</span></p>
<p>Therefore, the ultimate goal is to determine the unknown, hidden parameters <span class="math">\(\theta\)</span>, <span class="math">\(\phi\)</span>, and <span class="math">\(Z\)</span>.  This is done is through a process known as probabilistic inference.  What this basically means is, given that we know the general structure of how the documents are generated and we also know the words that are present in each document, can we work backwards and find the most likely parameters <span class="math">\(\theta\)</span>, <span class="math">\(\phi\)</span>, and <span class="math">\(Z\)</span> that could have generated these documents?  The next section explains how this inference process is carried out for the LDA model.</p>
<h2>Inference - Collapsed Gibbs Sampling</h2>
<p>We want to know the latent parameters <span class="math">\(\theta\)</span>, <span class="math">\(\phi\)</span>, and <span class="math">\(Z\)</span> which represent the topic distributions for each document, the word distributions for each topic and the topic assignments for each word respectively.  But the only information that we initially have is how many times each word appears in each document (LDA uses a bag-of-words approach).  So we need to infer these latent parameters from the observed data.  This means that we want to know <span class="math">\(P(\theta,\phi,Z|W)\)</span> which is known as the posterior distribution.  From the joint distribution shown above, the posterior distribution can be written as</p>
<p><span class="math">\(P(\theta,\phi,Z) | W)=\frac{P(\theta,\phi,Z,W|\alpha,\beta)}{P(W|\alpha,\beta)}\)</span></p>
<p>But it turns out that the denominator <span class="math">\(P(W|\alpha,\beta)\)</span> is intractable so we cannot compute the posterior is this usual way.  This tends to happen a lot in Bayesian inference, so there are a class of approximate inference methods that attempt to compute a distribution that is close to the actual posterior (but much easier to calculate).  In the case of LDA, the original paper by Blei et al. used variational inference to approximate the posterior.  But since then, collapsed Gibbs sampling (a MCMC inference technique) has been more commonly used to do this.  Before we get into the particular collapsed Gibbs sampling inference algorithm, it would be useful to have a quick review of Gibbs sampling and MCMC methods in general.</p>
<h3>Gibbs Sampling</h3>
<p>Gibbs sampling is part of a class of approximate inference methods known as Markov Chain Monte Carlo (MCMC) methods.  Generally speaking, MCMC methods work by creating a Markov Chain whose stationary distribution is the posterior distribution that we are looking for.  Consequently, if you simulate this Markov chain, eventually the state of the Markov chain will look like samples from the desired posterior distribution and given these samples you can get a good idea of what the posterior distribution looks like.  Gibbs sampling specifies a particular way of constructing this Markov chain. Say your target distribution is <span class="math">\(P(\textbf{x})=P(x_1,...,x_n)\)</span> but there is no closed form solution for <span class="math">\(P(\textbf{x})\)</span> (like our posterior distribution) so you cannot sample from it directly.  In Gibbs sampling, if you can sample from the conditional distribution <span class="math">\(P(x_i |x_{-i})\)</span>, a Markov chain can be constructed such that the samples from the conditionals converge to samples from the joint (i.e. the target distribution).    The Markov chain progresses as follows </p>
<ol>
<li>Randomly intialize the first sample <span class="math">\((x_1,...,x_n)\)</span></li>
<li>For each cycle <span class="math">\(t\)</span> until convergence.<ol>
<li>sample <span class="math">\(x_1^t\)</span> from <span class="math">\(P(x_1|x_2^{t-1},...,x_n^{t-1})\)</span></li>
<li>sample <span class="math">\(x_2^t\)</span> from <span class="math">\(P(x_2|x_1^{t-1},x_3^{t-1},...,x_n^{t-1})\)</span></li>
<li>continue until you sample <span class="math">\(x_n^t\)</span> from <span class="math">\(P(x_1|x_1^{t-1},...,x_{n-1}^{t-1})\)</span></li>
</ol>
</li>
</ol>
<p>So each cycle <span class="math">\(t\)</span> produces a new sample of <span class="math">\((x_1,...,x_n)\)</span>.  Eventually the Markov chain will converge and the samples will be very similar to those from the target distribution <span class="math">\(P(\textbf{x})\)</span>.</p>
<h3>Collapsed Gibbs Sampling for LDA</h3>
<p>Now let's return to LDA.  Before the collapsed Gibbs sampling procedure can be described, we must realize an important result.  Take an arbitrary parameter vector <span class="math">\(\theta_{i}\)</span> (i.e. the distribution over topics in document <span class="math">\(i\)</span>).  From the graphical model, we know that the topic assignments <span class="math">\(z_i\)</span> follow a multinomial distribution with a Dirichlet prior on <span class="math">\(\theta_i\)</span> .  These two distributions are conjugate pairs which means that that the posterior distribution of <span class="math">\(\theta_i\)</span> also follow a Dirichlet distribution <span class="math">\(Dir(\theta_i|n_i+\alpha)\)</span> (this is a standard result but you can find a <a href="http://en.wikipedia.org/wiki/Dirichlet_distribution#Conjugate_to_categorical.2Fmultinomial">short proof here</a>) where <span class="math">\(n_i\)</span> is the vector of word counts for document <span class="math">\(i\)</span>.  So, to get an estimate of <span class="math">\(\theta_i\)</span> we can take the expected value of this posterior distribution over <span class="math">\(\theta_i\)</span>.  By the definition of the expected value of a Dirichlet distribution, the estimate for <span class="math">\(\theta_{i,k}\)</span> (the proportion of topic <span class="math">\(k\)</span> in document <span class="math">\(i\)</span>) is 
<div class="math">$$\theta_{i,k}=\frac{n_i^k+\alpha_k}{\sum_{k=1}^Kn_i^k+\alpha_k}$$</div>
</p>
<p>where <span class="math">\(n_i^k\)</span> is the number of words in document <span class="math">\(i\)</span> that have been assigned to topic <span class="math">\(k\)</span>.  And by the exact same argument, the estimate for <span class="math">\(\phi_{k,w}\)</span> (the proportion of word <span class="math">\(w\)</span> in topic <span class="math">\(k\)</span>) is </p>
<p>
<div class="math">$$\phi_{k,w}=\frac{n_w^k+\beta_w}{\sum_{w=1}^Wn_w^k+\beta_w}$$</div>
</p>
<p>where <span class="math">\(n_w^k\)</span> is the number of times word <span class="math">\(w\)</span> is assigned to topic <span class="math">\(k\)</span> (over all documents in the corpus).  So what is the significance of this?  The point is that both of these estimates <span class="math">\(\theta\)</span> and <span class="math">\(\phi\)</span> only depend on the topic assignments <span class="math">\(Z\)</span>.  Therefore, we are able to only focus on inferring the latent variable <span class="math">\(Z\)</span> and the other latent variables can be computed directly from <span class="math">\(Z\)</span>. This makes things much easier.</p>
<p>We can infer the topic assignments <span class="math">\(Z\)</span> from the observed data using collapsed Gibbs sampling.  From the previous section on Gibbs sampling, we need to come up with a conditional distribution that is easy to sample from whose joint distribution is the posterior distribution that we are interested in.  The posterior distribution that we are interested in is <span class="math">\(P(Z|W)\)</span> which is the probability of all of the topic assignments given all of the observed words in all of the documents.  So the conditional that we need to sample from is <span class="math">\(P(z_i=j|z_{-i},w)\)</span>.  But we need to know the form of this distribution to make sure that we can easily sample from it.  So let's derive it. </p>
<p>From Bayes' rule, </p>
<p><span class="math">\(P(z_i=j|z_{-i},w) \propto P(w_i|z_i=j,z_{-i},w_{-i})P(z_i=j|z_{-i})\)</span></p>
<p>Now let's focus on the left-most term.  We can marginalize over <span class="math">\(\phi\)</span> to get </p>
<p><span class="math">\(P(w_i|z_i=j,z_{-i},w_{-i})=\int P(w_i | z_i=j,\phi_j)P(\phi_j |z_{-i},w_{-i})d\phi_j\)</span> </p>
<p>Let's first look at the first term inside the integral.  Since we are conditioning on the <span class="math">\(\phi_j\)</span> parameters, <span class="math">\(P(w_i | z_i=j,\phi_j)=\phi_{w_i,j}\)</span>.  The second term inside the integral <span class="math">\(P(\phi_j |z_{-i},w_{-i})\)</span> is the posterior distribution of a multinomial likelihood combined with a Dirichlet prior (again!) which has a nice closed form solution that we have seen before.  After solving this integral (exact steps ommitted, but not difficult) we get </p>
<p><span class="math">\(P(w_i|z_i=j,z_{-i},w_{-i})=\frac{n_{-i,j}^{w_i}+\beta}{n_{-i,j}+W\beta}\)</span> </p>
<p>Where <span class="math">\(n_{-i,j}^{w_i}\)</span> is the total number of <span class="math">\(w_i\)</span> instances assigned to topic <span class="math">\(j\)</span> not including the current <span class="math">\(w_i\)</span> and <span class="math">\(n_{-i,j}\)</span> is the total number of words assigned to topic <span class="math">\(j\)</span> not including the current word. As for the second term in the first equation <span class="math">\(P(z_i=j|z_{-i})\)</span> , we can follow the same general procedure.  First we marginalize over <span class="math">\(\theta_i\)</span> to get </p>
<p><span class="math">\(P(z_i=j|z_{-i})=\int P(z_i=j|\theta_i)P(\theta_{d_i}|z_{-i})d\theta_{d_i}\)</span> </p>
<p>And just as before, <span class="math">\(P(z_i=j|\theta_{d_i})=\theta_{d_{i},j}\)</span> and <span class="math">\(P(\theta_i|z_{-i})\)</span> is the posterior of a mutlinomial-Dirichlet conjugate pair.  The integral turns out to be </p>
<p><span class="math">\(P(z_i=j|z_{-i})=\frac{n_{-i,j}^{d_i}+\alpha}{n_{-i}^{d_i}+K \alpha}\)</span> </p>
<p>where <span class="math">\(n_{-i,j}^{d_i}\)</span> is the number of words assigned to topic <span class="math">\(j\)</span> in document <span class="math">\(d_i\)</span> not counting the current one and <span class="math">\(n_{-i}^{d_i}\)</span> is the total number of words in document <span class="math">\(d_i\)</span>. Now let's put everything together to get our final conditional distribution for the Gibbs sampler. </p>
<p>
<div class="math">$$P(z_i=j|z_{-i},w) \propto \frac{n_{-i,j}^{w_i}+\beta}{n_{-i,j}+W\beta}\frac{n_{-i,j}^{d_i}+\alpha}{n_{-i}^{d_i}+K \alpha}$$</div>
</p>
<p>Then we carry out the Gibbs sampling procedure as usual to get samples from the posterior</p>
<h2>Putting it all Together</h2>
<p>Hopefully, at this point, all aspects of Latent Dirichlet Allocation are somewhat clear.  In terms of implementation, everything hinges on the Gibbs sampling inference step.  The topic assignments are initialized randomly.  Then the Gibbs procedure is run while keeping track of the required document and topic counts.  Once the sampling has converged, the document/topic distributions <span class="math">\(\theta\)</span> and topic/word distributions <span class="math">\(\phi\)</span> can be trivially computed from the learned topic assignments.  The following pseudocode outlines this process in greater detail. </p>
<p><img alt="alt text" src="images/lda_algorithm.png" title="lda algorithm" /> </p>
<p>And, as stated previously, these topic assignments can be used to compute the counts necessary to determine the other hidden variables <span class="math">\(\theta\)</span> and <span class="math">\(\phi\)</span>. See <a href="/latent-dirichlet-allocation-in-scala-part-ii-the-code.html">Part II of this blog post</a> which explains how to translate this theory into Scala code.</p>
<h3>Resources</h3>
<ul>
<li>The original Paper on LDA that uses variational inference instead of Gibbs sampling.  <a href="http://machinelearning.wustl.edu/mlpapers/paper_files/BleiNJ03.pdf">Latent Dirichlet Allocation - Blei, Ng, Jordan</a></li>
<li>The original paper on collapsed Gibbs sampling for LDA.  <a href="http://psiexp.ss.uci.edu/research/papers/SteyversGriffithsLSABookFormatted.pdf">Probabilistic Topic Models - Griffiths, Steyvers</a></li>
<li>Another nice technical report that shows much more detail than what I have shown.  <a href="http://http://www.arbylon.net/publications/text-est.pdf">Parameter Estimation for Text Analysis - Heinrich</a></li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                    </article>
                </div>
            </aside><!-- /#featured -->
            
        
        

    
            <aside id="featured">
                <div class="body">
                    <article>
                        <h1 class="entry-title"><a href="/time-series-classification-and-clustering-with-python.html">Time Series Classification and Clustering with Python</a></h1>
<div class="post-info">
	<ul>
        <li class="vcard author">
                 by&nbsp;<a class="url fn" href="/author/alex-minnaar.html">Alex Minnaar</a>
        </li>
        <li class="published" title="2014-04-16T00:00:00+02:00">
          on&nbsp;Wed 16 April 2014
        </li>

	</ul>
<p>Category: <a href="/tag/supervised-learning.html">   Supervised Learning</a></p>
</div><!-- /.post-info --><p>I recently ran into a problem at work where I had to predict whether an account would churn  in the near future given the account's time series usage in a certain time interval.  So this is a binary-valued classification problem (i.e. churn or not churn) with a time series as a predictor.  This was not a very straight-forward problem to tackle because it seemed like there two possible strategies to employ.</p>
<ol>
<li>Extract features from the time series like its mean, maximum, minimum, and other differential features.  Then use well-known classification algorithms (Naive Bayes, SVMs, etc.) with these features to make a prediction.</li>
<li>Use a k-NN approach.  For a given time series example that you want to predict, find the most similar time series in the training set and use its corresponding output as the prediction.</li>
</ol>
<p>I tried both of these strategies and the latter produced the best results.  However this approach is not as simple as it may seem.  This is because finding a good similarity measure between time series is a very non-trivial task.</p>
<h2>Finding a Similarity Measure</h2>
<p>A naive choice for a similarity measure would be Euclidean distance.  The following example will show why this choice is not optimal.  Consider the following of 3 time series.</p>
<p><img alt="alt text" src="images/3_ts.png" title="time series examples" /> </p>
<p>In the above example, it is clear that <span class="math">\(ts1\)</span> and <span class="math">\(ts2\)</span> are most similar (they are both sin functions under different transformations). <span class="math">\(ts3\)</span> is clearly the most different. Let's compute the Euclidean distance <span class="math">\(d(ts1,ts2)\)</span> and <span class="math">\(d(ts1,ts3)\)</span> to see if the Euclidean distance measure agrees with what our intuition tells us. Let's first create a function that computes the Euclidean distance between two time series using</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">euclid_dist</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span><span class="n">t2</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">sqrt</span><span class="p">(</span><span class="nb">sum</span><span class="p">((</span><span class="n">t1</span><span class="o">-</span><span class="n">t2</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span>
</pre></div>


<p>It turns out that <span class="math">\(d(ts1,ts2)=26.9\)</span> and <span class="math">\(d(ts1,ts3)=23.2\)</span>. This is not good because according to the Euclidean distance measure, <span class="math">\(ts1\)</span> is more similar to <span class="math">\(ts3\)</span> than to <span class="math">\(ts2\)</span> which contradicts our intuition. This is the problem with using the Euclidean distance measure. It often produced pessimistic similarity measures when it encounters distortion in the time axis. The way to deal with this is to use dynamic time warping.</p>
<h3>Dynamic Time Warping</h3>
<p>Dynamic time warping finds the optimal non-linear alignment between two time series. The Euclidean distances between alignments are then much less susceptible to pessimistic similarity measurements due to distortion in the time axis. There is a price to pay for this, however, because dynamic time warping is quadratic in the length of the time series used.</p>
<p>Dynamic time warping works in the following way. Consider two time series <span class="math">\(Q\)</span> and <span class="math">\(C\)</span> of the same length <span class="math">\(n\)</span> where <span class="math">\(Q=q_1,q_2,...,q_n\)</span> and <span class="math">\(C=c_1,c_2,...,c_n\)</span> The first thing we do is construct an <span class="math">\(n\times n\)</span> matrix whose <span class="math">\(i,j^{th}\)</span> element is the Euclidean distance between <span class="math">\(q_i\)</span> and <span class="math">\(c_j\)</span>. We want to find a path through this matrix that minimizes the cumulative distance. This path then determines the optimal alignment between the two time series. It should be noted that it is possible for one point in a time series to be mapped to multiple points in the other time series.</p>
<p>Let's call the path <span class="math">\(W\)</span> where <span class="math">\(W=w_1,w_2,...,w_K\)</span> where each element of <span class="math">\(W\)</span> represents the distance between a point <span class="math">\(i\)</span> in <span class="math">\(Q\)</span> and a point <span class="math">\(j\)</span> in <span class="math">\(C\)</span> i.e. <span class="math">\(w_k=(q_i-c_j)^2\)</span></p>
<p>So we want to find the path with the minimum Euclidean distance <span class="math">\(W^*=argmin_W(\sqrt{\sum_{k=1}^Kw_k})\)</span> The optimal path is found via dynamic programming, specifically the following recursive function. <span class="math">\(\gamma(i,j)=d(q_i,c_j)+min ( \gamma(i-1,j-1),\gamma(i-1,j),\gamma(i,j-1))\)</span>. This can be implemented via the following python function.</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">DTWDistance</span><span class="p">(</span><span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">):</span>
    <span class="n">DTW</span><span class="o">=</span><span class="p">{}</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">s1</span><span class="p">)):</span>
        <span class="n">DTW</span><span class="p">[(</span><span class="n">i</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s">&#39;inf&#39;</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">s2</span><span class="p">)):</span>
        <span class="n">DTW</span><span class="p">[(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">i</span><span class="p">)]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s">&#39;inf&#39;</span><span class="p">)</span>
    <span class="n">DTW</span><span class="p">[(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">s1</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">s2</span><span class="p">)):</span>
            <span class="n">dist</span><span class="o">=</span> <span class="p">(</span><span class="n">s1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">s2</span><span class="p">[</span><span class="n">j</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span>
            <span class="n">DTW</span><span class="p">[(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)]</span> <span class="o">=</span> <span class="n">dist</span> <span class="o">+</span> <span class="nb">min</span><span class="p">(</span><span class="n">DTW</span><span class="p">[(</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">j</span><span class="p">)],</span><span class="n">DTW</span><span class="p">[(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="n">DTW</span><span class="p">[(</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">)])</span>

    <span class="k">return</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">DTW</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">s1</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">s2</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>


<p>The dynamic time warping Euclidean distances between the time series are <span class="math">\(DTWDistance(ts1,ts2)=17.9\)</span> and <span class="math">\(DTWDistance(ts1,ts3)=21.5\)</span>. As you can see, our results have changed from when we only used the Euclidean distance measure. Now, in agreement with our intuition, <span class="math">\(ts2\)</span> is shown to be more similar to <span class="math">\(ts1\)</span> than <span class="math">\(ts3\)</span> is.</p>
<h3>Speeding Up Dynamic Time Warping</h3>
<p>Dynamic time warping has a complexity of <span class="math">\(O(nm)\)</span> where <span class="math">\(n\)</span> is the length of the first time series and <span class="math">\(m\)</span> is the length of the second time series. If you are performing dynamic time warping multiple times on long time series data, this can be prohibitively expensive. However, there are a couple of ways to speed things up. The first is to enforce a locality constraint. This works under the assumption that it is unlikely for <span class="math">\(q_i\)</span> and <span class="math">\(c_j\)</span> to be matched if <span class="math">\(i\)</span> and <span class="math">\(j\)</span> are too far apart. The threshold is determined by a window size <span class="math">\(w\)</span>. This way, only mappings within this window are considered which speeds up the inner loop. The following is the modified code which includes the window size <span class="math">\(w\)</span>.</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">DTWDistance</span><span class="p">(</span><span class="n">s1</span><span class="p">,</span> <span class="n">s2</span><span class="p">,</span><span class="n">w</span><span class="p">):</span>
    <span class="n">DTW</span><span class="o">=</span><span class="p">{}</span>

    <span class="n">w</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="nb">abs</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">s1</span><span class="p">)</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">s2</span><span class="p">)))</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">s1</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="nb">len</span><span class="p">(</span><span class="n">s2</span><span class="p">)):</span>
            <span class="n">DTW</span><span class="p">[(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)]</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="s">&#39;inf&#39;</span><span class="p">)</span>
    <span class="n">DTW</span><span class="p">[(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">s1</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="o">-</span><span class="n">w</span><span class="p">),</span> <span class="nb">min</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">s2</span><span class="p">),</span> <span class="n">i</span><span class="o">+</span><span class="n">w</span><span class="p">)):</span>
            <span class="n">dist</span><span class="o">=</span> <span class="p">(</span><span class="n">s1</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">-</span><span class="n">s2</span><span class="p">[</span><span class="n">j</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span>
            <span class="n">DTW</span><span class="p">[(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">)]</span> <span class="o">=</span> <span class="n">dist</span> <span class="o">+</span> <span class="nb">min</span><span class="p">(</span><span class="n">DTW</span><span class="p">[(</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">j</span><span class="p">)],</span><span class="n">DTW</span><span class="p">[(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">)],</span> <span class="n">DTW</span><span class="p">[(</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">j</span><span class="o">-</span><span class="mi">1</span><span class="p">)])</span>

    <span class="k">return</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">DTW</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">s1</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">s2</span><span class="p">)</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</pre></div>


<p>Another way to speed things up is to use the LB Keogh lower bound of dynamic time warping. It is defined as </p>
<p><span class="math">\(LBKeogh(Q,C)=\sum_{i=1}^n (c_i-U_i)^2I(c_i &gt; U_i)+(c_i-L_i)^2I(c_i &lt; L_i)\)</span> </p>
<p>where <span class="math">\(U_i\)</span> and <span class="math">\(L_i\)</span> are upper and lower bounds for time series <span class="math">\(Q\)</span> which are defined as <span class="math">\(U_i=max(q_{i-r}:q_{i+r})\)</span> and <span class="math">\(L_i=min(q_{i-r}:q_{i+r})\)</span> for a reach <span class="math">\(r\)</span> and <span class="math">\(I(\cdot)\)</span> is the indicator function. It can be implemented with the following function.</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">LB_Keogh</span><span class="p">(</span><span class="n">s1</span><span class="p">,</span><span class="n">s2</span><span class="p">,</span><span class="n">r</span><span class="p">):</span>
    <span class="n">LB_sum</span><span class="o">=</span><span class="mi">0</span>
    <span class="k">for</span> <span class="n">ind</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">s1</span><span class="p">):</span>

        <span class="n">lower_bound</span><span class="o">=</span><span class="nb">min</span><span class="p">(</span><span class="n">s2</span><span class="p">[(</span><span class="n">ind</span><span class="o">-</span><span class="n">r</span> <span class="k">if</span> <span class="n">ind</span><span class="o">-</span><span class="n">r</span><span class="o">&gt;=</span><span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span><span class="p">):(</span><span class="n">ind</span><span class="o">+</span><span class="n">r</span><span class="p">)])</span>
        <span class="n">upper_bound</span><span class="o">=</span><span class="nb">max</span><span class="p">(</span><span class="n">s2</span><span class="p">[(</span><span class="n">ind</span><span class="o">-</span><span class="n">r</span> <span class="k">if</span> <span class="n">ind</span><span class="o">-</span><span class="n">r</span><span class="o">&gt;=</span><span class="mi">0</span> <span class="k">else</span> <span class="mi">0</span><span class="p">):(</span><span class="n">ind</span><span class="o">+</span><span class="n">r</span><span class="p">)])</span>

        <span class="k">if</span> <span class="n">i</span><span class="o">&gt;</span><span class="n">upper_bound</span><span class="p">:</span>
            <span class="n">LB_sum</span><span class="o">=</span><span class="n">LB_sum</span><span class="o">+</span><span class="p">(</span><span class="n">i</span><span class="o">-</span><span class="n">upper_bound</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
        <span class="k">elif</span> <span class="n">i</span><span class="o">&lt;</span><span class="n">lower_bound</span><span class="p">:</span>
            <span class="n">LB_sum</span><span class="o">=</span><span class="n">LB_sum</span><span class="o">+</span><span class="p">(</span><span class="n">i</span><span class="o">-</span><span class="n">lower_bound</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

    <span class="k">return</span> <span class="n">sqrt</span><span class="p">(</span><span class="n">LB_sum</span><span class="p">)</span>
</pre></div>


<p>The LB Keogh lower bound method is linear whereas dynamic time warping is quadratic in complexity which make it very advantageous for searching over large sets of time series.</p>
<h2>Classification and Clustering</h2>
<p>Now that we have a reliable method to determine the similarity between two time series, we can use the k-NN algorithm for classification. Empirically, the best results have come when <span class="math">\(k=1\)</span>. The following is the 1-NN algorithm that uses dynamic time warping Euclidean distance. In this algorithm, <span class="math">\(train\)</span> is the training set of time series examples where the class that the time series belongs to is appended to the end of the time series. <span class="math">\(test\)</span> is the test set whose corresponding classes you are trying to predict. In this algorithm, for every time series in the test set, a search must be performed through all points in the training set so that the most similar point is found. Given that dynamic time warping is quadratic, this can be very computationally expensive. We can speed up classification using the LB Keogh lower bound. Computing LB Keogh is much less expensive than performing dynamic time warping. And since <span class="math">\(LB Keogh(Q,C) \leq DTW(Q,C)\)</span> , we can eliminate time series that cannot possibly be more similar that the current most similar time series. In this way we are eliminating many unnecessary dynamic time warping computations.</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span>

<span class="k">def</span> <span class="nf">knn</span><span class="p">(</span><span class="n">train</span><span class="p">,</span><span class="n">test</span><span class="p">,</span><span class="n">w</span><span class="p">):</span>
    <span class="n">preds</span><span class="o">=</span><span class="p">[]</span>
    <span class="k">for</span> <span class="n">ind</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">test</span><span class="p">):</span>
        <span class="n">min_dist</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="s">&#39;inf&#39;</span><span class="p">)</span>
        <span class="n">closest_seq</span><span class="o">=</span><span class="p">[]</span>
        <span class="c">#print ind</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="n">train</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">LB_Keogh</span><span class="p">(</span><span class="n">i</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">j</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="mi">5</span><span class="p">)</span><span class="o">&lt;</span><span class="n">min_dist</span><span class="p">:</span>
                <span class="n">dist</span><span class="o">=</span><span class="n">DTWDistance</span><span class="p">(</span><span class="n">i</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">j</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">w</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">dist</span><span class="o">&lt;</span><span class="n">min_dist</span><span class="p">:</span>
                    <span class="n">min_dist</span><span class="o">=</span><span class="n">dist</span>
                    <span class="n">closest_seq</span><span class="o">=</span><span class="n">j</span>
        <span class="n">preds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">closest_seq</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">classification_report</span><span class="p">(</span><span class="n">test</span><span class="p">[:,</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">preds</span><span class="p">)</span>
</pre></div>


<p>Now let's test it on some data. We will use a window size of 4. Although the code is sped up with the use of the LB Keogh bound and the dynamic time warping locality constraint, it may still take a few minutes to run.</p>
<div class="highlight"><pre><span class="n">train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s">&#39;datasets/train.csv&#39;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s">&#39;</span><span class="se">\t</span><span class="s">&#39;</span><span class="p">)</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s">&#39;datasets/test.csv&#39;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s">&#39;</span><span class="se">\t</span><span class="s">&#39;</span><span class="p">)</span>
<span class="k">print</span> <span class="n">knn</span><span class="p">(</span><span class="n">train</span><span class="p">,</span><span class="n">test</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
</pre></div>


<p>The result is</p>
<p><img alt="alt text" src="images/perfromance.jpg" title="performance" /> </p>
<p>The same idea can also be applied to k-means clustering. In this algorithm, the number of clusters is set apriori and similar time series are clustered together.</p>
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">random</span>

<span class="k">def</span> <span class="nf">k_means_clust</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">num_clust</span><span class="p">,</span><span class="n">num_iter</span><span class="p">,</span><span class="n">w</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span>
    <span class="n">centroids</span><span class="o">=</span><span class="n">random</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="n">num_clust</span><span class="p">)</span>
    <span class="n">counter</span><span class="o">=</span><span class="mi">0</span>
    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iter</span><span class="p">):</span>
        <span class="n">counter</span><span class="o">+=</span><span class="mi">1</span>
        <span class="k">print</span> <span class="n">counter</span>
        <span class="n">assignments</span><span class="o">=</span><span class="p">{}</span>
        <span class="c">#assign data points to clusters</span>
        <span class="k">for</span> <span class="n">ind</span><span class="p">,</span><span class="n">i</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
            <span class="n">min_dist</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="s">&#39;inf&#39;</span><span class="p">)</span>
            <span class="n">closest_clust</span><span class="o">=</span><span class="bp">None</span>
            <span class="k">for</span> <span class="n">c_ind</span><span class="p">,</span><span class="n">j</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">centroids</span><span class="p">):</span>
                <span class="k">if</span> <span class="n">LB_Keogh</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span><span class="o">&lt;</span><span class="n">min_dist</span><span class="p">:</span>
                    <span class="n">cur_dist</span><span class="o">=</span><span class="n">DTWDistance</span><span class="p">(</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">,</span><span class="n">w</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">cur_dist</span><span class="o">&lt;</span><span class="n">min_dist</span><span class="p">:</span>
                        <span class="n">min_dist</span><span class="o">=</span><span class="n">cur_dist</span>
                        <span class="n">closest_clust</span><span class="o">=</span><span class="n">c_ind</span>
            <span class="k">if</span> <span class="n">closest_clust</span> <span class="ow">in</span> <span class="n">assignments</span><span class="p">:</span>
                <span class="n">assignments</span><span class="p">[</span><span class="n">closest_clust</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ind</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">assignments</span><span class="p">[</span><span class="n">closest_clust</span><span class="p">]</span><span class="o">=</span><span class="p">[]</span>

        <span class="c">#recalculate centroids of clusters</span>
        <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">assignments</span><span class="p">:</span>
            <span class="n">clust_sum</span><span class="o">=</span><span class="mi">0</span>
            <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">assignments</span><span class="p">[</span><span class="n">key</span><span class="p">]:</span>
                <span class="n">clust_sum</span><span class="o">=</span><span class="n">clust_sum</span><span class="o">+</span><span class="n">data</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>
            <span class="n">centroids</span><span class="p">[</span><span class="n">key</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="n">m</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">assignments</span><span class="p">[</span><span class="n">key</span><span class="p">])</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">clust_sum</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">centroids</span>
</pre></div>


<p>Let's test it on the entire data set (i.e. the training set and the test set stacked together).</p>
<div class="highlight"><pre><span class="n">train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s">&#39;datasets/train.csv&#39;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s">&#39;</span><span class="se">\t</span><span class="s">&#39;</span><span class="p">)</span>
<span class="n">test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">genfromtxt</span><span class="p">(</span><span class="s">&#39;datasets/test.csv&#39;</span><span class="p">,</span> <span class="n">delimiter</span><span class="o">=</span><span class="s">&#39;</span><span class="se">\t</span><span class="s">&#39;</span><span class="p">)</span>
<span class="n">data</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">((</span><span class="n">train</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span><span class="n">test</span><span class="p">[:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">]))</span>

<span class="kn">import</span> <span class="nn">matplotlib.pylab</span> <span class="kn">as</span> <span class="nn">plt</span>

<span class="n">centroids</span><span class="o">=</span><span class="n">k_means_clust</span><span class="p">(</span><span class="n">data</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">centroids</span><span class="p">:</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p><img alt="alt text" src="images/cluster.png" title="cluster" /> </p>
<h3>Code</h3>
<p>The code used in this blog post can be found in <a href="https://github.com/alexminnaar/time-series-classification-and-clustering">my gitHub repo</a>.</p>
<h3>References</h3>
<p>The vast majority of research in this area is done by Dr. Eamonn Keogh's group at UC Riverside.  All of the relevant papers are referenced in <a href="http://www.cs.ucr.edu/~eamonn/time_series_data/">the group's webpage</a>.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                    </article>
                </div>
            </aside><!-- /#featured -->
            
        
        

    
            <aside id="featured">
                <div class="body">
                    <article>
                        <h1 class="entry-title"><a href="/my-experience-with-churn-analysis.html">My Experience with Churn Analysis</a></h1>
<div class="post-info">
	<ul>
        <li class="vcard author">
                 by&nbsp;<a class="url fn" href="/author/alex-minnaar.html">Alex Minnaar</a>
        </li>
        <li class="published" title="2014-03-30T00:00:00+01:00">
          on&nbsp;Sun 30 March 2014
        </li>

	</ul>
<p>Category: <a href="/tag/supervised-learning.html">   Supervised Learning</a></p>
</div><!-- /.post-info --><p>A large chunk of my time at my last job was devoted to churn analysis and I wanted to use this blog entry to explain how I approached the various problems that it presented.  This is not meant to be a very technical post and the reasoning behind this is two-fold</p>
<ol>
<li>Obviously I do not have permission to use any company data and there is not really any good publicly-available churn datasets on the web.  Presenting technical code without data to run it on would not really make sense.</li>
<li>I have learned that churn analysis is very domain-specific and I want to make sure that what I say generalizes to many use-cases.</li>
</ol>
<p>Before I explain what I did, I should first define what churn is and the specific goals that I had in mind.</p>
<h2>What is Churn?</h2>
<p>Churn is a term that generally describes the process where customers stop using the products and/or services provided by a business.  However, it is of most interest in subscription-based services like phone plans, video games, etc.  In these services it is easy to know when a customer has churned i.e. when they cancel their subscription.  Needless to say, churn is bad for business.  Every company has a customer CPA (Cost Per Acquisition) so in order to replace a churned customer with a new customer, this cost must be paid.   Clearly it is cheaper for companies to keep customers than to replace them.  Churn analysis is used to attempt to answer the following questions</p>
<ol>
<li>Why are customers churning?</li>
<li>Can we predict customers who will churn in the near future (and then maybe convince them to stay with us)?</li>
<li>How long can we expect a given customer to stay with us?</li>
</ol>
<p>These are very important questions and if reliable answers can be obtained, they would be of great value.  We will also see that these main questions are closely linked to some other slightly different yet equally important questions such as</p>
<ol>
<li>What is the lifetime value of a given customer?</li>
<li>Who are our most valuable customers and who are out least valuable customers? What accounts for these differences?</li>
</ol>
<h2>Question:  Can we predict which customers will churn in the near future?</h2>
<p>Predicting which of your current customers will churn is a binary classification problem.  As it stands, this is an ill-defined problem.  This is because of the simple fact that in subscription-based services ALL CUSTOMERS WILL CANCEL EVENTUALLY!  You could have a classifier that predicts that all currently active accounts will cancel and it would be 100% correct!  But obviously this would be useless to a company.  What companies really want to know is which of the currently active accounts will cancel "soon".  This way companies can take action in an effort to prevent cancellation from occurring.  The specific preventative action that should be taken is beyond the scope of this blog post but the prediction problem itself will be explored.</p>
<h3>Dealing with the Time Component</h3>
<p>The first thing that you need to do is define a time period.  There is a trade-off here.  You want to know who is going to cancel as soon as possible so that you have the maximum amount of time to take preventative action. However if you predict too early, your predictions will be of lower quality.  This is because (in most cases) churn indicators become clearer the closer the customer is to his/her actual cancel date.  On the other hand, if you predict too late, your predictions will be more reliable but it will give you less time to take preventative action.  You need to decide on a good balance which most likely depends on your domain.  I can say that in the telecom domain a 2 week window is generally enough time to perform preventative action.</p>
<p>Once you have dealt with the time component, the classification problem becomes more well-defined however there is still a bit more work that needs to be done.</p>
<h3>Defining Positive and Negative Examples</h3>
<p>In any classification problem you need to build a training set of positive and negative examples.  It is clear that negative examples will come from customers that have churned in the past.  However it is a bit unclear what the positive examples should be.  You might initially think that we can use the currently active accounts as positive examples.  This is problematic sinse ultimately these are the accounts we will test on so we can't really use them for training as they are.</p>
<p>What you need to do is identify your long-time customers (they will most likely be currently active, but they could also have cancelled after using your service for a long time).  However, as previously stated, you cannot use them as they are because you are going to test on them.  You need to use the truncated versions of these examples as positive examples.  For example, if you have a long-time customer that has been active for two years, use this customer's behaviour from their first 365 days as a positive example.  In this way, you obtain positive examples of customers that you know will not cancel for a long time.  Also, testing will generally be done on an active customer's recent behaviour, so you are mitigating the risk of overfitting by training on that customer's past behaviour.</p>
<p>Now that you know what your positive and negative examples are you must extract relevant feature from them.</p>
<h3>Feature Extraction</h3>
<p>The feature extraction process is the most important part of this problem and, unfortunately, also the most unsystematic.  If you have dealt with supervised learning problems before you know that feature extraction is as much an art as it is a science.  Feature extraction is very domain-specific.  In some cases the relevant features that indicate churn likelihood are obvious, in others it is less clear.  It would be wise to consult someone with good domain expertise before you decide on the features you will use.  I will list some of the features that I found to be good indicators of churn in the telecom domain.</p>
<h4>Static Features</h4>
<p>Static features are features that are not time-dependent.</p>
<ul>
<li>Age at activation date</li>
<li>Lead source</li>
<li>Type of phone</li>
<li>Number of phones attached to account</li>
<li>Location</li>
<li>Credit card type</li>
</ul>
<h4>Usage-based Features</h4>
<p>Usage-based features deal with the customer's time-dependent usage patterns.</p>
<ul>
<li>Date of last usage</li>
<li>Max and min daily usage amount</li>
<li>Average usage amount over last 30 days</li>
<li>Average usage amount over last 30 days / overall average</li>
<li>Number of support tickets issued</li>
<li>Number support tickets in last 30 days / total # of support tickets</li>
<li>Max # of days without any usage</li>
<li>Current # of days without any usage / max # days without any usage</li>
</ul>
<p>However, as I said earlier, feature extraction is a very domain-specific problem so there is no guarantee that these features will be useful in your particular use case.</p>
<h3>The Class Imbalance Problem</h3>
<p>In almost all applications of this problem you will find that you have many more active accounts than cancelled accounts.  Therefore you will have many more positive training examples than negative training examples.  This is problematic because any classifier that predicts that no customers will churn will perform very well.  Consider the case where you apply the classifier to a set of 100 accounts - 90 that will not cancel in the next 2 weeks and 10 that will.  If the classifier predicts that all 100 will not cancel, it would have an accuracy of 90%.  Even though the classifier is very accurate, it is of little use because we need to identify these 10 accounts that are going to cancel.  This is called the class imbalance problem.</p>
<p>There has been a fair amount of research into this problem and survey of possible solutions can be found <a href="http://marmota.dlsi.uji.es/WebBIB/papers/2007/1_GarciaTamida2007.pdf">here</a>.  I have found that over-sampling the negative examples works well.  Specifically, I used stratified sampling on the negative examples such that my final training set contains a certain percentage of negative examples.  There is another trade-off here.  The higher the percentage of negative examples, the more false negatives (incorrectly predicting that a customer will churn) you will generate.  But if the percentage is too low, you will miss accounts that will cancel.  You must decide the threshold of false negatives that you can tolerate.</p>
<h3>Putting it All Together</h3>
<p>Now that you have defined your positive and negative examples, extracted features and dealt with the class imbalance problem, you can finally build your model.  The particular model that you choose is up to you.  I have found that random forests perform well in most applications.  The best results generally come from an ensemble of multiple models.  Obviously you would want to perform the usual train/test set splitting, cross-validation and parameter tuning that is required to reduce overfitting.  Once your model is trained up to your standards, you will apply it to your set of currently active customers.</p>
<p>You also want to decide how often to run this classifier.  Since the usage-based features are constantly changing, running the classifier frequently would be a good idea.  However, if you notice that no new accounts are being flagged, it might be a good idea to run it less frequently.  But if you have the capacity, there is really no downside to running it as often as possible.</p>
                    </article>
                </div>
            </aside><!-- /#featured -->
            
        
        

    
            <aside id="featured">
                <div class="body">
                    <article>
                        <h1 class="entry-title"><a href="/facebook-recruiting-iii-keyword-extraction-part-7.html">Facebook Recruiting III Keyword Extraction - Part 7</a></h1>
<div class="post-info">
	<ul>
        <li class="vcard author">
                 by&nbsp;<a class="url fn" href="/author/alex-minnaar.html">Alex Minnaar</a>
        </li>
        <li class="published" title="2013-11-24T00:00:00+01:00">
          on&nbsp;Sun 24 November 2013
        </li>

	</ul>
<p>Category: <a href="/tag/kaggle-competitions.html">   Kaggle Competitions</a><a href="/tag/nlp.html">   NLP</a></p>
</div><!-- /.post-info --><h2>Parameter Tuning</h2>
<p>Now the time has come to perform an actual out-of-sample test of our association rule algorithm.  The training data set has about 6 million rows so the association rule algorithm will be trained on 5 million rows and the remaining 1 million rows will be held out for testing.  From this we can accomplish two goals</p>
<p>It gives us an idea of how well our algorithm will perform on the actual test set without actually having to submit anything on the Kaggle website (you are only allowed 2 submissions per day).
It allows us to tune the parameters of our algorithm to achieve the best performance on our hold-out data set.
However, before we start tuning parameters we should first formalize our classification algorithm.</p>
<h3>Classification Algorithm</h3>
<p>The classification algorithm is fairly simple.  The following pseudo-code illustrates how it works</p>
<div class="highlight"><pre>I={}
For r in AR:
     if C(r)&gt;α AND supp(r)&gt;β AND r not in I:
          I.append(r)
</pre></div>


<p>The input to the algorithm is the learned association rules from both the post title and post body <span class="math">\(AR\)</span>.  <span class="math">\(I\)</span> denotes the set of tags that will be predicted for the given post, so initially <span class="math">\(I\)</span> is empty.  The algorithm searches through <span class="math">\(AR\)</span> to find association rules <span class="math">\(r\)</span> with confidence above a certain threshold <span class="math">\(\alpha\)</span> and support above a certain threshold <span class="math">\(\beta\)</span>.  The tags in these association rules are then added to the set <span class="math">\(I\)</span> which become the predicted tags for this post.</p>
<p>So we need to find the optimal parameters <span class="math">\(\alpha\)</span> and <span class="math">\(\beta\)</span>.  One way to do this is to vary these parameters and see how their values affect the test error.  The parameter values that produce the best test performance are the ones we would use.  Before we can do this, we must decide on a loss function.  Kaggle says that the loss function that submissions are scored upon is the mean F1-score loss function.  The mean F1-score loss function is defined as</p>
<p><span class="math">\(F1=2\frac{p \cdot r}{p+r}\)</span> where <span class="math">\(p=\frac{tp}{tp+fp}\)</span> and <span class="math">\(r=\frac{tp}{tp+fn}\)</span></p>
<p>This loss function means that both precision (<span class="math">\(p\)</span>) and recall (<span class="math">\(r\)</span>) are weighted equally so both must be optimized in order to obtain the best score. The F1-score can be written in Python code as the following</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">F1_score</span><span class="p">(</span><span class="n">tags</span><span class="p">,</span><span class="n">predicted</span><span class="p">):</span>
    <span class="n">tags</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">tags</span><span class="p">)</span>
    <span class="n">predicted</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">predicted</span><span class="p">)</span>
    <span class="n">tp</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tags</span> <span class="o">&amp;</span> <span class="n">predicted</span><span class="p">)</span>
    <span class="n">fp</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">predicted</span><span class="p">)</span> <span class="o">-</span> <span class="n">tp</span>
    <span class="n">fn</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">tags</span><span class="p">)</span> <span class="o">-</span> <span class="n">tp</span>

    <span class="k">if</span> <span class="n">tp</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
        <span class="n">precision</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">tp</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">tp</span><span class="o">+</span><span class="n">fp</span><span class="p">)</span>
        <span class="n">recall</span><span class="o">=</span><span class="nb">float</span><span class="p">(</span><span class="n">tp</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">tp</span><span class="o">+</span><span class="n">fn</span><span class="p">)</span>

        <span class="k">return</span> <span class="mi">2</span><span class="o">*</span><span class="p">((</span><span class="n">precision</span><span class="o">*</span><span class="n">recall</span><span class="p">)</span><span class="o">/</span><span class="p">(</span><span class="n">precision</span><span class="o">+</span><span class="n">recall</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="mi">0</span>
</pre></div>


<p>predicted is the list of tags that the classification algorithm predicts for a given post in the hold-out set and tags is the list of actual tags for that post.  These lists are then converted to Python sets so that tp, fp, and fn can be computed efficiently.</p>
<h3>Varying the Parameters</h3>
<p>Now that we have a cost function, we can run some tests to calibrate our parameters α and β.  As stated earlier, we split the data set into a training set and test set - mine association rules from the training set and test their performance on the test set.</p>
<h3>Why not use cross-validation?</h3>
<p>Usually one should use k-fold cross-validation to learn the optimal parameter values as it would give the most unbiased representation of the algorithm's performance on unseen data.  However, 5-fold cross-validation was tried initially but it became clear that validation errors for each of the 5 validation sets were very similar so taking the average would not be much different than the error of one of the validation sets.  For this reason (and the fact that each test will take 1/5th of the time), a single test set was used instead.</p>
<h2>Results</h2>
<p>After considering many combinations of support and confidence values, the best F1-score was found to be <strong>0.74021</strong> with confidence=0.5 and support=5.  The number 1 score was 0.81350 so I was not too far off.</p>
<p>There were many different strategies that I wanted to try but did not have time to implement.  Here is a list of improvements/alternative strategies that I would have liked to try if I had enough time.</p>
<ul>
<li>About 50,000 posts in the test set were predicted to have zero tags because there were no association rules for them that fell within the acceptable threshold.  These posts were scored as zero.  However, F1-score can never be negative so any random guess of tags could only improve the score.  A naive solution would be to simply use the baseline tags (i.e. javascript c# python php java) as predictions for these posts, however I suspect that there would be a more clever way of doing this.</li>
<li>The opposite problem also occurs in some predictions.  Some posts are predicted to have 8, 9, 10+ tags.  Given what we know from the training set and this kaggle thread, the maximum number of tags is 5 (and it is also quite rare to have that many).  This does not necessarily mean that we should cut off all predictions at 5 tags, but certainly 10+ tags is not a sensible number of tags to predict.  So I think that is very likely that our score would increase if we reduced the number of tags in these cases.</li>
<li>I considered each post as a bag-of-words - meaning that I only cared which words appeared in the post, not their ordering.  However, I suspect that if I was able to build bigram-based association rules, I could increase the classifier performance.  For example, if a post title contains the bigram "linux server", my current classifier would look at the association rules for "linux" and "server" independently and it might be unlikely that it would predict the tag "linux-server" for either of those terms.  However, I suspect that a bigram association rule between "linux server" and "linux-server" would be much more likely.  The trade-off is that it would be much more difficult to mine bigram association rules on a single computer in terms of memory capacity.</li>
<li>Another strategy would be to abandon the association rule classification strategy altogether and consider a more classical method.  One such method would be to create a very sparse binary valued matrix from the training set where each feature is a word that could appear in a post (assign it a value of 1 if it does, 0 if is doesn't).  Then use well-known classification methods to model the relationship between the input matrix and the output tags.  However, I would imagine that this could present problems due to the size and sparsity of this matrix.</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                    </article>
 
<div class="paginator">
            <div class="navButton"> <a href="/author/alex-minnaar.html" >Prev</a></div>
    <div class="navButton">Page 2 / 4</div>
        <div class="navButton"><a href="/author/alex-minnaar3.html" >Next</a></div>
</div>
                </div>
            </aside><!-- /#featured -->
            
        
        <section id="extras" >
       
        
        </section><!-- /#extras -->
	
        <footer id="contentinfo" >
                <address id="about" class="vcard ">
                Proudly powered by <a href="http://getpelican.com/" target="_blank">Pelican</a>, which takes
                great advantage of <a href="http://python.org" target="_blank">Python</a>. &copy; <a class="url fn" href="http://launchyard.com">LaunchYard</a>
		
                </address><!-- /#about -->
		

                
        </footer><!-- /#contentinfo -->

<script type="text/javascript">
    var disqus_shortname = 'alexminnaar';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
</body>
</html>