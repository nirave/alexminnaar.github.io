<!DOCTYPE html>
<html lang="en">
<head>
        <title>Deep Learning Basics: Neural Networks, Backpropagation and Stochastic Gradient Descent</title>
        <meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <link rel="shortcut icon" href="http://launchyard.com/images/favicon.png"/>
        <link rel="stylesheet" href="/theme/css/main.css" type="text/css" />

        <!--[if IE]>
                <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

        <!--[if lte IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="/css/ie.css"/>
                <script src="/js/IE8.js" type="text/javascript"></script><![endif]-->

        <!--[if lt IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="/css/ie6.css"/><![endif]-->
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.8/jquery.min.js" type="text/javascript"></script>


</head>

<body id="index" class="home">
	
  <!--      <header id="banner" class="body">
                <h1><a href="/"><img src="http://www.launchyard.com/images/logo.png" /></a></h1>
        </header> -->
<!-- /#banner -->
	      <div class="LaunchyardDetail"><p><a href="/"></a>
<a class="title" href="http://alexminnaar.github.io/">Alex Minnaar</a>
<br/>
Machine Learning at University College London. Natural language processing at VerticalScope.
<br/>
<br/>
<a href="mailto:minnaaralex@gmail.com">Email</a><br />
<a href="https://github.com/alexminnaar">Github</a><br/>
<a href="https://ca.linkedin.com/pub/alex-minnaar/56/a23/853">LinkedIn</a><br/>
</p>


<div id="recent_posts">
			<h3>Categories</h3>
			<div align="left">
			<ul>
				<li><a href="/tag/nlp.html">NLP & Topic Models</a></li>
				<li><a href="/tag/supervised-learning.html">Supervised Learning</a></li>
				<li><a href="/tag/bayesian-inference.html">Bayesian Inference</a></li>
				<li><a href="/tag/kaggle-competitions.html">Kaggle Competitions</a></li>
				<li><a href="/tag/software-engineering.html">Software Engineering</a></li>
			</ul>
			</div>
              <h3>Recent Posts</h3>
                <a href="building-a-shoutbox-app-with-cassandra-and-nodejs.html">Building a Shoutbox App with Cassandra and Node.js</a><br /><br />
                <a href="/building-a-distributed-binary-search-tree-with-akka.html">Building a Distributed Binary Search Tree with Akka</a><br /><br />
                <a href="/introduction-to-the-multithreading-problem-and-the-akka-actor-solution.html">Introduction to the Multithreading Problem and the Akka Actor Solution  </a><br /><br />
                <a href="/scalaner-a-scala-wrapper-for-the-stanford-ner-tool-with-some-added-features.html">ScalaNER: A Scala Wrapper for the Stanford NER Tool with Some Added Features  </a><br /><br />
                <a href="/online-latent-dirichlet-allocation-the-best-option-for-topic-modeling-with-large-data-sets.html">Online Latent Dirichlet Allocation - The Best Option for Topic Modeling with Large Data Sets  </a><br /><br />
                <a href="/latent-dirichlet-allocation-in-scala-part-ii-the-code.html">Latent Dirichlet Allocation in Scala Part II - The Code </a><br /><br />
          </div>

</div>

<section id="content" >
    <div class="body">
      <article>
        <header>
          <h1 class="entry-title">
            <a href="/deep-learning-basics-neural-networks-backpropagation-and-stochastic-gradient-descent.html" rel="bookmark"
               title="Permalink to Deep Learning Basics: Neural Networks, Backpropagation and Stochastic Gradient Descent">Deep Learning Basics: Neural Networks, Backpropagation and Stochastic Gradient Descent</a></h1>

        </header>

        <div class="entry-content">
<div class="post-info">
	<ul>
        <li class="vcard author">
                 by&nbsp;<a class="url fn" href="/author/alex-minnaar.html">Alex Minnaar</a>
        </li>
        <li class="published" title="2015-10-14T00:00:00+02:00">
          on&nbsp;Wed 14 October 2015
        </li>

	</ul>
<p>Category: <a href="/tag/supervised-learning.html">   Supervised Learning</a></p>
</div><!-- /.post-info -->          <p>In the last couple of years <em>Deep Learning</em> has received a great deal of press.  This press is not without warrant - <em>Deep Learning</em> has produced stat-of-the-art results in many computer vision and speech processing tasks.  However, I believe that the press has given people the impression that <em>Deep Learning</em> is some kind of imprenetrable, esoteric field that can only be understood by academics.  In this blog post I want to try to erase that impression and provide a practical overview of some of <em>Deep Learning's</em> basic concepts.</p>
<p>At its core, <em>Deep Learning</em> is a class of of neural network models.  That is, a model with an input layer, an output layer, and an arbitrary number of hidden layers. These layers are made up of neurons or neural units.  They are called neurons because they share some similarities with the behaviour of the neurons present in the human brain (though this comparison has drawn a lot of criticism from neuroscientists).  For our purposes, we can think of a neuron as a nonlinear function of the weighted sum of its inputs.  Since the neuron is really the most basic part of any <em>Deep Learning</em> model it is a good place to start.</p>
<h1>The Single Neuron Model</h1>
<p>A neuron is a function that maps an input vector <span class="math">\(\{x_1,...,x_K\}\)</span> to a scalar output <span class="math">\(y\)</span> via a weight vector <span class="math">\(\{w_1,...,w_K\}\)</span> and a nonlinear function <span class="math">\(f\)</span>.</p>
<p><img alt="general graphical model" src="images/neuron.png" title="=100x20" /> </p>
<p>The function <span class="math">\(f\)</span> takes a weighted sum of the inputs and returns <span class="math">\(y\)</span>.</p>
<p>
<div class="math">$$y=f(\sum_{i=0}^Kw_ix_i)=f(\mathbf{w^Tx})$$</div>
</p>
<p>Often an additional element is added to the input vector that is always equal to <span class="math">\(1\)</span> with a corresponding additional weight element which acts as a bias.  The function <span class="math">\(f\)</span> is called the link function which provides the nonlinearity between the input and output.  A common choice for this link function is the <strong>logistic function</strong> which is defined as</p>
<p>
<div class="math">$$f(u)=\frac{1}{1+e^{u}}$$</div>
</p>
<p>With the appropriate substitutions the final formula for the single neuron model becomes</p>
<p>
<div class="math">$$y=\frac{1}{1+e^{\mathbf{w^Tx}}}$$</div>
</p>
<p>If you plot the logistic function,</p>
<p><img alt="general graphical model" src="images/logistic.png" /> </p>
<p>you can see that it is smooth and differentiable and bound between <span class="math">\(0\)</span> and <span class="math">\(1\)</span>.  We shall see that these are two important properties.  The derivative of the logistic function is simply</p>
<p>
<div class="math">$$\frac{d f(u)}{d u}=f(u)(1-f(u))=f(u)f(-u)$$</div>
</p>
<p>This derivative will be used when we learn the weight vector <span class="math">\(\bf{w}\)</span> via <strong>stochastic gradient descent</strong>.</p>
<p>Like any optimization problem, our goal is to minimize an objective function.  Traditionally, the objective function measures the difference between the actual output <span class="math">\(t\)</span> and the predicted output <span class="math">\(f(\mathbf{w^Tx})\)</span>. In this case we will be using the squared loss function</p>
<p>
<div class="math">$$E=\frac{1}{2}(t - y)^2=\frac{1}{2}(t-f(\mathbf{w^Tx}))^2$$</div>
</p>
<p>We want to find the weights <span class="math">\(\mathbf{w}\)</span> such that the above objective is minimized.  We do this with stochastic gradient descent (SGD).  In SGD we iteratively update our weight parameters in the direction of the gradient of the loss function until we have reached a minimum.  Unlike traditional gradient descent, we do not use the entire dataset to compute the gradient at each iteration.  Instead, at each iteration we randomly select a single data point from our dataset and move in the direction of the gradient with respect to that data point.  Obviously this is only an approximation of the true gradient but it can be proven that we will eventually reach the minimum by following this <em>noisey</em> gradient.  There are several advantages to using stochastic gradient descent over traditional gradient descent.</p>
<ol>
<li>Gradient descent requires loading the entire dataset into main memory.  If your dataset is large this can be problematic.  Stochastic gradient descent only requires one data point at a time (or sometimes a minibatch of data points) which is much less memory intensive.</li>
<li>Most datasets have redundancy in them.  Traditional gradient descent requires one full pass over the data until an update is made.  Due to redundancy, a meaningful update can often be made without iterating over the entire dataset as with stochastic gradient descent.</li>
<li>As a consequence of the previous point, stochastic gradient descent can often converge faster than traditional gradient descent.  It is also guaranteed to find the global minimum if the loss function is convex.</li>
</ol>
<p>Our objective function <span class="math">\(E\)</span> is already defined in terms of a single data point so let's procede to compute its gradient with respect to an aribtrary element of our weight vector <span class="math">\(w_i\)</span>.</p>
<p>
<div class="math">$$\begin{align}
\frac{\partial E}{\partial w_i} &amp;= \frac{\partial E}{\partial y} \cdot \frac{\partial y}{\partial u} \cdot\frac{\partial u}{\partial w_i} \\
&amp;= (y-t) \cdot y(1-y) \cdot x_i
\end{align}$$</div>
</p>
<p>Now we are able to obtain the stochastic gradient descent update equation (in vector notation)</p>
<p>
<div class="math">$$\mathbf{w}^{new}=\mathbf{w}^{old}- \eta \cdot (y-t) \cdot y(1-y) \cdot \mathbf{x}$$</div>
</p>
<p>Where <span class="math">\(\eta&gt;0\)</span> is the step size.  As stated previously, <span class="math">\((\mathbf{x},y)\)</span> data points are sequentially fed into this update equation until the weights <span class="math">\(\mathbf{w}\)</span> converge to their optimal value.  This is how we use stochastic gradient descent to learn the weights for the single neuron model.</p>
<p>What we just did is also known as <strong>logistic regression</strong> and if we had replaced our logistic function with a unit step function we would have made what is known as a <strong>perceptron</strong>!  Now let's extend this relatively simple model to something a bit more complex...</p>
<h1>The Neural Network</h1>
<p>A neural network consists of an input layer, output layer, and hidden layer. Our input layer consists of the input vector <span class="math">\(\mathbf{x}=\{x_1,...,x_K\}\)</span>.  The hidden layer  consists of a vector of <span class="math">\(N\)</span> neurons <span class="math">\(\mathbf{h}=\{h_1,...,h_N\}\)</span>.  Finally there is an output layer with one neuron for every element of the output vector <span class="math">\(\mathbf{y}=\{y_1,...,y_M\}\)</span>.  Every element in the input layer is connected to every neuron in the hidden layer with <span class="math">\(w_{ki}\)</span> indicating the weight associated with the connection between the <span class="math">\(k^{th}\)</span> input element and the <span class="math">\(i^{th}\)</span> hidden neuron.  The same connection structure is present between the hidden and output layers with  <span class="math">\(w'_{ij}\)</span> indicating the weight associated with the connection between the <span class="math">\(i^{th}\)</span> hidden neuron and the <span class="math">\(j^{th}\)</span> output neuron.  This network structure is better illustrated in the below diagram.</p>
<p><img alt="general graphical model" src="images/neural_network.png" title="=250x" /> </p>
<p>It is helpful to think of the weight <span class="math">\(w_{ki}\)</span> as the the <span class="math">\((k,i)^{th}\)</span> entry in a <span class="math">\(K \times N\)</span> weight matrix <span class="math">\(\mathbf{W}\)</span> and similarly weight <span class="math">\(w'_{ij}\)</span> as the <span class="math">\((i,j)^{th}\)</span> entry in a <span class="math">\(N \times M\)</span> weight matrix <span class="math">\(\mathbf{W'}\)</span>.  The output of each neuron in the hidden and output layer is computed in the exact same way as before.  It is simply the logistic function applied to the weighted sum of the neuron's inputs.  For example, the output of an arbitrary neuron in the hidden layer <span class="math">\(h_i\)</span> is</p>
<p>
<div class="math">$$h_i=f(u_i)=f(\sum^K_{k=1}w_{ki}x_k)$$</div>
</p>
<p>and similarly for the output of an arbitrary output neuron <span class="math">\(y_j\)</span> is</p>
<p>
<div class="math">$$y_j=f(u'_j)=f(\sum^N_{i=1}w'_{ij}h_i)$$</div>
</p>
<p>The objective function is also the same as before except now it is summed over all elements in the output layer.</p>
<p>
<div class="math">$$E=\frac{1}{2}\sum^M_{j=1}(y_j-t_j)^2$$</div>
</p>
<p>Unlike before, we need to construct update equations for <em>both</em> sets of weights - the input-to-hidden layer weights <span class="math">\(w_{ki}\)</span> and the hidden-to-output weights <span class="math">\(w'_{ij}\)</span>.  In order to do this we need to compute the gradient of our objective function <span class="math">\(E\)</span> with respect to <span class="math">\(w_{ki}\)</span> as well as the gradient with respect to <span class="math">\(w'_{ij}\)</span>.  We must start with the gradient with respect to <span class="math">\(w'_{ij}\)</span> (the hidden-to-output weights) and we shall see why later.  In order to compute <span class="math">\(\frac{\partial E}{\partial{w'_{ij}}}\)</span> we must recall our high-school calculus, specifically the chain rule.  From the chain rule, we must first take the derivative of <span class="math">\(E\)</span> with respect to <span class="math">\(y'_j\)</span>.  Then we must take the derivative of <span class="math">\(y_j\)</span> (i.e. the logistic function) with respect to <span class="math">\(w'_{ij}\)</span> which needs yet another application of the chain rule.  We first take the derivative of the logistic function with respect to its input <span class="math">\(u'_j\)</span>, then finally we can take the derivative of this input with respect to <span class="math">\(w'_{ij}\)</span> and we arrive at our desired value.  This process is clearly defined below.</p>
<p>From the chain rule,</p>
<p>
<div class="math">$$\frac{\partial E}{\partial w'_{ij}}=\frac{\partial E}{\partial y_j} \cdot \frac{\partial y_j}{\partial u'_j} \cdot \frac{\partial u'_j}{\partial w'_{ij}}$$</div>
</p>
<p>The derivative of <span class="math">\(E\)</span> with respect to <span class="math">\(y_j\)</span> is simply,</p>
<p>
<div class="math">$$\frac{\partial E}{\partial y_j}=y_j-t_j$$</div>
</p>
<p>From the last section we saw that the derivative of the logistic function <span class="math">\(f\)</span> with respect to its input <span class="math">\(u\)</span> is <span class="math">\(f(u)(1-f(u))\)</span>.  If we apply this we get,</p>
<p>
<div class="math">$$\frac{\partial y_j}{\partial u'_j}=y_j(1-y_j)$$</div>
</p>
<p>where <span class="math">\(y_j=f(u'_j)\)</span>.  Next we compute the derivative of <span class="math">\(u'_j=\sum^N_{i=1}w'_{ij}h_i\)</span> with respect to a particular <span class="math">\(w'_{ij}\)</span> which is simply <span class="math">\(h_i\)</span>.  So, after making the appropriate subsitutions, we get</p>
<p>
<div class="math">$$\frac{\partial E}{\partial w'_{ij}}=(y_j-t_j) \cdot y_j(1-y_j) \cdot h_i$$</div>
</p>
<p>With this gradient we can construct the update equation for <span class="math">\(w'_{ij}\)</span></p>
<p>
<div class="math">$$w'^{new}_{ij}=w'^{old}_{ij} - \eta \cdot (y_j-t_j) \cdot y_j(1-y_j) \cdot h_i$$</div>
</p>
<p>Now let's turn our attention to the gradient of the objective function with respect to the input-to-hidden weights <span class="math">\(w_{ki}\)</span>.  As we shall see, this gradient has already been partially computed when we computed the previous gradient.</p>
<p>Using the chain rule, the full gradient is</p>
<p>
<div class="math">$$\frac{\partial E}{\partial w_{ki}}=\sum^M_{j=1}(\frac{\partial E}{\partial y_j}\cdot \frac{\partial y_j}{\partial u'_j} \cdot \frac{\partial u'_j}{\partial h_i} )\cdot \frac{\partial h_i}{\partial u_i} \cdot \frac{\partial u_i}{\partial w_{ki}}$$</div>
</p>
<p>The sum is due to the fact that the hidden unit that <span class="math">\(w_{ki}\)</span> connects to is itself connected to every output unit, thus each of these gradients need to be taken into account as well.  We have already computed both <span class="math">\(\frac{\partial E}{\partial y_j}\)</span> and <span class="math">\(\frac{\partial y_j}{\partial u'_j}\)</span> which means that</p>
<p>
<div class="math">$$\frac{\partial E}{\partial y_j}\cdot \frac{\partial y_j}{\partial u'_j} = (y_j-t_j) \cdot y_j(1-y_j)$$</div>
</p>
<p>Now we need to compute the remaining derivatives <span class="math">\(\frac{\partial u'_j}{\partial h_i}\)</span>, <span class="math">\(\frac{\partial h_i}{\partial u_i}\)</span>, and <span class="math">\(\frac{\partial u_i}{\partial w_{ki}}\)</span>.  So let's do just that.</p>
<p>
<div class="math">$$\frac{\partial u'_j}{\partial h_i}=\frac{\partial \sum^N_{i=1}w'_{ij}h_i}{\partial h_i}=w'_{ij}$$</div>
</p>
<p>and, again using the derivative of the logistic function</p>
<p>
<div class="math">$$\frac{\partial h_i}{\partial u_i}=h_i(1-h_i)$$</div>
</p>
<p>and finally</p>
<p>
<div class="math">$$\frac{\partial u_i}{\partial w_{ki}}=\frac{\partial \sum^K_{k=1}w_{ki}x_k}{\partial w_{ki}}=x_k$$</div>
</p>
<p>After making the appropriate substitutions we arrive at the gradient</p>
<p>
<div class="math">$$\frac{\partial E}{\partial w_{ki}}=\sum^M_{j=1}[(y_j-t_j) \cdot y_j(1-y_j) \cdot w'_{ij}] \cdot h_i(1-h_i) \cdot x_k$$</div>
</p>
<p>And the update equation becomes</p>
<p>
<div class="math">$$w^{new}_{ki}=w^{old}_{ki} - \eta \cdot \sum^M_{j=1}[(y_j-t_j) \cdot y_j(1-y_j) \cdot w'_{ij}] \cdot h_i(1-h_i) \cdot x_k$$</div>
</p>
<p>This process is known as <strong>backpropagation</strong> because we begin with the final output error <span class="math">\(y_j-t_j\)</span> for the output neuron <span class="math">\(j\)</span> and this error gets propagated backwards throughout the network in order to update the weights.</p>
<h1>Wrapping Everything Up</h1>
<p>In this blog post we started with the simple single neuron model and we learned the model weights by computing the gradient of the objective function and using it in the stochastic gradient descent update equation.  Then we moved on to the slightly more complicated neural network model.  In this case we computed the required gradients using a procedure known as backpropagation and we again used these gradients in the SGD update equations.  True <em>Deep Learning</em> models either contain many more hidden layers or neurons in different configurations but they still adhere to the basic principles described here.  Hopefully this post has made <em>Deep Learning</em> seem like a more understandable and less daunting field of machine learning.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
        </div><!-- /.entry-content -->
        <div class="comments">

          <div id="disqus_thread"></div>
          <script type="text/javascript">
            var disqus_identifier = "deep-learning-basics-neural-networks-backpropagation-and-stochastic-gradient-descent.html";
            (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = 'http://alexminnaar.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
            })();
          </script>
        </div>

      </article>
    </div>
</section>
        <section id="extras" >
       
        
        </section><!-- /#extras -->
	
        <footer id="contentinfo" >
                <address id="about" class="vcard ">
                Proudly powered by <a href="http://getpelican.com/" target="_blank">Pelican</a>, which takes
                great advantage of <a href="http://python.org" target="_blank">Python</a>. &copy; <a class="url fn" href="http://launchyard.com">LaunchYard</a>
		
                </address><!-- /#about -->
		

                
        </footer><!-- /#contentinfo -->

<script type="text/javascript">
    var disqus_shortname = 'alexminnaar';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
</body>
</html>