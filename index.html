<!DOCTYPE html>
<html lang="en">
<head>
        <title>Alex Minnaar's Blog - Machine Learning, Data Science and Software Engineering</title>
        <meta charset="utf-8" />
	<meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <link rel="shortcut icon" href="http://launchyard.com/images/favicon.png"/>
        <link rel="stylesheet" href="/theme/css/main.css" type="text/css" />

        <!--[if IE]>
                <script src="http://html5shiv.googlecode.com/svn/trunk/html5.js"></script><![endif]-->

        <!--[if lte IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="/css/ie.css"/>
                <script src="/js/IE8.js" type="text/javascript"></script><![endif]-->

        <!--[if lt IE 7]>
                <link rel="stylesheet" type="text/css" media="all" href="/css/ie6.css"/><![endif]-->
<script src="http://ajax.googleapis.com/ajax/libs/jquery/1.8/jquery.min.js" type="text/javascript"></script>


</head>

<body id="index" class="home">
	
  <!--      <header id="banner" class="body">
                <h1><a href="/"><img src="http://www.launchyard.com/images/logo.png" /></a></h1>
        </header> -->
<!-- /#banner -->
	      <div class="LaunchyardDetail"><p><a href="/"></a>
<a class="title" href="http://alexminnaar.github.io/">Alex Minnaar</a>
<br/>
Machine Learning at University College London. Natural language processing at VerticalScope.
<br/>
<br/>
<a href="mailto:minnaaralex@gmail.com">Email</a><br />
<a href="https://github.com/alexminnaar">Github</a><br/>
<a href="https://ca.linkedin.com/pub/alex-minnaar/56/a23/853">LinkedIn</a><br/>
</p>


<div id="recent_posts">
			<h3>Categories</h3>
			<div align="left">
			<ul>
				<li><a href="/tag/nlp.html">NLP & Topic Models</a></li>
				<li><a href="/tag/supervised-learning.html">Supervised Learning</a></li>
				<li><a href="/tag/bayesian-inference.html">Bayesian Inference</a></li>
				<li><a href="/tag/kaggle-competitions.html">Kaggle Competitions</a></li>
				<li><a href="/tag/software-engineering.html">Software Engineering</a></li>
			</ul>
			</div>
              <h3>Recent Posts</h3>
                <a href="word2vec-tutorial-part-i-the-skip-gram-model.html">Word2Vec Tutorial Part I: The Skip-Gram Model</a><br /><br />
                <a href="distributed-online-latent-dirichlet-allocation-with-apache-spark.html">Distributed Online Latent Dirichlet Allocation with Apache Spark</a><br /><br />
                <a href="deep-learning-basics-neural-networks-backpropagation-and-stochastic-gradient-descent.html">Deep Learning Basics: Neural Networks, Backpropagation and Stochastic Gradient Descent</a><br /><br />
                <a href="building-a-shoutbox-app-with-cassandra-and-nodejs.html">Building a Shoutbox App with Cassandra and Node.js</a><br /><br />
                <a href="/building-a-distributed-binary-search-tree-with-akka.html">Building a Distributed Binary Search Tree with Akka</a><br /><br />
                <a href="/introduction-to-the-multithreading-problem-and-the-akka-actor-solution.html">Introduction to the Multithreading Problem and the Akka Actor Solution  </a><br /><br />
                <a href="/scalaner-a-scala-wrapper-for-the-stanford-ner-tool-with-some-added-features.html">ScalaNER: A Scala Wrapper for the Stanford NER Tool with Some Added Features  </a><br /><br />
                <a href="/online-latent-dirichlet-allocation-the-best-option-for-topic-modeling-with-large-data-sets.html">Online Latent Dirichlet Allocation - The Best Option for Topic Modeling with Large Data Sets  </a><br /><br />
                <a href="/latent-dirichlet-allocation-in-scala-part-ii-the-code.html">Latent Dirichlet Allocation in Scala Part II - The Code </a><br /><br />
          </div>

</div>

        
        

    
            <aside id="featured">
                <div class="body">
                    <article>
                        <h1 class="entry-title"><a href="/word2vec-tutorial-part-ii-the-continuous-bag-of-words-model.html">Word2Vec Tutorial Part II: The Continuous Bag-of-Words Model</a></h1>
<div class="post-info">
	<ul>
        <li class="vcard author">
                 by&nbsp;<a class="url fn" href="/author/alex-minnaar.html">Alex Minnaar</a>
        </li>
        <li class="published" title="2015-05-18T00:00:00+02:00">
          on&nbsp;Mon 18 May 2015
        </li>

	</ul>
<p>Category: <a href="/tag/nlp.html">   NLP</a><a href="/tag/supervised-learning.html">   Supervised Learning</a></p>
</div><!-- /.post-info --><p>In the <a href="http://alexminnaar.com/word2vec-tutorial-part-i-the-skip-gram-model.html">previous post</a> the concept of word vectors was explained as was the derivation of the <em>skip-gram</em> model.  In this post we will explore the other <em>Word2Vec</em> model - the <em>continuous bag-of-words</em> (CBOW) model.  If you understand the <em>skip-gram</em> model then the <em>CBOW</em> model should be quite straight-forward because in many ways they are mirror images of each other.  For instance, if you look at the model diagram</p>
<p><img alt="skip-gram model" src="images/cbow.png" /> </p>
<p>it looks like the <em>skip-gram</em> model with the inputs and outputs reversed.  The input layer consists of the <em>one-hot</em> encoded input context words <span class="math">\(\{\mathbf{x_1},...,\mathbf{x_C}\}\)</span> for a word window of size <span class="math">\(C\)</span> and vocabulary of size <span class="math">\(V\)</span>.  The hidden layer is an N-dimensional vector <span class="math">\(\mathbf{h}\)</span>.  Finally, the output layer is output word <span class="math">\(\mathbf{y}\)</span> in the training example which is also <em>one-hot</em> encoded.  The <em>one-hot</em> encoded input vectors are connected to the hidden layer via a <span class="math">\(V \times N\)</span> weight matrix <span class="math">\(\mathbf{W}\)</span> and the hidden layer is connected to the output layer via a <span class="math">\(N \times V\)</span> wieght matrix <span class="math">\(\mathbf{W'}\)</span>.</p>
<h1>Forward Propagation</h1>
<p>We must first understand how the output is computed from the input (i.e. forward propagation).  The following assumes that we know the input and output weight matrices (I will explain how these are actually learned in the next section).  The first step is to evaluate the output of the hidden layer <span class="math">\(\mathbf{h}\)</span>.  This is computed by</p>
<p>
<div class="math">$$\mathbf{h}=\frac{1}{C} \mathbf{W} \cdot (\sum^C_{i=1} \mathbf{x_i})$$</div>
</p>
<p>which is the average of the input vectors weighted by the matrix <span class="math">\(\mathbf{W}\)</span>.  It is worth noting that this hidden layer output computation is one of the only differences between the <em>continuous bag-of-words</em> model and the <em>skip-gram</em> model (in terms of them being mirror images of course).  Next we compute the inputs to each node in the output layer</p>
<p>
<div class="math">$$u_j= \mathbf{v'_{w_j}}^T \cdot \mathbf{h}$$</div>
</p>
<p>where <span class="math">\(\mathbf{v'_{w_j}}\)</span> is the <span class="math">\(j^{th}\)</span> column of the output matrix <span class="math">\(\mathbf{W'}\)</span>.  And finally we compute the output of the output layer.  The output <span class="math">\(y_j\)</span> is obtained by passing the input <span class="math">\(u_j\)</span> throught the soft-max function.</p>
<p>
<div class="math">$$y_j=p(w_{y_j} | w_1,...,w_C)=\frac{\exp(u_j)}{\sum^V_{j'=1} \exp(u_j')}$$</div>
</p>
<p>Now that we know how forward propagation works we can learn the weight matrices <span class="math">\(\mathbf{W}\)</span> and <span class="math">\(\mathbf{W'}\)</span>.  </p>
<h1>Learning the Weight Matrices with Backpropagation</h1>
<p>In the process of learning the wieght matrices <span class="math">\(\mathbf{W}\)</span> and <span class="math">\(\mathbf{W'}\)</span>, we begin with randomly initialized values.  We then sequentially feed training examples into our model and observe the error which is some function of the difference between the expected output and the actual output.  We then compute the gradient of this error with respect to the elements of both weight matrices and correct them in the direction of this gradient.  This general optimization procedure is known as stochastic gradient descent (or sgd) but the method by which the gradients are derived is known as backpropagation.  </p>
<p>The first step is to define the loss function.  The objective is to maximize the conditional probability of the output word given the input context, therefore our loss function will be</p>
<p>
<div class="math">$$\begin{align}
E &amp;= -\log p(w_O | w_I) \\
&amp;= -u_{j*} - \log \sum_{j'=1}^V \exp(u_{j'})  \\
&amp;=- \mathbf{v_{w_O}}^T \cdot \mathbf{h} - \log \sum_{j'=1}^V \exp(\mathbf{v_{w_{j'}}}^T \cdot \mathbf{h}) 
\end{align}$$</div>
</p>
<p>Where <span class="math">\(j^*\)</span> is the index of the the actual output word.  The next step is to derive the update equation for the hidden-output layer weights <span class="math">\(\mathbf{W'}\)</span>, then derive the weights for the input-hidden layer weights <span class="math">\(\mathbf{W}\)</span></p>
<h2>Updating the hidden-output layer weights</h2>
<p>The first step is to compute the derivative of the loss function <span class="math">\(E\)</span> with respect to the input to the <span class="math">\(j^{th}\)</span> node in the output layer <span class="math">\(u_j\)</span>.</p>
<p>
<div class="math">$$\frac{\partial{E}}{\partial{u_j}}=y_j - t_j$$</div>
</p>
<p>where <span class="math">\(t_j=1\)</span> if <span class="math">\(j=j^*\)</span> otherwise <span class="math">\(t_j=0\)</span>.  This is simply the prediction error of node <span class="math">\(j\)</span> in the output layer.  Next we take the derivative of <span class="math">\(E\)</span> with respect to the output weight <span class="math">\(w'_{ij}\)</span> using the chain rule.</p>
<p>
<div class="math">$$\begin{align}
\frac{\partial{E}}{\partial{w'_{ij}}} &amp;= \frac{\partial{E}}{\partial{u_j}} \cdot \frac{\partial{u_j}}{\partial{w'_{ij}}} \\
&amp;= (y_j - t_j) \cdot h_i
\end{align}$$</div>
</p>
<p>Now that we have the gradient with respect to an arbitrary output weight <span class="math">\(w'_{ij}\)</span>, we can define the stochastic gradient descent equation.</p>
<p>
<div class="math">$$w_{ij}^{'(new)}=w_{ij}^{'(old)} - \eta \cdot (y_j - t_j) \cdot h_i$$</div>
</p>
<p>or
<div class="math">$$\mathbf{v'_{w_j}}^{(new)}=\mathbf{v'_{w_j}}^{(old)} - \eta \cdot (y_j - t_j) \cdot \mathbf{h}$$</div>
</p>
<p>where <span class="math">\(\eta&gt;0\)</span> is the learning rate. </p>
<h2>Updating the input-hidden layer weights</h2>
<p>Now let's try to derive a similar update equation for the input weights <span class="math">\(w_{ij}\)</span>.  The first step is to compute the derivative of <span class="math">\(E\)</span> with respect to an arbitrary hidden node <span class="math">\(h_i\)</span> (again using the chain rule).</p>
<p>
<div class="math">$$
\begin{align}
\frac{\partial{E}}{\partial{h_i}} &amp;= \sum^V_{j=1} \frac{\partial{E}}{\partial{u_j}} \cdot \frac{\partial{u_j}}{\partial{h_i}} \\
&amp;= \sum^V_{j=1} (y_j - t_j) \cdot w'_{ij}
\end{align}
$$</div>
</p>
<p>where the sum is do to the fact that the hidden layer node <span class="math">\(h_i\)</span> is connected to each node of the output layer and therefore each prediction error must be incorporated.  The next step is to compute the derivative of <span class="math">\(E\)</span> with respect to an arbitrary input weight <span class="math">\(w_{ki}\)</span>.</p>
<p>
<div class="math">$$
\begin{align}
\frac{\partial{E}}{\partial{w_{ki}}} &amp;= \frac{\partial{E}}{\partial{h_i}} \cdot \frac{\partial{h_i}}{\partial{w_{ki}}}\\
 &amp;= \sum^V_{j=1} (y_j - t_j) \cdot w'_{ij} \cdot \frac{1}{C} \cdot x_k \\
 &amp; = \frac{1}{C}(\mathbf{x} \cdot EH)
\end{align}
$$</div>
</p>
<p>Where <span class="math">\(EH\)</span> is an N-dimensional vector of elements <span class="math">\(\sum^V_{j=1} (y_j - t_j) \cdot w'_{ij}\)</span> from <span class="math">\(i=1,...,N\)</span>.  However, since the inputs <span class="math">\(\mathbf{x}\)</span> are <em>one-hot</em> encoded, only one row of the <span class="math">\(N \times V\)</span> matrix <span class="math">\(\frac{1}{C}(\mathbf{x} \cdot EH)\)</span> will be nonzero.  Thus the final stochastic gradient descent equation for the input weights is</p>
<p>
<div class="math">$$\mathbf{v'_{w_{I,c}}}^{(new)}=\mathbf{v'_{w_{I,c}}}^{(old)} - \eta \cdot \frac{1}{C} \cdot EH$$</div>
</p>
<p>where <span class="math">\(w_{I,c}\)</span> is the <span class="math">\(c^{th}\)</span> word in the input context.</p>
<h2>References</h2>
<ul>
<li><a href="http://alexminnaar.com/word2vec-tutorial-part-i-the-skip-gram-model.html">Word2Vec Tutorial Part I: The Skip-Gram Model</a></li>
<li><a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Distributed Representations of Words and Phrases and their Compositionality</a>, Mikolov et al.</li>
<li><a href="http://arxiv.org/abs/1103.0398">Natural Language Processing (almost) from Scratch</a>, Collobert et al.</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                    </article>
                </div>
            </aside><!-- /#featured -->
            
        
        

    
            <aside id="featured">
                <div class="body">
                    <article>
                        <h1 class="entry-title"><a href="/word2vec-tutorial-part-i-the-skip-gram-model.html">Word2Vec Tutorial Part I: The Skip-Gram Model</a></h1>
<div class="post-info">
	<ul>
        <li class="vcard author">
                 by&nbsp;<a class="url fn" href="/author/alex-minnaar.html">Alex Minnaar</a>
        </li>
        <li class="published" title="2015-04-12T00:00:00+02:00">
          on&nbsp;Sun 12 April 2015
        </li>

	</ul>
<p>Category: <a href="/tag/nlp.html">   NLP</a><a href="/tag/supervised-learning.html">   Supervised Learning</a></p>
</div><!-- /.post-info --><p>In many natural language processing tasks, words are often represented by their <em>tf-idf</em> scores.  While these scores give us some idea of a word's relative importance in a document, they do not give us any insight into its semantic meaning.  <em>Word2Vec</em> is the name given to a class of neural network models that, given an unlabelled training corpus, produce a vector for each word in the corpus that encodes its semantic information.  These vectors are usefull for two main reasons.</p>
<ol>
<li>We can measure the semantic similarity between two words are by calculating the cosine similarity between their corresponding word vectors.  </li>
<li>We can use these word vectors as features for various supervised NLP tasks such as document classification, named entity recognition, and sentiment analysis.  The semantic information that is contained in these vectors make them powerful features for these tasks.</li>
</ol>
<p>You may ask <em>"how do we know that these vectors effectively capture the semantic meanings of the words?"</em>.  The answer is because the vectors adhere surprisingly well to our intuition.  For instance, words that we know to be synonyms tend to have similar vectors in terms of cosine similarity and antonyms tend to have dissimilar vectors.  Even more surprisingly, word vectors tend to obey the laws of analogy.  For example, consider the analogy <em>"Woman is to queen as man is to king"</em>.  It turns out that</p>
<p>
<div class="math">$$v_{queen}-v_{woman}+v_{man} \approx v_{king}$$</div>
</p>
<p>where <span class="math">\(v_{queen}\)</span>,<span class="math">\(v_{woman}\)</span>,<span class="math">\(v_{man}\)</span>, and <span class="math">\(v_{king}\)</span> are the word vectors for <span class="math">\(queen\)</span>, <span class="math">\(woman\)</span>, <span class="math">\(man\)</span>, and <span class="math">\(king\)</span> respectively.  These observations strongly suggest that word vectors encode valuable semantic information about the words that they represent. </p>
<p>In this series of blog posts I will describe the two main <em>Word2Vec</em> models - the <em>skip-gram model</em> and the <em>continuous bag-of-words</em> model - as well as the techniques used to compute these models efficiently. In total there will be three posts</p>
<ol>
<li>The Skip-Gram Model (this post).</li>
<li><a href="www.alexminnaar.com/word2vec-tutorial-part-ii-the-continuous-bag-of-words-model.html">The Continuous Bag-of-Words Model</a>.</li>
<li>Techniques for Computational Efficiency (coming soon).</li>
</ol>
<p>Both of these models are simple neural networks with one hidden layer.  The word vectors are learned via backpropagation and stochastic gradient descent both of which I descibed in my previous <a href="http://alexminnaar.com/deep-learning-basics-neural-networks-backpropagation-and-stochastic-gradient-descent.html">Deep Learning Basics</a> blog post.</p>
<h1>The Skip-Gram Model</h1>
<p>Before we define the <em>skip-gram</em> model, it would be instructive to understand the format of the training data that it accepts.  The input of the <em>skip-gram</em> model is a single word <span class="math">\(w_I\)</span> and the output is the words in <span class="math">\(w_I\)</span>'s context <span class="math">\(\{w_{O,1},...,w_{O,C}\}\)</span> defined by a word window of size <span class="math">\(C\)</span>.  For example, consider the sentence <em>"I drove my car to the store"</em>.  A potential training instance could be the word "car" as an input and the words {"I","drove","my","to","the","store"} as outputs.  All of these words are <em>one-hot</em> encoded meaning they are vectors of length <span class="math">\(V\)</span> (the size of the vocabulary) with a value of <span class="math">\(1\)</span> at the index corresponding to the word and zeros in all other indexes.  As you can see, we are essentially <em>creating</em> training examples from plain text which means that we can have a virtually unlimited number of training examples at our disposal.</p>
<h2>Forward Propagation</h2>
<p>Now let's define the <em>skip-gram</em> nerual network model as follows.</p>
<p><img alt="skip-gram model" src="images/skip-gram.png" /> </p>
<p>In the above model <span class="math">\(\mathbf{x}\)</span> represents the <em>one-hot</em> encoded vector corresponding to the input word in the training instance and <span class="math">\(\{\mathbf{y_1},...\mathbf{y_C}\}\)</span> are the <em>one-hot</em> encoded vectors corresponding to the output words in the training instance.  The <span class="math">\(V \times N\)</span> matrix <span class="math">\(\mathbf{W}\)</span> is the weight matrix between the input layer and hidden layer whose <span class="math">\(i^{th}\)</span> row represents the weights corresponding to the <span class="math">\(i^{th}\)</span> word in the vocabulary. This weight matrix <span class="math">\(\mathbf{W}\)</span> is what we are interested in learning because it contains the vector encodings of all of the words in our vocabulary (as its rows).  Each output word vector also has an associated <span class="math">\(N \times V\)</span> output matrix <span class="math">\(\mathbf{W'}\)</span>. There is also a hidden layer consisting of <span class="math">\(N\)</span> nodes (the exact size of <span class="math">\(N\)</span> is a training parameter).</p>
<p>From my <a href="http://alexminnaar.com/deep-learning-basics-neural-networks-backpropagation-and-stochastic-gradient-descent.html">previous blog post</a>, we know that the input to a unit in the hidden layer <span class="math">\(h_i\)</span> is simply the weighted sum of its inputs.  Since the input vector <span class="math">\(\mathbf{x}\)</span> is <em>one-hot</em> encoded, the weights coming from the nonzero element will be the only ones contributing to the hidden layer.  Therefore, for the input <span class="math">\(\mathbf{x}\)</span> with <span class="math">\(x_k=1\)</span> and <span class="math">\(x_{k'}=0\)</span> for all <span class="math">\(k' \neq k\)</span> the outputs of the hidden layer will be equivalent to the <span class="math">\(k^{th}\)</span> row of <span class="math">\(\mathbf{W}\)</span>.  Or mathematically,</p>
<p>
<div class="math">$$\mathbf{h}=\mathbf{x}^T \mathbf{W}=\mathbf{W}_{(k, .)} := \mathbf{v}_{w_I}$$</div>
</p>
<p>Notice that there is no activation function used here.  This is presumably because the inputs are bounded by the <em>one-hot</em> encoding.  In the same way, the inputs to each of the <span class="math">\(C \times V\)</span> output nodes is computed by the weighted sum of its inputs.  Therefore, the input to the <span class="math">\(j^{th}\)</span> node of the <span class="math">\(c^{th}\)</span> output word is</p>
<p>
<div class="math">$$u_{c,j}=\mathbf{v'}_{w_j}^T \mathbf{h}$$</div>
</p>
<p>However we can also observe that the output layers for each output word share the same weights therefore <span class="math">\(u_{c,j}=u_j\)</span>.  We can finally compute the output of the <span class="math">\(j^{th}\)</span> node of the <span class="math">\(c^{th}\)</span> output word via the <em>softmax</em> function which produces a multinomial distribution </p>
<p>
<div class="math">$$p(w_{c,j}=w_{O,c} | w_I)=y_{c,j}=\frac{exp(u_{c,j})}{\sum_{j'=1}^V exp(u_{j'})}$$</div>
</p>
<p>In plain english, this value is the probability that the output of the <span class="math">\(j^{th}\)</span> node of the <span class="math">\(c^{th}\)</span> output word is equal to the actual value of the <span class="math">\(j^{th}\)</span> index of the <span class="math">\(c^{th}\)</span> output vector (which is <em>one-hot</em> encoded).</p>
<h2>Learning the Weights with Backpropagation and Stochastic Gradient Descent</h2>
<p>Now that we know how inputs are propograted forward through the network to produce outputs, we can derive the error gradients necessary for the backpropagation algorithm which we will use to learn both <span class="math">\(\mathbf{W}\)</span> and <span class="math">\(\mathbf{W'}\)</span>.  The first step in deriving the gradients is defining a loss function.  This loss function will be</p>
<p>
<div class="math">$$\begin{align}
E &amp;= -\log p(w_{O,1}, w_{O,2}, ..., w_{O,C} | w_I) \\
&amp;= -\log \prod^C_{c=1} \frac{exp(u_{c,j^*_c})}{\sum^V_{j'=1} exp(u_j')} \\
&amp;= -\sum^C_{c=1}u_{j^*_c} +C \cdot \log \sum^V_{j'=1} exp(u_{j'})
\end{align}$$</div>
</p>
<p>which is simply the probability of the output words (the words in the input word's context) given the input word.  Here, <span class="math">\(j^*_c\)</span> is the index of the <span class="math">\(c^{th}\)</span> output word.  If we take the derivative with respect to <span class="math">\(u_{c,j}\)</span> we get</p>
<p>
<div class="math">$$\frac{\partial{E}}{\partial{u_{c,j}}}=y_{c,j}-t_{c,j}$$</div>
</p>
<p>where <span class="math">\(t_{c,j}=1\)</span> if the <span class="math">\(j^{th}\)</span> node of the <span class="math">\(c^{th}\)</span> true output word is equal to <span class="math">\(1\)</span> (from its <em>one-hot</em> encoding), otherwise <span class="math">\(t_{c,j}=1\)</span>.  This is the prediction error for node <span class="math">\(c,j\)</span> (or the <span class="math">\(j^{th}\)</span> node of the <span class="math">\(c^{th}\)</span> output word).</p>
<p>Now that we have the error derivative with respect to inputs of the final layer, we can derive the derivative with respect to the output matrix <span class="math">\(\mathbf{W'}\)</span>.  Here we use the chain rule </p>
<p>
<div class="math">$$\begin{align}
\frac{\partial E}{\partial w'_{ij}} &amp;=\sum^C_{c=1} \frac{\partial E}{\partial u_{c,j}} \cdot \frac{\partial u_{c,j}}{\partial w'_{ij}} \\
&amp;=\sum^C_{c=1} (y_{c,j}-t_{c,j}) \cdot h_i
\end{align}$$</div>
</p>
<p>Therefore the gradient descent update equation for the output matrix <span class="math">\(\mathbf{W'}\)</span> is</p>
<p>
<div class="math">$$w'^{(new)}_{ij} =w'^{(old)}_{ij}- \eta \cdot \sum^C_{c=1} (y_{c,j}-t_{c,j}) \cdot h_i$$</div>
</p>
<p>Now we can derive the update equation for the input-hidden layer weights in <span class="math">\(\mathbf{W}\)</span>.  Let's start by computing the error derivative with respect to the hidden layer.</p>
<p>
<div class="math">$$\begin{align}
\frac{\partial{E}}{\partial{h_i}} &amp;=\sum^V_{j=1} \frac{\partial{E}}{\partial{u_j}} \cdot \frac{\partial{u_j}}{\partial{h_i}}  \\
&amp;=\sum^V_{j=1} \sum^C_{c=1} (y_{c,j}-t_{c,j})  \cdot w'_{ij}
\end{align}$$</div>
</p>
<p>Now we are able to compute the derivative with respect to <span class="math">\(\mathbf{W}\)</span></p>
<p>
<div class="math">$$\begin{align}
\frac{\partial{E}}{\partial{w_{ki}}} &amp;=\frac{\partial{E}}{\partial{h_i}} \cdot \frac{\partial{h_i}}{\partial{w_{ki}}} \\
&amp;= \sum^V_{j=1} \sum^C_{c=1} (y_{c,j}-t_{c,j})  \cdot w'_{ij} \cdot x_k
\end{align}$$</div>
</p>
<p>and finally we arrive at our gradient descent equation for our input weights</p>
<p>
<div class="math">$$w^{(new)}_{ij} =w^{(old)}_{ij}- \eta \cdot \sum^V_{j=1} \sum^C_{c=1} (y_{c,j}-t_{c,j})  \cdot w'_{ij} \cdot x_j$$</div>
</p>
<p>As you can see, each gradient descent update requires a sum over the entire vocabulary <span class="math">\(V\)</span> which is computationally expensive.  In a later post I will descibe two techniques - hierarchical softmax and negative sampling - which make this computation more efficient.</p>
<h2>References</h2>
<ul>
<li>
<p><a href="www.alexminnaar.com/word2vec-tutorial-part-ii-the-continuous-bag-of-words-model.html">Word2Vec Tutorial Part II: The Continuous Bag-of-Words Model</a></p>
</li>
<li>
<p><a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Distributed Representations of Words and Phrases and their Compositionality</a>, Mikolov et al.</p>
</li>
<li><a href="http://arxiv.org/abs/1103.0398">Natural Language Processing (almost) from Scratch</a>, Collobert et al.</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                    </article>
                </div>
            </aside><!-- /#featured -->
            
        
        

    
            <aside id="featured">
                <div class="body">
                    <article>
                        <h1 class="entry-title"><a href="/distributed-online-latent-dirichlet-allocation-with-apache-spark.html">Distributed Online Latent Dirichlet Allocation with Apache Spark</a></h1>
<div class="post-info">
	<ul>
        <li class="vcard author">
                 by&nbsp;<a class="url fn" href="/author/alex-minnaar.html">Alex Minnaar</a>
        </li>
        <li class="published" title="2015-03-20T00:00:00+01:00">
          on&nbsp;Fri 20 March 2015
        </li>

	</ul>
<p>Category: <a href="/tag/nlp.html">   NLP</a><a href="/tag/software-engineering.html">   Software Engineering</a></p>
</div><!-- /.post-info --><p>In the past, I have studied the online LDA algorithm from <a href="https://www.google.ca/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CDUQFjAA&amp;url=https%3A%2F%2Fwww.cs.princeton.edu%2F~blei%2Fpapers%2FHoffmanBleiBach2010b.pdf&amp;ei=pib7VP_ZIsewggS3pYAY&amp;usg=AFQjCNHLmU8Gk_P4usBj2QcGcaolw87w4w&amp;sig2=TVmpNtdTBqqPScHDqBGYcg&amp;bvm=bv.87611401,d.eXY">Hoffman et al.</a> in some depth resulting in <a href="http://alexminnaar.com/online-latent-dirichlet-allocation-the-best-option-for-topic-modeling-with-large-data-sets.html">this blog post</a> and corresponding <a href="https://github.com/alexminnaar/ScalaTopicModels">Scala code</a>.  Before we go further I will provide a general description of how the algorithm works.  In online LDA, minibatches of documents are sequentially processed to update a global topic/word matrix which defines the topics that have been learned.  The processing consists of two steps:</p>
<ul>
<li><strong>The E-Step:</strong>  Given the minibatch of documents, updates to the corresponding rows of the topic/word matrix are computed.</li>
<li><strong>The M-Step:</strong>  The updates from the E-Step are blended with the current topic/word matrix resulting in the updated topic/word matrix.</li>
</ul>
<p>This post details how I developed a distributed version of online LDA using the Apache Spark engine.  At first glance, it might seem redundant to build a distributed version of online LDA since one of the main advantages of the algorithm is its scalability (documents are streamed sequentially so they do not need to be kept in main memory).  While it is true that the original algorithm is scalable in terms of memory, using a distributed computing framework (such as Spark) can speed the algorithm up immensely.  Today, companies are demanding real-time or near real-time data processing which makes a Spark solution advantageous.  Furthermore, there are some cases - for example if you choose to learn a sufficiently large number of topics with a sufficiently large vocabulary size - where the original algorithm can in fact run into memory issues.  Hopefully I have shown that a distributed version of online LDA would be beneficial.</p>
<p>This blog post will be divided into a few sections.  First I will give a very broad overview of how Spark works.  Then I will outline how some processes of the original online LDA algorithm can be parallelized (again, for a more detailed outline of how the original online LDA algorithm works I encourage you to read <a href="http://alexminnaar.com/online-latent-dirichlet-allocation-the-best-option-for-topic-modeling-with-large-data-sets.html">my prevous blog post</a>).  Finally I will provide the code for the Spark implemenation as well as a demo.</p>
<h1>The Basics of Spark</h1>
<p>The beauty of Spark is in its simplicity.  It has abstracted away all of the complicated aspects of MapReduce programming and it leaves us with a simple interface with which to build our distributed processing jobs.  Here I will provide you with a very brief and incomplete overview of how Spark works (refer to the <a href="https://spark.apache.org/docs/latest/">Spark documentation</a> for more details). </p>
<p>In Spark, all distributed computations are done on RDDs (Resilient Distributed Datasets).  RDDs are linear data structures that are distributed across the nodes in your cluster.  The most common operation that you can perform on an RDD is a <code>map</code>.  The <code>map</code> function takes a function as an input an applies this function to each element of the RDD in parallel and returns another RDD containing the result.  Another common operation that is performed on RDDs is the <code>reduce</code> function.  <code>reduce</code> also takes a function as an input (that must be commutative and associative) and aggregates the elements of the RDD in based on that function.  There are many other very useful RDD operations (eg. <em>reduceByKey</em>, <em>join</em>, <em>zip</em> etc.) that are beyond the scope of this blog post.  To learn more read the <a href="https://spark.apache.org/docs/latest/programming-guide.html">Spark programming guide</a>.</p>
<p>It is also important to note that, aside from the user-friendly interface, Spark's main advantage is its speed.  Unlike previous MapReduce frameworks, Spark utilizes the RAM of the machines in the clusters.  When data is kept in memory, disk serialization/deserialzation is greatly reduced.  This, in turn, allows us to distribute iterative algorithms much faster (we no longer have to wait for the data to write to disk after each iteration).  Online LDA is a distributed algorithm which makes it a great fit for Spark.</p>
<h1>Parallelizing the Online LDA Algorithm</h1>
<p>Now let's use Spark to parallelize the online LDA algorith.  There are four aspects of the algorithm that could benefit from parallelzation - the global topic/word matrix, the minibatch, the E-Step, and the M-Step.  Let's start with the topic/word matrix.</p>
<h2>A Distributed Topic/Word Matrix</h2>
<p>The global topic/word matrix is the data structure that contains all of the topics that have been learned so far.  The rows of this matrix correspond to the words in the vocabulary and the columns correspond to the topics.  At first, it might seem like this data structure is small enough to exist locally in the driver.  After all, there are only so many words in the English language which bounds the number of rows and the number of topics should be less than the number of words in the vocabulary.  However, I have experienced out-of-memory errors when this matrix is too large.  This is likely not because the matrix itself is too large to fit in the driver, but because size of this matrix combined with everything else that is associated with the Spark application is too large.  In addition, distributing the topic/word matrix allows for more parallelism in other parts of the online LDA algorithm which results in an overall speedup.  Spark's MLlib library provides a distributed matrix data structure called <a href="https://spark.apache.org/docs/1.0.0/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.IndexedRowMatrix">indexedRowMatrix</a> which is an RDD of tuples containing the rows of the matrix and their corresponding indexes.  We will use this data structure to store our topic/word matrix.</p>
<h2>A Distributed Minibatch</h2>
<p>The minibatch is a collection of documents with which we update the LDA model.  In the original algorithm, the documents in the minibatch must be converted to <em>bag-of-words</em> format (i.e. a set of (wordId, frequency) tuples).  If the minibatch is made into an RDD (where each element of the RDD consists of a document in the minibatch), the <em>bag-of-words</em> conversion can be performed in parallel via a <em>map</em> and a <em>toBagOfWords</em> function as shown below.</p>
<div class="highlight"><pre><span class="k">val</span> <span class="n">bagOfWordsRDD</span><span class="k">:</span><span class="kt">RDD</span><span class="o">[</span><span class="kt">List</span><span class="o">[(</span><span class="kt">Int</span>,<span class="kt">Int</span><span class="o">)]]</span> <span class="k">=</span> <span class="n">minibatchRDD</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="n">toBagOfWords</span><span class="o">(</span><span class="n">x</span><span class="o">))</span>
</pre></div>


<p>Clearly, this is more efficient that converting each document to its <em>bag-of-words</em> format sequentially.  Furthermore, well shall see later that the minibatch must be an RDD in order to parallelize future computations that involve the minibatch.</p>
<h2>A Distributed E-Step</h2>
<p>The following is the original non-distributed implementation of the E-step for online LDA.  It is not important to understand exactly what the code is doing - the important part is observing the presence of the inner and outer loops (indicated in the comments).</p>
<div class="highlight"><pre> <span class="k">def</span> <span class="n">eStep</span><span class="o">(</span><span class="n">miniBatch</span><span class="k">:</span> <span class="kt">List</span><span class="o">[</span><span class="kt">List</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Int</span><span class="o">)]],</span> <span class="n">expELogBeta</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span><span class="k">:</span> <span class="o">(</span><span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="nc">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span> <span class="k">=</span> <span class="o">{</span>

    <span class="k">val</span> <span class="n">gamma</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">](</span><span class="n">miniBatch</span><span class="o">.</span><span class="n">length</span><span class="o">,</span> <span class="n">numTopics</span><span class="o">,</span> <span class="nc">Gamma</span><span class="o">(</span><span class="mf">100.0</span><span class="o">,</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="mf">100.0</span><span class="o">).</span><span class="n">sample</span><span class="o">(</span><span class="n">numTopics</span> <span class="o">*</span> <span class="n">miniBatch</span><span class="o">.</span><span class="n">length</span><span class="o">).</span><span class="n">toArray</span><span class="o">)</span>

    <span class="k">val</span> <span class="n">eLogTheta</span> <span class="k">=</span> <span class="n">dirichletExpectation</span><span class="o">(</span><span class="n">gamma</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">expELogTheta</span> <span class="k">=</span> <span class="n">exp</span><span class="o">(</span><span class="n">eLogTheta</span><span class="o">)</span>
    <span class="k">var</span> <span class="n">sstats</span> <span class="k">=</span> <span class="nc">DenseMatrix</span><span class="o">.</span><span class="n">zeros</span><span class="o">[</span><span class="kt">Double</span><span class="o">](</span><span class="n">numTopics</span><span class="o">,</span> <span class="n">numTerms</span><span class="o">)</span>

    <span class="k">for</span> <span class="o">((</span><span class="n">doc</span><span class="o">,</span> <span class="n">idx</span><span class="o">)</span> <span class="k">&lt;-</span> <span class="n">miniBatch</span><span class="o">.</span><span class="n">zipWithIndex</span><span class="o">)</span> <span class="o">{</span>  <span class="c1">// &lt;---------- Outer Loop</span>

      <span class="k">val</span> <span class="n">idCtList</span> <span class="k">=</span> <span class="n">doc</span><span class="o">.</span><span class="n">toList</span><span class="o">.</span><span class="n">sortBy</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">_1</span><span class="o">)</span>
      <span class="k">val</span> <span class="n">wordIDs</span> <span class="k">=</span> <span class="n">idCtList</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">_1</span><span class="o">)</span>
      <span class="k">val</span> <span class="n">cts</span> <span class="k">=</span> <span class="n">idCtList</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="k">_</span><span class="o">.</span><span class="n">_2</span><span class="o">.</span><span class="n">toDouble</span><span class="o">)</span>

      <span class="k">val</span> <span class="n">gammaD</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="k">=</span> <span class="n">gamma</span><span class="o">(</span><span class="n">idx</span><span class="o">,</span> <span class="o">::).</span><span class="n">t</span><span class="o">.</span><span class="n">toDenseMatrix</span>
      <span class="k">val</span> <span class="n">expELogThetaD</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="k">=</span> <span class="n">expELogTheta</span><span class="o">(</span><span class="n">idx</span><span class="o">,</span> <span class="o">::).</span><span class="n">t</span><span class="o">.</span><span class="n">toDenseMatrix</span>
      <span class="k">val</span> <span class="n">expELogBetaD</span> <span class="k">=</span> <span class="n">expELogBeta</span><span class="o">(</span><span class="mi">0</span> <span class="n">until</span> <span class="n">expELogBeta</span><span class="o">.</span><span class="n">rows</span><span class="o">,</span> <span class="n">wordIDs</span><span class="o">.</span><span class="n">toIndexedSeq</span><span class="o">).</span><span class="n">toDenseMatrix</span>
      <span class="k">val</span> <span class="n">phiNorm</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="k">=</span> <span class="n">expELogThetaD</span> <span class="o">*</span> <span class="n">expELogBetaD</span> <span class="o">+</span> <span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">100</span>
      <span class="k">val</span> <span class="n">docCounts</span> <span class="k">=</span> <span class="nc">DenseMatrix</span><span class="o">(</span><span class="n">cts</span><span class="o">.</span><span class="n">toArray</span><span class="o">)</span>

      <span class="c1">//Recursive loop to infer phiNorm, gammaD and exoElogThetaD parameters</span>
      <span class="k">def</span> <span class="n">gammaUpdate</span><span class="o">(</span><span class="n">pn</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="n">expETD</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
        <span class="k">val</span> <span class="n">term1</span> <span class="k">=</span> <span class="n">expETD</span> <span class="o">:*</span> <span class="o">(</span><span class="n">docCounts</span> <span class="o">/</span> <span class="n">pn</span><span class="o">)</span> <span class="o">*</span> <span class="n">expELogBetaD</span><span class="o">.</span><span class="n">t</span>
        <span class="n">term1</span><span class="o">(::,</span> <span class="o">*)</span> <span class="o">+</span> <span class="n">alpha</span>
      <span class="o">}</span>

      <span class="k">def</span> <span class="n">thetaUpdate</span><span class="o">(</span><span class="n">gD</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
        <span class="n">exp</span><span class="o">(</span><span class="n">dirichletExpectation</span><span class="o">(</span><span class="n">gD</span><span class="o">))</span>
      <span class="o">}</span>

      <span class="k">def</span> <span class="n">phiUpdate</span><span class="o">(</span><span class="n">expETD</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
        <span class="n">expETD</span> <span class="o">*</span> <span class="n">expELogBetaD</span> <span class="o">+</span> <span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">100</span>
      <span class="o">}</span>

      <span class="k">def</span> <span class="n">eStepIterator</span><span class="o">(</span><span class="n">phiNorm</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="n">expELogThetaD</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="n">gammaD</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span><span class="k">:</span> <span class="o">(</span><span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="nc">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="nc">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span> <span class="k">=</span> <span class="o">{</span>

        <span class="k">val</span> <span class="n">lastGamma</span> <span class="k">=</span> <span class="nc">DenseMatrix</span><span class="o">.</span><span class="n">zeros</span><span class="o">[</span><span class="kt">Double</span><span class="o">](</span><span class="n">gammaD</span><span class="o">.</span><span class="n">rows</span><span class="o">,</span> <span class="n">gammaD</span><span class="o">.</span><span class="n">cols</span><span class="o">)</span>

        <span class="k">def</span> <span class="n">loop</span><span class="o">(</span><span class="n">counter</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">newGamma</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="n">newTheta</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="n">newPhi</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="n">lastGamma</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span><span class="k">:</span> <span class="o">(</span><span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="nc">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="nc">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span> <span class="k">=</span> <span class="o">{</span>
          <span class="k">if</span> <span class="o">(((</span><span class="n">mean</span><span class="o">(</span><span class="n">abs</span><span class="o">(</span><span class="n">newGamma</span> <span class="o">-</span> <span class="n">lastGamma</span><span class="o">)))</span> <span class="o">&lt;</span> <span class="n">gammaThreshold</span><span class="o">)</span> <span class="o">||</span> <span class="o">(</span><span class="n">counter</span> <span class="o">&gt;</span> <span class="n">iterations</span><span class="o">))</span> <span class="o">{</span>
            <span class="o">(</span><span class="n">newGamma</span><span class="o">,</span> <span class="n">newPhi</span><span class="o">,</span> <span class="n">newTheta</span><span class="o">)</span>
          <span class="o">}</span>
          <span class="k">else</span> <span class="o">{</span>
            <span class="k">val</span> <span class="n">term1</span> <span class="k">=</span> <span class="n">gammaUpdate</span><span class="o">(</span><span class="n">newPhi</span><span class="o">,</span> <span class="n">newTheta</span><span class="o">)</span>
            <span class="k">val</span> <span class="n">term2</span> <span class="k">=</span> <span class="n">thetaUpdate</span><span class="o">(</span><span class="n">term1</span><span class="o">)</span>
            <span class="k">val</span> <span class="n">term3</span> <span class="k">=</span> <span class="n">phiUpdate</span><span class="o">(</span><span class="n">term2</span><span class="o">)</span>
            <span class="n">loop</span><span class="o">(</span><span class="n">counter</span> <span class="o">+</span> <span class="mi">1</span><span class="o">,</span> <span class="n">term1</span><span class="o">,</span> <span class="n">term2</span><span class="o">,</span> <span class="n">term3</span><span class="o">,</span> <span class="n">newGamma</span><span class="o">)</span>
          <span class="o">}</span>
        <span class="o">}</span>
        <span class="n">loop</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="n">gammaD</span><span class="o">,</span> <span class="n">expELogThetaD</span><span class="o">,</span> <span class="n">phiNorm</span><span class="o">,</span> <span class="n">lastGamma</span><span class="o">)</span>
      <span class="o">}</span>

      <span class="c1">//execute recursive loop function</span>
      <span class="k">val</span> <span class="o">(</span><span class="n">newGammaD</span><span class="o">,</span> <span class="n">newPhiNorm</span><span class="o">,</span> <span class="n">newExpELogThetaD</span><span class="o">)</span> <span class="k">=</span> <span class="n">eStepIterator</span><span class="o">(</span><span class="n">phiNorm</span><span class="o">,</span> <span class="n">expELogThetaD</span><span class="o">,</span> <span class="n">gammaD</span><span class="o">)</span>  <span class="c1">// &lt;---------- Inner Loop</span>
      <span class="n">gamma</span><span class="o">(</span><span class="n">idx</span><span class="o">,</span> <span class="o">::)</span> <span class="o">:=</span> <span class="n">newGammaD</span><span class="o">.</span><span class="n">toDenseVector</span><span class="o">.</span><span class="n">t</span>
      <span class="k">val</span> <span class="n">sstatTerm</span> <span class="k">=</span> <span class="n">newExpELogThetaD</span><span class="o">.</span><span class="n">t</span> <span class="o">*</span> <span class="o">(</span><span class="n">docCounts</span> <span class="o">/</span> <span class="n">newPhiNorm</span><span class="o">)</span>

      <span class="k">for</span> <span class="o">((</span><span class="n">i</span><span class="o">,</span> <span class="n">ct</span><span class="o">)</span> <span class="k">&lt;-</span> <span class="n">wordIDs</span><span class="o">.</span><span class="n">zipWithIndex</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">sstats</span><span class="o">(::,</span> <span class="n">i</span><span class="o">)</span> <span class="o">:+=</span> <span class="n">sstatTerm</span><span class="o">(::,</span> <span class="n">ct</span><span class="o">)</span>
      <span class="o">}</span>
    <span class="o">}</span>

    <span class="n">sstats</span> <span class="k">=</span> <span class="n">sstats</span> <span class="o">:*</span> <span class="n">expELogBeta</span>
    <span class="o">(</span><span class="n">gamma</span><span class="o">,</span> <span class="n">sstats</span><span class="o">)</span>
  <span class="o">}</span>
</pre></div>


<p>As you can see, there is an outer loop over all documents in the minibatch, and an inner (recursive) loop for each document.  In the distributed implementation, the inner loop is performed in parallel for every document in the minibatch RDD (using a <code>map</code> function) and then the results are combined (using a <code>reduce</code> function).  </p>
<p>So let's first focus on the <code>map</code> function.  This map function is applied to every document in the minibatch RDD in parallel so the function should take only one document as an input and be essentially the same as the inner loop of the above non-distributed version.  Unfortunately, this produces a big problem because this code also requires the global topic/word matrix (the <code>expELogBeta</code> variable) which is now an RDD.  <strong>Spark does not allow nested RDD computations!</strong>  That is, one cannot apply a map function to an RDD that involves another RDD (this will create a serialization error).  We have to be a bit more creative!</p>
<p>This problem can be solved by first observing that the E-Step does not necessarily require all of the rows of the topic/word matrix, only those corresponding to the words that are in the minibatch.   Therefore, let's have our <code>map</code> function take a list of 3-tuples that consist of the wordId, the word count, and the row of the topic/word matrix corresponding to the wordId. This way, we can compute the E-Step without working with the distributed topic/word matrix directly.  The code below shows this implementation.</p>
<div class="highlight"><pre><span class="k">def</span> <span class="n">eStep</span><span class="o">(</span><span class="n">expELogBeta</span><span class="k">:</span> <span class="kt">Array</span><span class="o">[(</span><span class="kt">V</span>, <span class="kt">Int</span>, <span class="kt">Int</span><span class="o">)],</span> <span class="n">numTopics</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">alpha</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">gammaThreshold</span><span class="k">:</span> <span class="kt">Double</span><span class="o">,</span> <span class="n">iterations</span><span class="k">:</span> <span class="kt">Int</span><span class="o">)</span><span class="k">:</span> <span class="o">(</span><span class="kt">Array</span><span class="o">[(</span><span class="kt">Int</span>, <span class="kt">Array</span><span class="o">[</span><span class="kt">Double</span><span class="o">])],</span> <span class="nc">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span> <span class="k">=</span> <span class="o">{</span>

    <span class="k">var</span> <span class="n">gammaD</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">](</span><span class="n">numTopics</span><span class="o">,</span> <span class="mi">1</span><span class="o">,</span> <span class="nc">Gamma</span><span class="o">(</span><span class="mf">100.0</span><span class="o">,</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="mf">100.0</span><span class="o">).</span><span class="n">sample</span><span class="o">(</span><span class="n">numTopics</span><span class="o">).</span><span class="n">toArray</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">eLogThetaD</span> <span class="k">=</span> <span class="n">dirichletExpectation</span><span class="o">(</span><span class="n">gammaD</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">expELogThetaD</span> <span class="k">=</span> <span class="n">exp</span><span class="o">(</span><span class="n">eLogThetaD</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">cts</span> <span class="k">=</span> <span class="n">expELogBeta</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">_3</span><span class="o">.</span><span class="n">toDouble</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">wordIDs</span> <span class="k">=</span> <span class="n">expELogBeta</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">_2</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">expELogBetaD</span> <span class="k">=</span> <span class="n">expELogBeta</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="n">x</span><span class="o">.</span><span class="n">_1</span><span class="o">.</span><span class="n">toArray</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">expELogBetaDDM</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">DenseMatrix</span><span class="o">(</span><span class="n">expELogBetaD</span><span class="o">.</span><span class="n">size</span><span class="o">,</span> <span class="n">numTopics</span><span class="o">,</span> <span class="n">expELogBetaD</span><span class="o">.</span><span class="n">flatten</span><span class="o">,</span> <span class="mi">0</span><span class="o">,</span> <span class="n">numTopics</span><span class="o">,</span> <span class="kc">true</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">phiNorm</span> <span class="k">=</span> <span class="n">expELogBetaDDM</span> <span class="o">*</span> <span class="n">expELogThetaD</span> <span class="o">+</span> <span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">100</span>
    <span class="k">val</span> <span class="n">docCounts</span> <span class="k">=</span> <span class="nc">DenseMatrix</span><span class="o">(</span><span class="n">cts</span><span class="o">)</span>

    <span class="k">def</span> <span class="n">gammaUpdate</span><span class="o">(</span><span class="n">pn</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="n">expETD</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
      <span class="k">val</span> <span class="n">term1</span> <span class="k">=</span> <span class="n">expETD</span> <span class="o">:*</span> <span class="o">(</span><span class="n">docCounts</span> <span class="o">/</span> <span class="n">pn</span><span class="o">.</span><span class="n">t</span><span class="o">)</span> <span class="o">*</span> <span class="n">expELogBetaDDM</span>
      <span class="n">term1</span><span class="o">(::,</span> <span class="o">*)</span> <span class="o">+</span> <span class="n">alpha</span>
    <span class="o">}</span>

    <span class="k">def</span> <span class="n">thetaUpdate</span><span class="o">(</span><span class="n">gD</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
      <span class="n">exp</span><span class="o">(</span><span class="n">dirichletExpectation</span><span class="o">(</span><span class="n">gD</span><span class="o">))</span>
    <span class="o">}</span>

    <span class="k">def</span> <span class="n">phiUpdate</span><span class="o">(</span><span class="n">expETD</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="k">=</span> <span class="o">{</span>
      <span class="n">expELogBetaDDM</span> <span class="o">*</span> <span class="n">expETD</span> <span class="o">+</span> <span class="mi">1</span><span class="n">e</span><span class="o">-</span><span class="mi">100</span>
    <span class="o">}</span>

    <span class="k">def</span> <span class="n">eStepIterator</span><span class="o">(</span><span class="n">phiNorm</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="n">expELogThetaD</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span>
                      <span class="n">gammaD</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span><span class="k">:</span> <span class="o">(</span><span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="nc">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="nc">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span> <span class="k">=</span> <span class="o">{</span>

      <span class="k">val</span> <span class="n">lastGamma</span> <span class="k">=</span> <span class="nc">DenseMatrix</span><span class="o">.</span><span class="n">zeros</span><span class="o">[</span><span class="kt">Double</span><span class="o">](</span><span class="n">gammaD</span><span class="o">.</span><span class="n">rows</span><span class="o">,</span> <span class="n">gammaD</span><span class="o">.</span><span class="n">cols</span><span class="o">)</span>

      <span class="k">def</span> <span class="n">loop</span><span class="o">(</span><span class="n">counter</span><span class="k">:</span> <span class="kt">Int</span><span class="o">,</span> <span class="n">newGamma</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="n">newTheta</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="n">newPhi</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span>
               <span class="n">lastGamma</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span><span class="k">:</span> <span class="o">(</span><span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="nc">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">],</span> <span class="nc">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">])</span> <span class="k">=</span> <span class="o">{</span>
        <span class="k">if</span> <span class="o">(((</span><span class="n">mean</span><span class="o">(</span><span class="n">abs</span><span class="o">(</span><span class="n">newGamma</span> <span class="o">-</span> <span class="n">lastGamma</span><span class="o">)))</span> <span class="o">&lt;</span> <span class="n">gammaThreshold</span><span class="o">)</span> <span class="o">||</span> <span class="o">(</span><span class="n">counter</span> <span class="o">&gt;</span> <span class="n">iterations</span> <span class="o">-</span> <span class="mi">1</span><span class="o">))</span> <span class="o">{</span>
          <span class="o">(</span><span class="n">newGamma</span><span class="o">,</span> <span class="n">newPhi</span><span class="o">,</span> <span class="n">newTheta</span><span class="o">)</span>
        <span class="o">}</span>
        <span class="k">else</span> <span class="o">{</span>
          <span class="k">val</span> <span class="n">term1</span> <span class="k">=</span> <span class="n">gammaUpdate</span><span class="o">(</span><span class="n">newPhi</span><span class="o">,</span> <span class="n">newTheta</span><span class="o">)</span>
          <span class="k">val</span> <span class="n">term2</span> <span class="k">=</span> <span class="n">thetaUpdate</span><span class="o">(</span><span class="n">term1</span><span class="o">)</span>
          <span class="k">val</span> <span class="n">term3</span> <span class="k">=</span> <span class="n">phiUpdate</span><span class="o">(</span><span class="n">term2</span><span class="o">)</span>
          <span class="n">loop</span><span class="o">(</span><span class="n">counter</span> <span class="o">+</span> <span class="mi">1</span><span class="o">,</span> <span class="n">term1</span><span class="o">,</span> <span class="n">term2</span><span class="o">,</span> <span class="n">term3</span><span class="o">,</span> <span class="n">newGamma</span><span class="o">)</span>
        <span class="o">}</span>
      <span class="o">}</span>
      <span class="n">loop</span><span class="o">(</span><span class="mi">0</span><span class="o">,</span> <span class="n">gammaD</span><span class="o">,</span> <span class="n">expELogThetaD</span><span class="o">,</span> <span class="n">phiNorm</span><span class="o">,</span> <span class="n">lastGamma</span><span class="o">)</span>
    <span class="o">}</span>

    <span class="c1">//execute recursive loop function</span>
    <span class="k">val</span> <span class="o">(</span><span class="n">newGammaD</span><span class="o">,</span> <span class="n">newPhiNorm</span><span class="o">,</span> <span class="n">newExpELogThetaD</span><span class="o">)</span> <span class="k">=</span> <span class="n">eStepIterator</span><span class="o">(</span><span class="n">phiNorm</span><span class="o">,</span> <span class="n">expELogThetaD</span><span class="o">,</span> <span class="n">gammaD</span><span class="o">)</span>
    <span class="n">gammaD</span> <span class="k">=</span> <span class="n">newGammaD</span>
    <span class="k">val</span> <span class="n">sstatTerm</span><span class="k">:</span> <span class="kt">DenseMatrix</span><span class="o">[</span><span class="kt">Double</span><span class="o">]</span> <span class="k">=</span> <span class="o">(</span><span class="n">docCounts</span> <span class="o">/</span> <span class="n">newPhiNorm</span><span class="o">.</span><span class="n">t</span><span class="o">).</span><span class="n">t</span> <span class="o">*</span> <span class="n">newExpELogThetaD</span><span class="o">.</span><span class="n">t</span>
    <span class="o">(</span><span class="n">sstatTerm</span><span class="o">.</span><span class="n">t</span><span class="o">.</span><span class="n">toArray</span><span class="o">.</span><span class="n">grouped</span><span class="o">(</span><span class="n">sstatTerm</span><span class="o">.</span><span class="n">cols</span><span class="o">).</span><span class="n">toArray</span><span class="o">.</span><span class="n">zip</span><span class="o">(</span><span class="n">wordIDs</span><span class="o">).</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">x</span><span class="o">.</span><span class="n">_2</span><span class="o">,</span> <span class="n">x</span><span class="o">.</span><span class="n">_1</span><span class="o">)),</span> <span class="n">gammaD</span><span class="o">)</span>
  <span class="o">}</span>
</pre></div>


<p>However, currently our minibatch RDD only contains lists of <code>(wordId, frequency)</code> tuples (i.e. bag-of-words format).  We need to somehow combine our current minibatch RDD with the distributed topic/word matrix to get lists of <code>(wordId, frequency, row)</code> 3-tuples in order to be able to apply our new <code>map</code> function to it.  This can be done in the following way.</p>
<ul>
<li>Give each document in the minibatch RDD a unique Id via the <code>zipWithIndex</code> function followed by a <code>map</code> function.</li>
<li>Apply the <code>flatMap</code> function to the RDD to produce an RDD of <code>(docId, wordId, frequency)</code> 3-tuples.</li>
<li>Use the <code>join</code> function to join this RDD with the distributed topic/word matrix via the wordId key to produce an RDD of <code>(docId, wordId, frequency, row)</code> 4-tuples.</li>
<li>Finally use the <code>groupByKey</code> on the <code>docId</code> field to get our minibatch RDD in the correct <code>(row, wordId, frequency)</code> 3-tuple format.</li>
</ul>
<p>This process is shown in the following code.</p>
<div class="highlight"><pre><span class="k">val</span> <span class="n">rowIdCtRDD</span><span class="k">=</span><span class="n">bowRDD</span><span class="o">.</span><span class="n">zipWithIndex</span><span class="o">()</span>   <span class="c1">//give each document and Id</span>
        <span class="o">.</span><span class="n">map</span><span class="o">{</span><span class="k">case</span> <span class="o">(</span><span class="n">bow</span><span class="o">,</span><span class="n">docId</span><span class="o">)=&gt;(</span><span class="n">docId</span><span class="o">,</span><span class="n">bow</span><span class="o">)}</span>   
        <span class="o">.</span><span class="n">flatMap</span><span class="o">{</span><span class="k">case</span> <span class="o">(</span><span class="n">docId</span><span class="o">,</span> <span class="n">bow</span><span class="o">)</span><span class="k">=&gt;</span><span class="n">bow</span><span class="o">.</span><span class="n">map</span><span class="o">{</span><span class="k">case</span> <span class="o">(</span><span class="n">wordId</span><span class="o">,</span><span class="n">ct</span><span class="o">)=&gt;(</span><span class="n">docId</span><span class="o">,(</span><span class="n">wordId</span><span class="o">,</span><span class="n">ct</span><span class="o">))}}</span>  <span class="c1">// create desired tuple</span>
        <span class="o">.</span><span class="n">map</span><span class="o">{</span><span class="k">case</span> <span class="o">(</span><span class="n">docID</span><span class="o">,(</span><span class="n">wordID</span><span class="o">,</span><span class="n">ct</span><span class="o">))</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">wordID</span><span class="o">,(</span><span class="n">docID</span><span class="o">,</span><span class="n">ct</span><span class="o">))}</span>
        <span class="o">.</span><span class="n">join</span><span class="o">(</span>
          <span class="n">expELogBeta</span><span class="o">.</span><span class="n">rows</span><span class="o">.</span><span class="n">map</span><span class="o">{</span><span class="n">x</span><span class="o">=&gt;(</span><span class="n">x</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">toInt</span><span class="o">,</span><span class="n">x</span><span class="o">.</span><span class="n">vector</span><span class="o">)}</span>   <span class="c1">//join to get rows matching wordIds</span>
          <span class="o">)</span>
        <span class="o">.</span><span class="n">map</span><span class="o">{</span><span class="k">case</span> <span class="o">(</span><span class="n">wordId</span><span class="o">,((</span><span class="n">docId</span><span class="o">,</span><span class="n">ct</span><span class="o">),</span><span class="n">row</span><span class="o">))</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">docId</span><span class="o">,(</span><span class="n">row</span><span class="o">,</span><span class="n">wordId</span><span class="o">,</span><span class="n">ct</span><span class="o">))}</span>
        <span class="o">.</span><span class="n">groupByKey</span><span class="o">()</span>   <span class="c1">//group by document Id</span>
        <span class="o">.</span><span class="n">map</span><span class="o">{</span><span class="k">case</span> <span class="o">(</span><span class="n">docId</span><span class="o">,</span><span class="n">rowStats</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">rowStats</span><span class="o">.</span><span class="n">toArray</span><span class="o">}</span>
</pre></div>


<p>Now the E-Step can be performed on the minibatch RDD via a <code>map</code> operation followed by a <code>reduce</code> operation which simply sums the results by id.</p>
<div class="highlight"><pre><span class="k">val</span> <span class="n">eStepRDD</span><span class="k">=</span><span class="n">rowIdCtRDD</span>
        <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span><span class="k">=&gt;</span><span class="n">eStep</span><span class="o">(</span><span class="n">x</span><span class="o">,</span> <span class="n">numTopics</span><span class="o">,</span> <span class="n">alpha</span><span class="o">,</span> <span class="n">gammaThreshold</span><span class="o">,</span> <span class="n">iterations</span><span class="o">).</span><span class="n">_1</span><span class="o">)</span>   <span class="c1">//perform E-Step</span>
        <span class="o">.</span><span class="n">flatMap</span><span class="o">(</span><span class="n">z</span><span class="k">=&gt;</span><span class="n">z</span><span class="o">)</span>
        <span class="o">.</span><span class="n">reduceByKey</span><span class="o">(</span><span class="n">arraySum</span><span class="o">)</span>  <span class="c1">//sum results by rowId</span>
</pre></div>


<h2>A Distributed M-Step</h2>
<p>In the original non-distributed implementation the M-Step was easy.  The new global topc/word matrix was computed by a simple weighted sum of the previous topic\word matrix and the output of the E-Step for the current minibatch (which were both local matrices).  It is a bit more complicated with the distributed implementation because the topic/word matrix is distributed as is the result of the E-step.  Furthermore, the result of the E-step does not contain all rows of the topic/word matrix, only those corresponding to the words present in the current minibatch.  However, we can still compute the same weighted sum with a <code>leftOuterJoin</code> based on the indexes of the rows of the topic/word matrix and the result of the E-step followed by a <em>map</em> that sums the joined rows.  This is shown in the following code.</p>
<div class="highlight"><pre><span class="k">val</span> <span class="n">mStepRDD</span><span class="k">=</span><span class="n">sstatsRM</span><span class="o">.</span><span class="n">rows</span>
        <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span><span class="o">=&gt;(</span><span class="n">x</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">toInt</span><span class="o">,</span><span class="n">x</span><span class="o">.</span><span class="n">vector</span><span class="o">.</span><span class="n">toArray</span><span class="o">))</span>   <span class="c1">//get matrix rows</span>
        <span class="o">.</span><span class="n">leftOuterJoin</span><span class="o">(</span>  <span class="c1">//join with E-step result by rowId</span>
          <span class="n">eStepRDD</span>
            <span class="o">.</span><span class="n">map</span><span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">rowId</span><span class="o">,</span><span class="n">row</span><span class="o">)</span><span class="k">=&gt;</span>
            <span class="o">(</span><span class="n">rowId</span><span class="o">,</span><span class="n">blend</span><span class="o">(</span><span class="n">rho</span><span class="o">,</span><span class="n">row</span><span class="o">,</span><span class="n">mbSize</span><span class="o">.</span><span class="n">toInt</span><span class="o">,</span> <span class="n">threadsSeen</span><span class="o">))}</span>  
        <span class="o">)</span>
        <span class="o">.</span><span class="n">map</span><span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">rowId</span><span class="o">,(</span><span class="n">oldRow</span><span class="o">,</span> <span class="n">newRow</span><span class="o">))</span><span class="k">=&gt;</span>
          <span class="o">(</span><span class="n">rowId</span><span class="o">,</span><span class="n">optionArraySum</span><span class="o">(</span><span class="n">oldRow</span><span class="o">,</span><span class="n">newRow</span><span class="o">))}</span>  <span class="c1">//sum matching rows</span>
        <span class="o">.</span><span class="n">join</span><span class="o">(</span>
          <span class="n">expELogBeta</span><span class="o">.</span><span class="n">rows</span>
            <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span><span class="o">=&gt;(</span><span class="n">x</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">toInt</span><span class="o">,</span><span class="n">x</span><span class="o">.</span><span class="n">vector</span><span class="o">.</span><span class="n">toArray</span><span class="o">))</span>
        <span class="o">)</span>
        <span class="o">.</span><span class="n">map</span><span class="o">{</span><span class="k">case</span> <span class="o">(</span><span class="n">rowId</span><span class="o">,(</span><span class="n">oldRow</span><span class="o">,</span> <span class="n">newRow</span><span class="o">))</span> <span class="k">=&gt;</span>
        <span class="o">(</span><span class="n">rowId</span><span class="o">,</span><span class="n">arrayElMultiply</span><span class="o">(</span><span class="n">oldRow</span><span class="o">,</span><span class="n">newRow</span><span class="o">))}</span>  <span class="c1">//also elementwise multiply with previous matrix</span>
</pre></div>


<p>As you can see, this is exactly the same as performing a weighted sum of two matrices, it just takes a bit more work when they are both distributed.</p>
<h1>The Full Algorithm</h1>
<p>Now we are at a point at which we can describe the full algorithm.  For each minibatch of documents, the following steps are taken.</p>
<ol>
<li>The minibatch RDD is transformed into its bag-of-words form.</li>
<li>The minibatch RDD is joined with the topic/word matrix RDD to get the rows corresponding to the words in the minibatch.</li>
<li>The E-Step function is applied to the minibatch RDD.</li>
<li>The M-Step function is applied to the output RDD of the previous step.</li>
<li>The rows of the topic/word matrix RDD corresponding to the words in the minibatch are updated.</li>
</ol>
<p>These steps are repeated for each minibatch.  The following diagram illustrates this process.</p>
<p><img alt="full algorithm" src="images/full_algorithm.png" /> </p>
<h1>Code Demo</h1>
<p>Now let's try this algorithm on a real dataset of documents.  Let's use the NIPS dataset that was used in the code demo from my previous post on online LDA.  In order to actually implement this algorithm, we must have a way to to iterate over minibatches.  The minibatch iterator should be implemented by the user since it is dependent on how the documents are stored. The following is the code which implements online LDA in Spark (with the minibatch iterator and Spark context code omitted).</p>
<div class="highlight"><pre><span class="k">while</span> <span class="o">(</span><span class="n">mbIterator</span><span class="o">.</span><span class="n">hasNext</span><span class="o">)</span> <span class="o">{</span>

    <span class="k">val</span> <span class="n">mb</span> <span class="k">=</span> <span class="n">mbIterator</span><span class="o">.</span><span class="n">next</span>

    <span class="k">val</span> <span class="n">mbSize</span> <span class="k">=</span> <span class="n">mb</span><span class="o">.</span><span class="n">count</span><span class="o">()</span>

    <span class="n">threadsSeen</span> <span class="o">+=</span> <span class="n">mbSize</span><span class="o">.</span><span class="n">toInt</span>
    <span class="n">numUpdates</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">rho</span> <span class="k">=</span> <span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="o">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">numUpdates</span><span class="o">,</span> <span class="o">-</span><span class="n">decay</span><span class="o">)</span>

    <span class="c1">//raw text to bag-of-words</span>
    <span class="k">val</span> <span class="n">bowRDD</span> <span class="k">=</span> <span class="n">mb</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="n">toBagOfWords</span><span class="o">(</span><span class="n">x</span><span class="o">,</span> <span class="n">vocab</span><span class="o">))</span>

    <span class="c1">//preprocess sstats matrix</span>
    <span class="k">val</span> <span class="n">sstatsPlusEta</span> <span class="k">=</span> <span class="nc">RmElementwiseAdd</span><span class="o">(</span><span class="n">sstatsRM</span><span class="o">,</span> <span class="n">eta</span><span class="o">)</span>
    <span class="k">val</span> <span class="n">expELogBeta</span> <span class="k">=</span> <span class="n">matrixExp</span><span class="o">(</span><span class="n">dirichletExpectation</span><span class="o">(</span><span class="n">sstatsPlusEta</span><span class="o">))</span>

    <span class="c1">//preprocess minibatch RDD</span>
    <span class="k">val</span> <span class="n">rowIdCtRDD</span> <span class="k">=</span> <span class="n">bowRDD</span><span class="o">.</span><span class="n">zipWithIndex</span><span class="o">()</span>
      <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">bow</span><span class="o">,</span> <span class="n">docId</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">docId</span><span class="o">,</span> <span class="n">bow</span><span class="o">)}</span>
      <span class="o">.</span><span class="n">flatMap</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">docId</span><span class="o">,</span> <span class="n">bow</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">bow</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">wordId</span><span class="o">,</span> <span class="n">ct</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">docId</span><span class="o">,</span> <span class="o">(</span><span class="n">wordId</span><span class="o">,</span> <span class="n">ct</span><span class="o">))}}</span>
      <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">docID</span><span class="o">,</span> <span class="o">(</span><span class="n">wordID</span><span class="o">,</span> <span class="n">ct</span><span class="o">))</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">wordID</span><span class="o">,</span> <span class="o">(</span><span class="n">docID</span><span class="o">,</span> <span class="n">ct</span><span class="o">))}</span>
      <span class="o">.</span><span class="n">join</span><span class="o">(</span>
        <span class="n">expELogBeta</span><span class="o">.</span><span class="n">rows</span><span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="n">x</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">x</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">toInt</span><span class="o">,</span> <span class="n">x</span><span class="o">.</span><span class="n">vector</span><span class="o">)}</span>
      <span class="o">)</span>
      <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">wordId</span><span class="o">,</span> <span class="o">((</span><span class="n">docId</span><span class="o">,</span> <span class="n">ct</span><span class="o">),</span> <span class="n">row</span><span class="o">))</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">docId</span><span class="o">,</span> <span class="o">(</span><span class="n">row</span><span class="o">,</span> <span class="n">wordId</span><span class="o">,</span> <span class="n">ct</span><span class="o">))}</span>
      <span class="o">.</span><span class="n">groupByKey</span><span class="o">()</span>
      <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">docId</span><span class="o">,</span> <span class="n">rowStats</span><span class="o">)</span> <span class="k">=&gt;</span> <span class="n">rowStats</span><span class="o">.</span><span class="n">toArray</span><span class="o">}</span>

    <span class="c1">//perform E-Step</span>
    <span class="k">val</span> <span class="n">eStepRDD</span> <span class="k">=</span> <span class="n">rowIdCtRDD</span>
      <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="n">eStep</span><span class="o">(</span><span class="n">x</span><span class="o">,</span> <span class="n">numTopics</span><span class="o">,</span> <span class="n">alpha</span><span class="o">,</span> <span class="n">gammaThreshold</span><span class="o">,</span> <span class="n">iterations</span><span class="o">).</span><span class="n">_1</span><span class="o">)</span>
      <span class="o">.</span><span class="n">flatMap</span><span class="o">(</span><span class="n">z</span> <span class="k">=&gt;</span> <span class="n">z</span><span class="o">)</span>
      <span class="o">.</span><span class="n">reduceByKey</span><span class="o">(</span><span class="n">arraySum</span><span class="o">)</span>

    <span class="c1">//perform M-Step</span>
    <span class="k">val</span> <span class="n">mStepRDD</span> <span class="k">=</span> <span class="n">sstatsRM</span><span class="o">.</span><span class="n">rows</span>
      <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">x</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">toInt</span><span class="o">,</span> <span class="n">x</span><span class="o">.</span><span class="n">vector</span><span class="o">.</span><span class="n">toArray</span><span class="o">))</span>
      <span class="o">.</span><span class="n">leftOuterJoin</span><span class="o">(</span>
        <span class="n">eStepRDD</span>
          <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">rowId</span><span class="o">,</span> <span class="n">row</span><span class="o">)</span> <span class="k">=&gt;</span>
          <span class="o">(</span><span class="n">rowId</span><span class="o">,</span> <span class="n">blend</span><span class="o">(</span><span class="n">rho</span><span class="o">,</span> <span class="n">row</span><span class="o">,</span> <span class="n">mbSize</span><span class="o">.</span><span class="n">toInt</span><span class="o">,</span> <span class="n">threadsSeen</span><span class="o">))</span>
        <span class="o">}</span>
      <span class="o">)</span>
      <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">rowId</span><span class="o">,</span> <span class="o">(</span><span class="n">oldRow</span><span class="o">,</span> <span class="n">newRow</span><span class="o">))</span> <span class="k">=&gt;</span>
      <span class="o">(</span><span class="n">rowId</span><span class="o">,</span> <span class="n">optionArraySum</span><span class="o">(</span><span class="n">oldRow</span><span class="o">,</span> <span class="n">newRow</span><span class="o">))</span>
    <span class="o">}</span>
      <span class="o">.</span><span class="n">join</span><span class="o">(</span>
        <span class="n">expELogBeta</span><span class="o">.</span><span class="n">rows</span>
          <span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="o">(</span><span class="n">x</span><span class="o">.</span><span class="n">index</span><span class="o">.</span><span class="n">toInt</span><span class="o">,</span> <span class="n">x</span><span class="o">.</span><span class="n">vector</span><span class="o">.</span><span class="n">toArray</span><span class="o">))</span>
      <span class="o">)</span>
      <span class="o">.</span><span class="n">map</span> <span class="o">{</span> <span class="k">case</span> <span class="o">(</span><span class="n">rowId</span><span class="o">,</span> <span class="o">(</span><span class="n">oldRow</span><span class="o">,</span> <span class="n">newRow</span><span class="o">))</span> <span class="k">=&gt;</span>
      <span class="o">(</span><span class="n">rowId</span><span class="o">,</span> <span class="n">arrayElMultiply</span><span class="o">(</span><span class="n">oldRow</span><span class="o">,</span> <span class="n">newRow</span><span class="o">))</span>
    <span class="o">}</span>

    <span class="c1">//update sstats matrix</span>
    <span class="n">sstatsRM</span> <span class="k">=</span> <span class="k">new</span> <span class="nc">IndexedRowMatrix</span><span class="o">(</span>
      <span class="n">mStepRDD</span><span class="o">.</span><span class="n">map</span><span class="o">(</span><span class="n">x</span> <span class="k">=&gt;</span> <span class="nc">IndexedRow</span><span class="o">(</span><span class="n">x</span><span class="o">.</span><span class="n">_1</span><span class="o">.</span><span class="n">toLong</span><span class="o">,</span> <span class="nc">Vectors</span><span class="o">.</span><span class="n">dense</span><span class="o">(</span><span class="n">x</span><span class="o">.</span><span class="n">_2</span><span class="o">)))</span>
    <span class="o">)</span>
  <span class="o">}</span>

  <span class="c1">//print learned topics</span>
  <span class="n">showTopics</span><span class="o">(</span><span class="mi">10</span><span class="o">,</span> <span class="n">sstatsRM</span><span class="o">,</span> <span class="n">id2Word</span><span class="o">)</span>
</pre></div>


<p>The above code produces the following topics</p>
<div class="highlight"><pre><span class="nx">Topic</span> <span class="mi">0</span><span class="o">:</span> <span class="nx">List</span><span class="p">((</span><span class="nx">activity</span><span class="p">,</span><span class="mf">0.032092510865724705</span><span class="p">),</span> <span class="p">(</span><span class="nx">voltage</span><span class="p">,</span><span class="mf">0.028580820005928525</span><span class="p">),</span> <span class="p">(</span><span class="nx">neurons</span><span class="p">,</span><span class="mf">0.025338836825008867</span><span class="p">),</span> <span class="p">(</span><span class="nx">cortex</span><span class="p">,</span><span class="mf">0.024546295981522796</span><span class="p">),</span> <span class="p">(</span><span class="nx">rhythmic</span><span class="p">,</span><span class="mf">0.023029320016109932</span><span class="p">),</span> <span class="p">(</span><span class="nx">wta</span><span class="p">,</span><span class="mf">0.02047031045772205</span><span class="p">),</span> <span class="p">(</span><span class="nx">tones</span><span class="p">,</span><span class="mf">0.02044761938141161</span><span class="p">),</span> <span class="p">(</span><span class="nx">obs</span><span class="p">,</span><span class="mf">0.016662670108027338</span><span class="p">),</span> <span class="p">(</span><span class="nx">analog</span><span class="p">,</span><span class="mf">0.01639142696901859</span><span class="p">),</span> <span class="p">(</span><span class="nx">cortical</span><span class="p">,</span><span class="mf">0.016364893763539945</span><span class="p">))</span>

<span class="nx">Topic</span> <span class="mi">1</span><span class="o">:</span> <span class="nx">List</span><span class="p">((</span><span class="nx">inference</span><span class="p">,</span><span class="mf">0.06811446156014736</span><span class="p">),</span> <span class="p">(</span><span class="nx">models</span><span class="p">,</span><span class="mf">0.038610276907281436</span><span class="p">),</span> <span class="p">(</span><span class="nx">data</span><span class="p">,</span><span class="mf">0.03699453925479673</span><span class="p">),</span> <span class="p">(</span><span class="nx">prior</span><span class="p">,</span><span class="mf">0.033185332552837114</span><span class="p">),</span> <span class="p">(</span><span class="nx">posterior</span><span class="p">,</span><span class="mf">0.032290300391455466</span><span class="p">),</span> <span class="p">(</span><span class="nx">probability</span><span class="p">,</span><span class="mf">0.025078686881619315</span><span class="p">),</span> <span class="p">(</span><span class="nx">parent</span><span class="p">,</span><span class="mf">0.02041949893883364</span><span class="p">),</span> <span class="p">(</span><span class="nx">parameters</span><span class="p">,</span><span class="mf">0.018027714272498996</span><span class="p">),</span> <span class="p">(</span><span class="nx">gaussian</span><span class="p">,</span><span class="mf">0.017631357263122132</span><span class="p">),</span> <span class="p">(</span><span class="nx">log</span><span class="p">,</span><span class="mf">0.017515618349787442</span><span class="p">))</span>

<span class="nx">Topic</span> <span class="mi">2</span><span class="o">:</span> <span class="nx">List</span><span class="p">((</span><span class="nx">state</span><span class="p">,</span><span class="mf">0.04652001892330229</span><span class="p">),</span> <span class="p">(</span><span class="nx">policy</span><span class="p">,</span><span class="mf">0.04113501660902407</span><span class="p">),</span> <span class="p">(</span><span class="nx">eligibility</span><span class="p">,</span><span class="mf">0.03905536641647694</span><span class="p">),</span> <span class="p">(</span><span class="nx">sarsa</span><span class="p">,</span><span class="mf">0.03905536631097504</span><span class="p">),</span> <span class="p">(</span><span class="nx">truncated</span><span class="p">,</span><span class="mf">0.03326530692419029</span><span class="p">),</span> <span class="p">(</span><span class="nx">traces</span><span class="p">,</span><span class="mf">0.03142425104787902</span><span class="p">),</span> <span class="p">(</span><span class="nx">memoryless</span><span class="p">,</span><span class="mf">0.027722772212976254</span><span class="p">),</span> <span class="p">(</span><span class="nx">policies</span><span class="p">,</span><span class="mf">0.024839780749834842</span><span class="p">),</span> <span class="p">(</span><span class="nx">agent</span><span class="p">,</span><span class="mf">0.02303785156504393</span><span class="p">),</span> <span class="p">(</span><span class="nx">pomdps</span><span class="p">,</span><span class="mf">0.02301863583267124</span><span class="p">))</span>

<span class="nx">Topic</span> <span class="mi">3</span><span class="o">:</span> <span class="nx">List</span><span class="p">((</span><span class="nx">algorithm</span><span class="p">,</span><span class="mf">0.02135927293979804</span><span class="p">),</span> <span class="p">(</span><span class="nx">learning</span><span class="p">,</span><span class="mf">0.010459512976094452</span><span class="p">),</span> <span class="p">(</span><span class="nx">error</span><span class="p">,</span><span class="mf">0.008990052150829781</span><span class="p">),</span> <span class="p">(</span><span class="nx">weight</span><span class="p">,</span><span class="mf">0.0073065564236364155</span><span class="p">),</span> <span class="p">(</span><span class="kd">function</span><span class="p">,</span><span class="mf">0.007146485129287556</span><span class="p">),</span> <span class="p">(</span><span class="nx">vector</span><span class="p">,</span><span class="mf">0.006762208984454446</span><span class="p">),</span> <span class="p">(</span><span class="nx">number</span><span class="p">,</span><span class="mf">0.006051699485084758</span><span class="p">),</span> <span class="p">(</span><span class="o">-</span><span class="nx">rcb</span><span class="o">-</span><span class="p">,</span><span class="mf">0.005855829811131096</span><span class="p">),</span> <span class="p">(</span><span class="nx">results</span><span class="p">,</span><span class="mf">0.005837170436069406</span><span class="p">),</span> <span class="p">(</span><span class="nx">probability</span><span class="p">,</span><span class="mf">0.005788740947910415</span><span class="p">))</span>

<span class="nx">Topic</span> <span class="mi">4</span><span class="o">:</span> <span class="nx">List</span><span class="p">((</span><span class="nx">attentional</span><span class="p">,</span><span class="mf">0.031205396268822388</span><span class="p">),</span> <span class="p">(</span><span class="nx">image</span><span class="p">,</span><span class="mf">0.030771091189727498</span><span class="p">),</span> <span class="p">(</span><span class="nx">location</span><span class="p">,</span><span class="mf">0.02025190023001232</span><span class="p">),</span> <span class="p">(</span><span class="nx">images</span><span class="p">,</span><span class="mf">0.01848811029404064</span><span class="p">),</span> <span class="p">(</span><span class="nx">target</span><span class="p">,</span><span class="mf">0.01788585445419973</span><span class="p">),</span> <span class="p">(</span><span class="nx">streams</span><span class="p">,</span><span class="mf">0.014740100512122468</span><span class="p">),</span> <span class="p">(</span><span class="nx">field</span><span class="p">,</span><span class="mf">0.014659463334666768</span><span class="p">),</span> <span class="p">(</span><span class="nx">tones</span><span class="p">,</span><span class="mf">0.014100436801905392</span><span class="p">),</span> <span class="p">(</span><span class="nx">bottom</span><span class="o">-</span><span class="nx">up</span><span class="p">,</span><span class="mf">0.013031110714897202</span><span class="p">),</span> <span class="p">(</span><span class="nx">dts</span><span class="p">,</span><span class="mf">0.012683978720498826</span><span class="p">))</span>
</pre></div>


<p>As you can see, these topics seem quite coherent and roughly correspond to different fields of machine learning.</p>
<h1>References</h1>
<ul>
<li><a href="https://github.com/alexminnaar/SparkOnlineLDA">Github repo</a> containing example code.</li>
<li><a href="http://alexminnaar.com/online-latent-dirichlet-allocation-the-best-option-for-topic-modeling-with-large-data-sets.html">Previous post</a> on online LDA.</li>
<li><a href="http://spark.apache.org/docs/1.2.1/programming-guide.html">Spark programming guide</a>.</li>
</ul>
                    </article>
                </div>
            </aside><!-- /#featured -->
            
        
        

    
            <aside id="featured">
                <div class="body">
                    <article>
                        <h1 class="entry-title"><a href="/deep-learning-basics-neural-networks-backpropagation-and-stochastic-gradient-descent.html">Deep Learning Basics: Neural Networks, Backpropagation and Stochastic Gradient Descent</a></h1>
<div class="post-info">
	<ul>
        <li class="vcard author">
                 by&nbsp;<a class="url fn" href="/author/alex-minnaar.html">Alex Minnaar</a>
        </li>
        <li class="published" title="2015-02-14T00:00:00+01:00">
          on&nbsp;Sat 14 February 2015
        </li>

	</ul>
<p>Category: <a href="/tag/supervised-learning.html">   Supervised Learning</a></p>
</div><!-- /.post-info --><p>In the last couple of years <em>Deep Learning</em> has received a great deal of press.  This press is not without warrant - <em>Deep Learning</em> has produced stat-of-the-art results in many computer vision and speech processing tasks.  However, I believe that the press has given people the impression that <em>Deep Learning</em> is some kind of imprenetrable, esoteric field that can only be understood by academics.  In this blog post I want to try to erase that impression and provide a practical overview of some of <em>Deep Learning's</em> basic concepts.</p>
<p>At its core, <em>Deep Learning</em> is a class of of neural network models.  That is, a model with an input layer, an output layer, and an arbitrary number of hidden layers. These layers are made up of neurons or neural units.  They are called neurons because they share some similarities with the behaviour of the neurons present in the human brain (though this comparison has drawn a lot of criticism from neuroscientists).  For our purposes, we can think of a neuron as a nonlinear function of the weighted sum of its inputs.  Since the neuron is really the most basic part of any <em>Deep Learning</em> model it is a good place to start.</p>
<h1>The Single Neuron Model</h1>
<p>A neuron is a function that maps an input vector <span class="math">\(\{x_1,...,x_K\}\)</span> to a scalar output <span class="math">\(y\)</span> via a weight vector <span class="math">\(\{w_1,...,w_K\}\)</span> and a nonlinear function <span class="math">\(f\)</span>.</p>
<p><img alt="general graphical model" src="images/neuron.png" title="=100x20" /> </p>
<p>The function <span class="math">\(f\)</span> takes a weighted sum of the inputs and returns <span class="math">\(y\)</span>.</p>
<p>
<div class="math">$$y=f(\sum_{i=0}^Kw_ix_i)=f(\mathbf{w^Tx})$$</div>
</p>
<p>Often an additional element is added to the input vector that is always equal to <span class="math">\(1\)</span> with a corresponding additional weight element which acts as a bias.  The function <span class="math">\(f\)</span> is called the link function which provides the nonlinearity between the input and output.  A common choice for this link function is the <strong>logistic function</strong> which is defined as</p>
<p>
<div class="math">$$f(u)=\frac{1}{1+e^{u}}$$</div>
</p>
<p>With the appropriate substitutions the final formula for the single neuron model becomes</p>
<p>
<div class="math">$$y=\frac{1}{1+e^{\mathbf{w^Tx}}}$$</div>
</p>
<p>If you plot the logistic function,</p>
<p><img alt="general graphical model" src="images/logistic.png" /> </p>
<p>you can see that it is smooth and differentiable and bound between <span class="math">\(0\)</span> and <span class="math">\(1\)</span>.  We shall see that these are two important properties.  The derivative of the logistic function is simply</p>
<p>
<div class="math">$$\frac{d f(u)}{d u}=f(u)(1-f(u))=f(u)f(-u)$$</div>
</p>
<p>This derivative will be used when we learn the weight vector <span class="math">\(\bf{w}\)</span> via <strong>stochastic gradient descent</strong>.</p>
<p>Like any optimization problem, our goal is to minimize an objective function.  Traditionally, the objective function measures the difference between the actual output <span class="math">\(t\)</span> and the predicted output <span class="math">\(f(\mathbf{w^Tx})\)</span>. In this case we will be using the squared loss function</p>
<p>
<div class="math">$$E=\frac{1}{2}(t - y)^2=\frac{1}{2}(t-f(\mathbf{w^Tx}))^2$$</div>
</p>
<p>We want to find the weights <span class="math">\(\mathbf{w}\)</span> such that the above objective is minimized.  We do this with stochastic gradient descent (SGD).  In SGD we iteratively update our weight parameters in the direction of the gradient of the loss function until we have reached a minimum.  Unlike traditional gradient descent, we do not use the entire dataset to compute the gradient at each iteration.  Instead, at each iteration we randomly select a single data point from our dataset and move in the direction of the gradient with respect to that data point.  Obviously this is only an approximation of the true gradient but it can be proven that we will eventually reach the minimum by following this <em>noisey</em> gradient.  There are several advantages to using stochastic gradient descent over traditional gradient descent.</p>
<ol>
<li>Gradient descent requires loading the entire dataset into main memory.  If your dataset is large this can be problematic.  Stochastic gradient descent only requires one data point at a time (or sometimes a minibatch of data points) which is much less memory intensive.</li>
<li>Most datasets have redundancy in them.  Traditional gradient descent requires one full pass over the data until an update is made.  Due to redundancy, a meaningful update can often be made without iterating over the entire dataset as with stochastic gradient descent.</li>
<li>As a consequence of the previous point, stochastic gradient descent can often converge faster than traditional gradient descent.  It is also guaranteed to find the global minimum if the loss function is convex.</li>
</ol>
<p>Our objective function <span class="math">\(E\)</span> is already defined in terms of a single data point so let's procede to compute its gradient with respect to an aribtrary element of our weight vector <span class="math">\(w_i\)</span>.</p>
<p>
<div class="math">$$\begin{align}
\frac{\partial E}{\partial w_i} &amp;= \frac{\partial E}{\partial y} \cdot \frac{\partial y}{\partial u} \cdot\frac{\partial u}{\partial w_i} \\
&amp;= (y-t) \cdot y(1-y) \cdot x_i
\end{align}$$</div>
</p>
<p>Now we are able to obtain the stochastic gradient descent update equation (in vector notation)</p>
<p>
<div class="math">$$\mathbf{w}^{new}=\mathbf{w}^{old}- \eta \cdot (y-t) \cdot y(1-y) \cdot \mathbf{x}$$</div>
</p>
<p>Where <span class="math">\(\eta&gt;0\)</span> is the step size.  As stated previously, <span class="math">\((\mathbf{x},y)\)</span> data points are sequentially fed into this update equation until the weights <span class="math">\(\mathbf{w}\)</span> converge to their optimal value.  This is how we use stochastic gradient descent to learn the weights for the single neuron model.</p>
<p>What we just did is also known as <strong>logistic regression</strong> and if we had replaced our logistic function with a unit step function we would have made what is known as a <strong>perceptron</strong>!  Now let's extend this relatively simple model to something a bit more complex...</p>
<h1>The Neural Network</h1>
<p>A neural network consists of an input layer, output layer, and hidden layer. Our input layer consists of the input vector <span class="math">\(\mathbf{x}=\{x_1,...,x_K\}\)</span>.  The hidden layer  consists of a vector of <span class="math">\(N\)</span> neurons <span class="math">\(\mathbf{h}=\{h_1,...,h_N\}\)</span>.  Finally there is an output layer with one neuron for every element of the output vector <span class="math">\(\mathbf{y}=\{y_1,...,y_M\}\)</span>.  Every element in the input layer is connected to every neuron in the hidden layer with <span class="math">\(w_{ki}\)</span> indicating the weight associated with the connection between the <span class="math">\(k^{th}\)</span> input element and the <span class="math">\(i^{th}\)</span> hidden neuron.  The same connection structure is present between the hidden and output layers with  <span class="math">\(w'_{ij}\)</span> indicating the weight associated with the connection between the <span class="math">\(i^{th}\)</span> hidden neuron and the <span class="math">\(j^{th}\)</span> output neuron.  This network structure is better illustrated in the below diagram.</p>
<p><img alt="general graphical model" src="images/neural_network.png" title="=250x" /> </p>
<p>It is helpful to think of the weight <span class="math">\(w_{ki}\)</span> as the the <span class="math">\((k,i)^{th}\)</span> entry in a <span class="math">\(K \times N\)</span> weight matrix <span class="math">\(\mathbf{W}\)</span> and similarly weight <span class="math">\(w'_{ij}\)</span> as the <span class="math">\((i,j)^{th}\)</span> entry in a <span class="math">\(N \times M\)</span> weight matrix <span class="math">\(\mathbf{W'}\)</span>.  The output of each neuron in the hidden and output layer is computed in the exact same way as before.  It is simply the logistic function applied to the weighted sum of the neuron's inputs.  For example, the output of an arbitrary neuron in the hidden layer <span class="math">\(h_i\)</span> is</p>
<p>
<div class="math">$$h_i=f(u_i)=f(\sum^K_{k=1}w_{ki}x_k)$$</div>
</p>
<p>and similarly for the output of an arbitrary output neuron <span class="math">\(y_j\)</span> is</p>
<p>
<div class="math">$$y_j=f(u'_j)=f(\sum^N_{i=1}w'_{ij}h_i)$$</div>
</p>
<p>The objective function is also the same as before except now it is summed over all elements in the output layer.</p>
<p>
<div class="math">$$E=\frac{1}{2}\sum^M_{j=1}(y_j-t_j)^2$$</div>
</p>
<p>Unlike before, we need to construct update equations for <em>both</em> sets of weights - the input-to-hidden layer weights <span class="math">\(w_{ki}\)</span> and the hidden-to-output weights <span class="math">\(w'_{ij}\)</span>.  In order to do this we need to compute the gradient of our objective function <span class="math">\(E\)</span> with respect to <span class="math">\(w_{ki}\)</span> as well as the gradient with respect to <span class="math">\(w'_{ij}\)</span>.  We must start with the gradient with respect to <span class="math">\(w'_{ij}\)</span> (the hidden-to-output weights) and we shall see why later.  In order to compute <span class="math">\(\frac{\partial E}{\partial{w'_{ij}}}\)</span> we must recall our high-school calculus, specifically the chain rule.  From the chain rule, we must first take the derivative of <span class="math">\(E\)</span> with respect to <span class="math">\(y'_j\)</span>.  Then we must take the derivative of <span class="math">\(y_j\)</span> (i.e. the logistic function) with respect to <span class="math">\(w'_{ij}\)</span> which needs yet another application of the chain rule.  We first take the derivative of the logistic function with respect to its input <span class="math">\(u'_j\)</span>, then finally we can take the derivative of this input with respect to <span class="math">\(w'_{ij}\)</span> and we arrive at our desired value.  This process is clearly defined below.</p>
<p>From the chain rule,</p>
<p>
<div class="math">$$\frac{\partial E}{\partial w'_{ij}}=\frac{\partial E}{\partial y_j} \cdot \frac{\partial y_j}{\partial u'_j} \cdot \frac{\partial u'_j}{\partial w'_{ij}}$$</div>
</p>
<p>The derivative of <span class="math">\(E\)</span> with respect to <span class="math">\(y_j\)</span> is simply,</p>
<p>
<div class="math">$$\frac{\partial E}{\partial y_j}=y_j-t_j$$</div>
</p>
<p>From the last section we saw that the derivative of the logistic function <span class="math">\(f\)</span> with respect to its input <span class="math">\(u\)</span> is <span class="math">\(f(u)(1-f(u))\)</span>.  If we apply this we get,</p>
<p>
<div class="math">$$\frac{\partial y_j}{\partial u'_j}=y_j(1-y_j)$$</div>
</p>
<p>where <span class="math">\(y_j=f(u'_j)\)</span>.  Next we compute the derivative of <span class="math">\(u'_j=\sum^N_{i=1}w'_{ij}h_i\)</span> with respect to a particular <span class="math">\(w'_{ij}\)</span> which is simply <span class="math">\(h_i\)</span>.  So, after making the appropriate subsitutions, we get</p>
<p>
<div class="math">$$\frac{\partial E}{\partial w'_{ij}}=(y_j-t_j) \cdot y_j(1-y_j) \cdot h_i$$</div>
</p>
<p>With this gradient we can construct the update equation for <span class="math">\(w'_{ij}\)</span></p>
<p>
<div class="math">$$w'^{new}_{ij}=w'^{old}_{ij} - \eta \cdot (y_j-t_j) \cdot y_j(1-y_j) \cdot h_i$$</div>
</p>
<p>Now let's turn our attention to the gradient of the objective function with respect to the input-to-hidden weights <span class="math">\(w_{ki}\)</span>.  As we shall see, this gradient has already been partially computed when we computed the previous gradient.</p>
<p>Using the chain rule, the full gradient is</p>
<p>
<div class="math">$$\frac{\partial E}{\partial w_{ki}}=\sum^M_{j=1}(\frac{\partial E}{\partial y_j}\cdot \frac{\partial y_j}{\partial u'_j} \cdot \frac{\partial u'_j}{\partial h_i} )\cdot \frac{\partial h_i}{\partial u_i} \cdot \frac{\partial u_i}{\partial w_{ki}}$$</div>
</p>
<p>The sum is due to the fact that the hidden unit that <span class="math">\(w_{ki}\)</span> connects to is itself connected to every output unit, thus each of these gradients need to be taken into account as well.  We have already computed both <span class="math">\(\frac{\partial E}{\partial y_j}\)</span> and <span class="math">\(\frac{\partial y_j}{\partial u'_j}\)</span> which means that</p>
<p>
<div class="math">$$\frac{\partial E}{\partial y_j}\cdot \frac{\partial y_j}{\partial u'_j} = (y_j-t_j) \cdot y_j(1-y_j)$$</div>
</p>
<p>Now we need to compute the remaining derivatives <span class="math">\(\frac{\partial u'_j}{\partial h_i}\)</span>, <span class="math">\(\frac{\partial h_i}{\partial u_i}\)</span>, and <span class="math">\(\frac{\partial u_i}{\partial w_{ki}}\)</span>.  So let's do just that.</p>
<p>
<div class="math">$$\frac{\partial u'_j}{\partial h_i}=\frac{\partial \sum^N_{i=1}w'_{ij}h_i}{\partial h_i}=w'_{ij}$$</div>
</p>
<p>and, again using the derivative of the logistic function</p>
<p>
<div class="math">$$\frac{\partial h_i}{\partial u_i}=h_i(1-h_i)$$</div>
</p>
<p>and finally</p>
<p>
<div class="math">$$\frac{\partial u_i}{\partial w_{ki}}=\frac{\partial \sum^K_{k=1}w_{ki}x_k}{\partial w_{ki}}=x_k$$</div>
</p>
<p>After making the appropriate substitutions we arrive at the gradient</p>
<p>
<div class="math">$$\frac{\partial E}{\partial w_{ki}}=\sum^M_{j=1}[(y_j-t_j) \cdot y_j(1-y_j) \cdot w'_{ij}] \cdot h_i(1-h_i) \cdot x_k$$</div>
</p>
<p>And the update equation becomes</p>
<p>
<div class="math">$$w^{new}_{ki}=w^{old}_{ki} - \eta \cdot \sum^M_{j=1}[(y_j-t_j) \cdot y_j(1-y_j) \cdot w'_{ij}] \cdot h_i(1-h_i) \cdot x_k$$</div>
</p>
<p>This process is known as <strong>backpropagation</strong> because we begin with the final output error <span class="math">\(y_j-t_j\)</span> for the output neuron <span class="math">\(j\)</span> and this error gets propagated backwards throughout the network in order to update the weights.</p>
<h1>Wrapping Everything Up</h1>
<p>In this blog post we started with the simple single neuron model and we learned the model weights by computing the gradient of the objective function and using it in the stochastic gradient descent update equation.  Then we moved on to the slightly more complicated neural network model.  In this case we computed the required gradients using a procedure known as backpropagation and we again used these gradients in the SGD update equations.  True <em>Deep Learning</em> models either contain many more hidden layers or neurons in different configurations but they still adhere to the basic principles described here.  Hopefully this post has made <em>Deep Learning</em> seem like a more understandable and less daunting field of machine learning.</p>
<h1>References</h1>
<ul>
<li><a href="https://www.coursera.org/course/neuralnets">Neural Networks for Machine Learning</a> Coursera course from Geoffrey Hinton.</li>
<li><a href="http://deeplearning.stanford.edu/tutorial/">Deep Learning Tutorial</a> from Stanford.</li>
</ul>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = '//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML';
    mathjaxscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: 'center'," +
        "    displayIndent: '0em'," +
        "    showMathMenu: true," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }" +
        "    } " +
        "}); ";
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
                    </article>
                </div>
            </aside><!-- /#featured -->
            
        
        

    
            <aside id="featured">
                <div class="body">
                    <article>
                        <h1 class="entry-title"><a href="/building-a-shoutbox-app-with-cassandra-and-nodejs.html">Building a Shoutbox App with Cassandra and Node.js</a></h1>
<div class="post-info">
	<ul>
        <li class="vcard author">
                 by&nbsp;<a class="url fn" href="/author/alex-minnaar.html">Alex Minnaar</a>
        </li>
        <li class="published" title="2015-02-08T00:00:00+01:00">
          on&nbsp;Sun 08 February 2015
        </li>

	</ul>
<p>Category: <a href="/tag/software-engineering.html">   Software Engineering</a></p>
</div><!-- /.post-info --><p>In this blog post I wanted to provide a demonstration of how to use the Cassandra database system in a practical web application.  We'll create a shoutbox app which you can think of as a very simple version of Twitter.</p>
<h1>What is Cassandra?</h1>
<p>Cassandra is a NoSQL distributed database system that is designed to deal with large amounts of data.  Its key advantages are the following.</p>
<ul>
<li><strong>Scalability:</strong>  Allows you to increase capacity by adding cheap commodity hardware without any downtime.</li>
<li><strong>Fault Tolerant:</strong>  There is data replications and no single point of failure so if a node goes down the system remains operational.</li>
<li><strong>Linear Performance:</strong> Throughput increases linearly with the number of nodes in the cluster.</li>
<li><strong>Data Flexibility:</strong>  Supports many data formats and allows for a variable number of columns for each row.</li>
</ul>
<p>In terms of the <a href="http://en.wikipedia.org/wiki/CAP_theorem">CAP theorem</a>, Cassandra values availability and partition tolerance at the expence of consistency.  This means that Cassandra uses <em>eventual consistency</em> which means that eventually the system will reach a consistent state but at any given time it is possible that different nodes can hold different versions of the same data.  The actual architecture of Cassandra is beyond the scope of this blog post but it involves commit logs, memtables, and peer-to-peer gossip protocols.</p>
<h1>How does Cassandra Work?</h1>
<h2>Column Families</h2>
<p>Column families are similar to RDMS tables.  Cassandra is a <em>column-oriented</em> database which means that columns are defined beforehand as well as the data format that each column can hold.  Unlike the RDMS system, rows in Cassandra can contain any number of columns in any order like in the following example.</p>
<p><img alt="column family example" src="images/static_column_family.png" /></p>
<p>Cassandra also offers <em>dynamic</em> column families which allows the user to create a row that contains precomputed results that can be quickly retrieved.  This is why Cassandra is described as a <em>key-value</em> database where column <em>values</em> can be quickly lookedup by their associated row <em>key</em>. </p>
<h2>Keyspaces</h2>
<p>Cassandra Keyspaces are similar to schemas in RDMS databases.  A keyspace groups and applications column families together.  Replication is done on a <em>per-keyspace</em> basis so data that resides in the same keyspace is replicated in the same way.  There is usually one keyspace per application.</p>
<h2>Denormalization</h2>
<p>One of the disadvantages of Cassandra compared to RDMS databases is that it is not well-suited for ad-hoc complex queries.  As a result you must know what queries you will need to perform beforehand and create your keyspaces accordingly.  Cassandra does not have the same foreign key relationships that RDMS databases have therefore you cannot <em>join</em> multiple column families.  As a consequence, the data that you want to retrieve in your queries should be contained in the same column family.  Therefore the queries that you plan on performing will define your data model.</p>
<h1>Building a Shoutbox App</h1>
<p>Say you have developed a Twitter-like app where users can post their thoughts (shouts) as well as follow other users.  Let's also assume that this app has exploded in popularity to the point where RDMS systems can no longer handle the huge amount of data so you turn to Cassandra.  Let's learn how an app such as this can interact with Cassandra.</p>
<h2>Queries we will Use</h2>
<p>As stated previously, Cassandra uses denormalization which means that we need to first decide what queries we want to perform, then optimize our data model according to these queries.  In our shoutbox app we would like to</p>
<ul>
<li>Show all users.</li>
<li>Show who a user is following.</li>
<li>Show who is following a user.</li>
<li>Show the shouts of a particular user.</li>
<li>Show all shouts.</li>
</ul>
<p>Consequently, we will create the following column families to accomodate these queries.</p>
<ul>
<li><strong>USERS</strong> which will contain <em>username</em> and <em>password</em> columns.</li>
<li><strong>FOLLOWING</strong> which will contain <em>username</em> and <em>followed</em> columns.</li>
<li><strong>FOLLOWERS</strong> which will contain <em>username</em> and <em>following</em> columns.</li>
<li><strong>SHOUTS</strong> which will contain <em>shout_id</em>, <em>username</em>, and <em>body</em> columns.</li>
<li><strong>USERSHOUTS</strong> which will contain <em>username</em>, <em>shout_id</em>, and <em>body</em> columns.</li>
<li><strong>SHOUTWALL</strong> which will contain <em>username</em>, <em>shout_id</em>, <em>posted_by</em> and <em>body</em> columns.</li>
</ul>
<p>It may seem redundant to create all of these column families from the RDMS perspective but denomalization is necessary in Cassandra.</p>
<h2>Creating the Column Families</h2>
<p>We can actually create these column families using the CQL shell.  But first we have to create the keyspace for our app which we will call <code>shoutkeyspace</code>.</p>
<div class="highlight"><pre>CREATE KEYSPACE shoutkeyspace WITH REPLICATION - {&#39;class&#39; : &#39;SimpleStrategy&#39;, &#39;replication_factor&#39; : 3};
</pre></div>


<p>In this example we will not be using multiple data centers so <code>SimpleStrategy</code> replication is used rather than  <code>NetworkTopology</code>.  We also use a <code>replication_factor</code> of 3 which indicates that our data will be replicated 3 times in our cluster.  We will use this keyspace to create our column families with the <code>USE shoutkeyspace;</code> command.</p>
<p>Let's first create the <strong>USERS</strong> column family.</p>
<div class="highlight"><pre>CREATE TABLE users(username text PRIMARY KEY, password text);
</pre></div>


<p>and now the <strong>FOLLOWING</strong> column family.</p>
<div class="highlight"><pre>CREATE TABLE following(username text PRIMARY KEY, followed text);
</pre></div>


<p>and now the <strong>FOLLOWERS</strong> column family.</p>
<div class="highlight"><pre>CREATE TABLE followers(username text PRIMARY KEY, following text);
</pre></div>


<p>and now the <strong>SHOUTS</strong> column family.</p>
<div class="highlight"><pre>CREATE TABLE shouts(shout_id uuid PRIMARY KEY, username text, body text);
</pre></div>


<p>and now the <strong>USERSHOUTS</strong> column family.</p>
<div class="highlight"><pre>CREATE TABLE usershouts(username text, shout_id timeuuid, body text, PRIMARY KEY(username, shout_id));
</pre></div>


<p>and finally the <strong>SHOUTWALL</strong> column family.</p>
<div class="highlight"><pre>CREATE TABLE shoutwall(username text, shout_id timeuuid, posted_by text, body text, PRIMARY KEY(username, shout_id));
</pre></div>


<p>The columns associated with a <code>PRIMARY KEY</code> indicate the fields by which we will want to search our column families in our queries.</p>
<h2>Inserting/Selecting Data</h2>
<p>Inserting data into these column families will be done within the Node.js application but here is an example CQL statment to give you some idea.</p>
<div class="highlight"><pre>INSERT INTO users(username, password) VALUES (&#39;user1&#39;, &#39;password1&#39;);
</pre></div>


<p>Obviously this example is purely instructive and in a real-world application more security would be used with respect to storing passwords.  You can also use batch commands to insert multiple values at once.  Selecting data can also be done with the normal SQL syntax, note however that <code>WHERE</code> clauses can only be applied to indexed fields (i.e. fields that are assigned a primary key).</p>
<h1>Creating the Node.js Frontend</h1>
<p>The first step in creating a Node.js frontend that interacts with Cassandra is adding the <a href="https://github.com/datastax/nodejs-driver">Node.js Cassandra client driver</a> to the <em>package.json</em> file.  This driver will enable us to connect with the Cassandra database that we have just created.  This can be done with the following code which should be added to the <em>app.js</em> file as well as any javascript file in which a Cassandra connection is made.</p>
<div class="highlight"><pre><span class="kd">var</span> <span class="nx">client</span><span class="o">=</span><span class="k">new</span> <span class="nx">cassandra</span><span class="p">.</span><span class="nx">Client</span><span class="p">({</span><span class="nx">contactPoint</span> <span class="o">:</span> <span class="p">[</span><span class="s1">&#39;127.0.0.1&#39;</span><span class="p">]});</span>
<span class="nx">client</span><span class="p">.</span><span class="nx">connect</span><span class="p">(</span><span class="kd">function</span><span class="p">(</span><span class="nx">err</span><span class="p">,</span><span class="nx">result</span><span class="p">){</span>
    <span class="nx">consolde</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="s1">&#39;cassandra connected&#39;</span><span class="p">)</span>
<span class="p">});</span>
</pre></div>


<p>Note that in the above code we are connecting to a local instance of Cassandra.  This <code>client</code> variable is what we will be using to execute our Cassandra queries</p>
<h2>Displaying Users</h2>
<p>The most basic function that we want our app to perform is displaying all of the users.  To this end, we create a <em>users.js</em> file in the <em>routes</em> folder and add the following code.</p>
<div class="highlight"><pre><span class="kd">var</span> <span class="nx">getAllUsers</span> <span class="o">=</span> <span class="s1">&#39;SELECT * FROM shoutkeyspace.users&#39;</span><span class="p">;</span>

<span class="nx">router</span><span class="p">.</span><span class="nx">get</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">,</span> <span class="kd">function</span><span class="p">(</span><span class="nx">req</span><span class="p">,</span> <span class="nx">res</span><span class="p">)</span> <span class="p">{</span>
  <span class="nx">client</span><span class="p">.</span><span class="nx">execute</span><span class="p">(</span><span class="nx">getAllUsers</span><span class="p">,[],</span> <span class="kd">function</span><span class="p">(</span><span class="nx">err</span><span class="p">,</span> <span class="nx">result</span><span class="p">){</span>
    <span class="k">if</span><span class="p">(</span><span class="nx">err</span><span class="p">){</span>
        <span class="nx">res</span><span class="p">.</span><span class="nx">status</span><span class="p">(</span><span class="mi">404</span><span class="p">).</span><span class="nx">send</span><span class="p">({</span><span class="nx">msg</span><span class="o">:</span> <span class="nx">err</span><span class="p">});</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="nx">res</span><span class="p">.</span><span class="nx">render</span><span class="p">(</span><span class="s1">&#39;users&#39;</span><span class="p">,{</span>
            <span class="nx">users</span><span class="o">:</span> <span class="nx">result</span><span class="p">.</span><span class="nx">rows</span>
        <span class="p">});</span>
    <span class="p">}</span>
  <span class="p">});</span>
<span class="p">});</span>
</pre></div>


<p>As you can see, the query to select all rows from the <code>users</code> column family is first saved as a string variable.  Then inside the <code>router.get()</code> function, the Cassandra <code>client</code> variable that we defined eariler executes this query and if it is successfull, the corresponding <code>users</code> view is rendered with the query result.  The <code>users</code> view is a <em>jade</em> file and for the purposes of this example it is very simple.</p>
<div class="highlight"><pre>block content
    h1 Users
    ul
        each user, i in users
            li #{user.username}
</pre></div>


<p>The <code>each user, i in users</code> line loops through the query result from <em>users.js</em> and the resulting usernames are displayed in a list with <code>#{user.username}</code>.</p>
<h2>Selecting Individual Users</h2>
<p>Next we want to be able to select individual users from this list which will navigate us to a new page showing the selected user's information.  We create a new file in the <em>routes</em> folder called <em>user.js</em> and the code is very similar to what we have seen before.</p>
<div class="highlight"><pre><span class="kd">var</span> <span class="nx">getByUsername</span> <span class="o">=</span> <span class="s1">&#39;SELECT * FROM shoutkeyspace.users WHERE username = ?&#39;</span><span class="p">;</span>

<span class="nx">router</span><span class="p">.</span><span class="nx">get</span><span class="p">(</span><span class="s1">&#39;/:username&#39;</span><span class="p">,</span> <span class="kd">function</span><span class="p">(</span><span class="nx">req</span><span class="p">,</span> <span class="nx">res</span><span class="p">){</span>
    <span class="nx">client</span><span class="p">.</span><span class="nx">execute</span><span class="p">(</span><span class="nx">getByUsername</span><span class="p">,[</span><span class="nx">req</span><span class="p">.</span><span class="nx">params</span><span class="p">.</span><span class="nx">username</span><span class="p">],</span> <span class="kd">function</span><span class="p">(</span><span class="nx">err</span><span class="p">,</span> <span class="nx">result</span><span class="p">){</span>
        <span class="k">if</span><span class="p">(</span><span class="nx">err</span><span class="p">){</span>
            <span class="nx">res</span><span class="p">.</span><span class="nx">status</span><span class="p">(</span><span class="mi">404</span><span class="p">).</span><span class="nx">send</span><span class="p">({</span><span class="nx">msg</span><span class="o">:</span> <span class="nx">err</span><span class="p">});</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
            <span class="nx">res</span><span class="p">.</span><span class="nx">render</span><span class="p">(</span><span class="s1">&#39;user&#39;</span><span class="p">,{</span>
                <span class="nx">username</span><span class="o">:</span> <span class="nx">result</span><span class="p">.</span><span class="nx">rows</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nx">username</span><span class="p">,</span>
                <span class="nx">email</span><span class="o">:</span> <span class="nx">result</span><span class="p">.</span><span class="nx">rows</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nx">email</span><span class="p">,</span>
                <span class="nx">full_name</span><span class="o">:</span> <span class="nx">result</span><span class="p">.</span><span class="nx">rows</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nx">full_name</span>
            <span class="p">});</span>
        <span class="p">}</span>
    <span class="p">});</span>
<span class="p">});</span>
</pre></div>


<p>You may have noticed the strange <code>WHERE username = ?</code> in the query string.  This is because we have to be told which username we are selecting.  This variable is passed into our query by the <code>[req.params.username]</code> parameter.  If the query is executed successfully, the specified user's <code>username</code>, <code>email</code>, and <code>full_name</code> fields are sent to the <em>user</em> view.  Again, the view is quite simple.  It simply displays the fields sent to it in <em>user.js</em>.</p>
<div class="highlight"><pre>block content
    h1 User Info
    ul
        li Name:
            strong #{full_name}
        li Username:
            strong #{username}
        li Email Address
            strong #{email}
</pre></div>


<h2>Adding Users</h2>
<p>Just like before we need to add a route and a view for adding a user.  Let's call the route <em>adduser.js</em>.  This will involve both a <em>GET</em> and a <em>POST</em> HTTP request - we need to get the data from the <em>adduser</em> form and then post it to Cassandra.</p>
<div class="highlight"><pre><span class="nx">router</span><span class="p">.</span><span class="nx">get</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">,</span> <span class="kd">function</span><span class="p">(</span><span class="nx">req</span><span class="p">,</span> <span class="nx">res</span><span class="p">){</span>
    <span class="nx">res</span><span class="p">.</span><span class="nx">render</span><span class="p">(</span><span class="s1">&#39;adduser&#39;</span><span class="p">);</span>
<span class="p">});</span>

<span class="kd">var</span> <span class="nx">upsertUser</span>  <span class="o">=</span> <span class="s1">&#39;INSERT INTO shoutkeyspace.users(username, password, email, full_name) VALUES(?,?,?,?)&#39;</span><span class="p">;</span>

<span class="nx">router</span><span class="p">.</span><span class="nx">post</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">,</span> <span class="kd">function</span><span class="p">(</span><span class="nx">req</span><span class="p">,</span> <span class="nx">res</span><span class="p">){</span>
    <span class="nx">client</span><span class="p">.</span><span class="nx">execute</span><span class="p">(</span><span class="nx">upsertUser</span><span class="p">,</span> <span class="p">[</span><span class="nx">req</span><span class="p">.</span><span class="nx">body</span><span class="p">.</span><span class="nx">username</span><span class="p">,</span> <span class="nx">req</span><span class="p">.</span><span class="nx">body</span><span class="p">.</span><span class="nx">password</span><span class="p">,</span> <span class="nx">req</span><span class="p">.</span><span class="nx">body</span><span class="p">.</span><span class="nx">email</span><span class="p">,</span> <span class="nx">req</span><span class="p">.</span><span class="nx">body</span><span class="p">.</span><span class="nx">full_name</span><span class="p">],</span>
        <span class="kd">function</span><span class="p">(</span><span class="nx">err</span><span class="p">,</span> <span class="nx">result</span><span class="p">){</span>
            <span class="k">if</span><span class="p">(</span><span class="nx">err</span><span class="p">){</span>
                <span class="nx">res</span><span class="p">.</span><span class="nx">status</span><span class="p">(</span><span class="mi">404</span><span class="p">).</span><span class="nx">send</span><span class="p">({</span><span class="nx">msg</span><span class="o">:</span> <span class="nx">err</span><span class="p">});</span>
            <span class="p">}</span> <span class="k">else</span><span class="p">{</span>
                <span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="s1">&#39;User Added&#39;</span><span class="p">);</span>
                <span class="nx">res</span><span class="p">.</span><span class="nx">redirect</span><span class="p">(</span><span class="s1">&#39;/users&#39;</span><span class="p">);</span>
            <span class="p">}</span>
        <span class="p">});</span>
<span class="p">});</span>
</pre></div>


<p>So first we use use a GET request to render the <em>adduser</em> view.  This view contains the form where the required data is added (<em>username</em>, <em>password</em>, <em>email</em>, <em>full_name</em>).</p>
<div class="highlight"><pre>block content
    h1 Add User
    form#formAddUser(name=&quot;adduser&quot;, method=&quot;post&quot;, action=&quot;/adduser&quot;)
        div
            input#inputUserName(type=&quot;text&quot;, placeholder=&quot;Enter Username&quot; name=&quot;username&quot;)
        div
            input#inputPassword(type=&quot;password&quot;, placeholder=&quot;Enter Password&quot; name=&quot;password&quot;)
        div
            input#inputEmail(type=&quot;text&quot;, placeholder=&quot;Enter Email&quot; name=&quot;email&quot;)
        div
            input#inputFullName(type=&quot;text&quot;, placeholder=&quot;Enter Full Name&quot; name=&quot;full_name&quot;)
        div
            button#btnSubmit(type=&quot;submit&quot;) Submit
</pre></div>


<p>Then we define our query to insert a new user into our Cassandra <em>users</em> column family. We then execute this query with the required fields and if successfull we redirect to the <em>users</em> view which should now contain our new user.</p>
<h2>Updating Users</h2>
<p>We also want the ability for a user to update their information. To edit a user's information, we want to present the same form as when the user was intially added with the same information already in the form's fields so that modification is easy.  Again, let's make an <em>edituser</em> route and view.  The <em>edituser.js</em> route looks like the following.</p>
<div class="highlight"><pre><span class="kd">var</span> <span class="nx">getByUsername</span> <span class="o">=</span> <span class="s1">&#39;SELECT * FROM shoutkeyspace.users WHERE username = ?&#39;</span><span class="p">;</span>

<span class="nx">router</span><span class="p">.</span><span class="nx">get</span><span class="p">(</span><span class="s1">&#39;/:username&#39;</span><span class="p">,</span> <span class="kd">function</span><span class="p">(</span><span class="nx">req</span><span class="p">,</span> <span class="nx">res</span><span class="p">){</span>
    <span class="nx">client</span><span class="p">.</span><span class="nx">execute</span><span class="p">(</span><span class="nx">getByUsername</span><span class="p">,[</span><span class="nx">req</span><span class="p">.</span><span class="nx">params</span><span class="p">.</span><span class="nx">username</span><span class="p">],</span> <span class="kd">function</span><span class="p">(</span><span class="nx">err</span><span class="p">,</span><span class="nx">result</span><span class="p">){</span>
        <span class="k">if</span><span class="p">(</span><span class="nx">err</span><span class="p">){</span>
            <span class="nx">res</span><span class="p">.</span><span class="nx">status</span><span class="p">(</span><span class="mi">404</span><span class="p">).</span><span class="nx">send</span><span class="p">({</span><span class="nx">msg</span><span class="o">:</span> <span class="nx">err</span><span class="p">});</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
            <span class="nx">res</span><span class="p">.</span><span class="nx">render</span><span class="p">(</span><span class="s1">&#39;edituser&#39;</span><span class="p">,</span> <span class="p">{</span>
                <span class="nx">username</span><span class="o">:</span> <span class="nx">result</span><span class="p">.</span><span class="nx">rows</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nx">username</span><span class="p">,</span>
                <span class="nx">email</span><span class="o">:</span> <span class="nx">result</span><span class="p">.</span><span class="nx">rows</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nx">email</span><span class="p">,</span>
                <span class="nx">full_name</span><span class="o">:</span> <span class="nx">result</span><span class="p">.</span><span class="nx">rows</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nx">full_name</span><span class="p">,</span>
                <span class="nx">password</span><span class="o">:</span> <span class="nx">result</span><span class="p">.</span><span class="nx">rows</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="nx">password</span>
            <span class="p">});</span>
        <span class="p">}</span>
    <span class="p">});</span>
<span class="p">});</span>

<span class="kd">var</span> <span class="nx">upsertUser</span>  <span class="o">=</span> <span class="s1">&#39;INSERT INTO shoutkeyspace.users(username, password, email, full_name) VALUES(?,?,?,?)&#39;</span><span class="p">;</span>

<span class="nx">router</span><span class="p">.</span><span class="nx">post</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">,</span> <span class="kd">function</span><span class="p">(</span><span class="nx">req</span><span class="p">,</span> <span class="nx">res</span><span class="p">){</span>
    <span class="nx">client</span><span class="p">.</span><span class="nx">execute</span><span class="p">(</span><span class="nx">upsertUser</span><span class="p">,</span> <span class="p">[</span><span class="nx">req</span><span class="p">.</span><span class="nx">body</span><span class="p">.</span><span class="nx">username</span><span class="p">,</span> <span class="nx">req</span><span class="p">.</span><span class="nx">body</span><span class="p">.</span><span class="nx">password</span><span class="p">,</span> <span class="nx">req</span><span class="p">.</span><span class="nx">body</span><span class="p">.</span><span class="nx">email</span><span class="p">,</span> <span class="nx">req</span><span class="p">.</span><span class="nx">body</span><span class="p">.</span><span class="nx">full_name</span><span class="p">],</span>
        <span class="kd">function</span><span class="p">(</span><span class="nx">err</span><span class="p">,</span> <span class="nx">result</span><span class="p">){</span>
            <span class="k">if</span><span class="p">(</span><span class="nx">err</span><span class="p">){</span>
                <span class="nx">res</span><span class="p">.</span><span class="nx">status</span><span class="p">(</span><span class="mi">404</span><span class="p">).</span><span class="nx">send</span><span class="p">({</span><span class="nx">msg</span><span class="o">:</span> <span class="nx">err</span><span class="p">});</span>
            <span class="p">}</span> <span class="k">else</span><span class="p">{</span>
                <span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="s1">&#39;User Updated&#39;</span><span class="p">);</span>
                <span class="nx">res</span><span class="p">.</span><span class="nx">redirect</span><span class="p">(</span><span class="s1">&#39;/user/&#39;</span><span class="o">+</span> <span class="nx">req</span><span class="p">.</span><span class="nx">body</span><span class="p">.</span><span class="nx">username</span><span class="p">);</span>
            <span class="p">}</span>
        <span class="p">});</span>
<span class="p">});</span>
</pre></div>


<p>First we execute a Cassandra query to get the information needed to populate the form.  When this is executed, the <em>edituser</em> view is rendered with form information from the query result appearing in the fields.  The rest is exactly the same as <em>adduser</em>.  Furthermore, the <em>edituser</em> view is almost exactly the same as the <em>adduser.js</em> view (with <code>action=/adduser</code> replaced with <code>action=/edituser</code> and the passed in values added) so I will not show it.</p>
<h2>Deleting Users</h2>
<p>We also want to be able to delete users.  To do this we add the following code to the <em>user.js</em> route.</p>
<div class="highlight"><pre><span class="kd">var</span> <span class="nx">deleteUser</span> <span class="o">=</span> <span class="s2">&quot;DELETE FROM shoutkeyspace.users WHERE username = ?&quot;</span><span class="p">;</span>

<span class="nx">router</span><span class="p">.</span><span class="k">delete</span><span class="p">(</span><span class="s1">&#39;/:username&#39;</span><span class="p">,</span> <span class="kd">function</span><span class="p">(</span><span class="nx">req</span><span class="p">,</span> <span class="nx">res</span><span class="p">){</span>
    <span class="nx">client</span><span class="p">.</span><span class="nx">execute</span><span class="p">(</span><span class="nx">deleteUser</span><span class="p">,[</span><span class="nx">req</span><span class="p">.</span><span class="nx">params</span><span class="p">.</span><span class="nx">username</span><span class="p">],</span> <span class="kd">function</span><span class="p">(</span><span class="nx">err</span><span class="p">,</span> <span class="nx">result</span><span class="p">){</span>
        <span class="k">if</span><span class="p">(</span><span class="nx">err</span><span class="p">){</span>
            <span class="nx">res</span><span class="p">.</span><span class="nx">status</span><span class="p">(</span><span class="mi">404</span><span class="p">).</span><span class="nx">send</span><span class="p">({</span><span class="nx">msg</span><span class="o">:</span> <span class="nx">err</span><span class="p">});</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
            <span class="nx">res</span><span class="p">.</span><span class="nx">json</span><span class="p">(</span><span class="nx">result</span><span class="p">);</span>
        <span class="p">}</span>
    <span class="p">});</span>
<span class="p">});</span>
</pre></div>


<p>First we create the delete query and execute it as usual.  However, we still need a way of creating this <em>DELETE</em> request when the <em>deleteuser</em> button is clicked.  We will do this with JQuery and AJAX.  We create a <em>main.js</em> file in the <em>/public/javascripts</em> folder which contains the following JQuery code.</p>
<div class="highlight"><pre><span class="nx">$</span><span class="p">(</span><span class="nb">document</span><span class="p">).</span><span class="nx">ready</span><span class="p">(</span><span class="kd">function</span><span class="p">(){</span>
    <span class="nx">$</span><span class="p">(</span><span class="s1">&#39;.deleteuser&#39;</span><span class="p">).</span><span class="nx">on</span><span class="p">(</span><span class="s1">&#39;click&#39;</span><span class="p">,</span> <span class="nx">deleteUser</span><span class="p">);</span>
<span class="p">});</span>

<span class="kd">function</span> <span class="nx">deleteUser</span><span class="p">(){</span>
    <span class="nx">event</span><span class="p">.</span><span class="nx">preventDefault</span><span class="p">();</span>

    <span class="kd">var</span> <span class="nx">confirmation</span> <span class="o">=</span> <span class="nx">confirm</span><span class="p">(</span><span class="s1">&#39;Are you sure that you want to delete this user?&#39;</span><span class="p">);</span>

    <span class="k">if</span><span class="p">(</span><span class="nx">confirmation</span><span class="p">){</span>
        <span class="nx">$</span><span class="p">.</span><span class="nx">ajax</span><span class="p">({</span>
            <span class="nx">type</span><span class="o">:</span> <span class="s1">&#39;DELETE&#39;</span><span class="p">,</span>
            <span class="nx">url</span><span class="o">:</span> <span class="s1">&#39;/user/&#39;</span><span class="o">+</span> <span class="nx">$</span><span class="p">(</span><span class="s1">&#39;.deleteuser&#39;</span><span class="p">).</span><span class="nx">data</span><span class="p">(</span><span class="s1">&#39;user&#39;</span><span class="p">)</span>
        <span class="p">}).</span><span class="nx">done</span><span class="p">(</span><span class="kd">function</span><span class="p">(</span><span class="nx">response</span><span class="p">){</span>
            <span class="nb">window</span><span class="p">.</span><span class="nx">location</span><span class="p">.</span><span class="nx">replace</span><span class="p">(</span><span class="s1">&#39;/users&#39;</span><span class="p">);</span>
        <span class="p">});</span>
    <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
        <span class="k">return</span> <span class="kc">false</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>


<p>So when the <em>deleteuser</em> button is clicked, the <em>deleteUser()</em> function is called which uses AJAX to send the <em>DELETE</em> request to the correct URL.</p>
<h2>Displaying Shouts</h2>
<p>Now let's turn our attention to displaying shouts.  Again we must create a <em>shouts</em> route and view.  Our <em>shouts.js</em> route should look like the following.</p>
<div class="highlight"><pre><span class="kd">var</span> <span class="nx">getAllShouts</span> <span class="o">=</span> <span class="s1">&#39;SELECT * FROM shoutkeyspace.shouts&#39;</span><span class="p">;</span>

<span class="nx">router</span><span class="p">.</span><span class="nx">get</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">,</span> <span class="kd">function</span><span class="p">(</span><span class="nx">req</span><span class="p">,</span> <span class="nx">res</span><span class="p">){</span>
    <span class="nx">client</span><span class="p">.</span><span class="nx">execute</span><span class="p">(</span><span class="nx">getAllShouts</span><span class="p">,[],</span> <span class="kd">function</span><span class="p">(</span><span class="nx">err</span><span class="p">,</span> <span class="nx">result</span><span class="p">){</span>
        <span class="k">if</span><span class="p">(</span><span class="nx">err</span><span class="p">){</span>
            <span class="nx">res</span><span class="p">.</span><span class="nx">status</span><span class="p">(</span><span class="mi">404</span><span class="p">).</span><span class="nx">send</span><span class="p">({</span><span class="nx">msg</span><span class="o">:</span> <span class="nx">err</span><span class="p">});</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
            <span class="nx">res</span><span class="p">.</span><span class="nx">render</span><span class="p">(</span><span class="s1">&#39;shouts&#39;</span><span class="p">,{</span>
                <span class="nx">shouts</span><span class="o">:</span> <span class="nx">result</span><span class="p">.</span><span class="nx">rows</span>
            <span class="p">});</span>
        <span class="p">}</span>
    <span class="p">});</span>
<span class="p">});</span>

<span class="kd">var</span> <span class="nx">getUserShouts</span> <span class="o">=</span> <span class="s1">&#39;SELECT * FROM shoutkeyspace.usershouts WHERE username = ?&#39;</span><span class="p">;</span>

<span class="nx">router</span><span class="p">.</span><span class="nx">get</span><span class="p">(</span><span class="s1">&#39;/:username&#39;</span><span class="p">,</span> <span class="kd">function</span><span class="p">(</span><span class="nx">req</span><span class="p">,</span> <span class="nx">res</span><span class="p">){</span>
    <span class="nx">client</span><span class="p">.</span><span class="nx">execute</span><span class="p">(</span><span class="nx">getUserShouts</span><span class="p">,</span> <span class="p">[</span><span class="nx">req</span><span class="p">.</span><span class="nx">params</span><span class="p">.</span><span class="nx">username</span><span class="p">],</span> <span class="kd">function</span><span class="p">(</span><span class="nx">err</span><span class="p">,</span> <span class="nx">result</span><span class="p">){</span>
        <span class="k">if</span><span class="p">(</span><span class="nx">err</span><span class="p">){</span>
            <span class="nx">res</span><span class="p">.</span><span class="nx">status</span><span class="p">(</span><span class="mi">404</span><span class="p">).</span><span class="nx">send</span><span class="p">({</span><span class="nx">msg</span><span class="o">:</span> <span class="nx">err</span><span class="p">});</span>
        <span class="p">}</span> <span class="k">else</span> <span class="p">{</span>
            <span class="nx">res</span><span class="p">.</span><span class="nx">render</span><span class="p">(</span><span class="s1">&#39;shouts&#39;</span><span class="p">,{</span>
                <span class="nx">shouts</span><span class="o">:</span> <span class="nx">result</span><span class="p">.</span><span class="nx">rows</span>
            <span class="p">});</span>
        <span class="p">}</span>
    <span class="p">});</span>
<span class="p">});</span>
</pre></div>


<p>By now, this should be quite straight-forward.  The first <em>GET</em> request displays all shouts to the <em>shouts</em> view from the Cassandra query and the second <em>GET</em> request displays a specified user's shouts to the <em>shouts</em> view from another Cassandra query.</p>
<h2>Adding Shouts</h2>
<p>The final piece of the puzzle is enabling users to add shouts.  On the view side, this can be done with the following form.</p>
<div class="highlight"><pre>form#formAddShout(name=&quot;addshout&quot;, method=&quot;post&quot;, action=&quot;/addshout&quot;)
        div
            select(name=&quot;username&quot;)
                option(value=&quot;devuser1&quot;) devuser1
                option(value=&quot;devuser2&quot;) devuser2
                option(value=&quot;devuser3&quot;) devuser3
                option(value=&quot;devuser4&quot;) devuser4
        br
        div
            textarea(name=&quot;body&quot;, placeholder=&quot;Shout Something!&quot;, cols=&quot;50&quot;, rows=&quot;5&quot;)
        br
        div
            button#btnSubmit(type=&quot;submit&quot;) Submit
</pre></div>


<p>This form contains a textarea in which the user can write their shout as well as the ability to post it as a specific user (ideally you would have a login system but this is beyond the scope of this app).  Next we create the route as the following.</p>
<div class="highlight"><pre><span class="nx">router</span><span class="p">.</span><span class="nx">post</span><span class="p">(</span><span class="s1">&#39;/&#39;</span><span class="p">,</span> <span class="kd">function</span><span class="p">(</span><span class="nx">req</span><span class="p">,</span> <span class="nx">res</span><span class="p">){</span>
    <span class="kd">var</span> <span class="nx">id1</span> <span class="o">=</span> <span class="nx">cassandra</span><span class="p">.</span><span class="nx">types</span><span class="p">.</span><span class="nx">uuid</span><span class="p">();</span>
    <span class="kd">var</span> <span class="nx">id2</span> <span class="o">=</span> <span class="nx">cassandra</span><span class="p">.</span><span class="nx">types</span><span class="p">.</span><span class="nx">timeuuid</span><span class="p">();</span>

    <span class="kd">var</span> <span class="nx">queries</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">{</span>
            <span class="nx">query</span><span class="o">:</span> <span class="s1">&#39;INSERT INTO shoutkeyspace.shouts(shout_id, username, body) VALUES(?,?,?)&#39;</span><span class="p">,</span>
            <span class="nx">params</span><span class="o">:</span><span class="p">[</span><span class="nx">id1</span><span class="p">,</span> <span class="nx">req</span><span class="p">.</span><span class="nx">body</span><span class="p">.</span><span class="nx">username</span><span class="p">,</span> <span class="nx">req</span><span class="p">.</span><span class="nx">body</span><span class="p">.</span><span class="nx">body</span><span class="p">]</span>
        <span class="p">},</span>
        <span class="p">{</span>
            <span class="nx">query</span><span class="o">:</span> <span class="s1">&#39;INSERT INTO shoutkeyspace.usershouts(username, shout_id, body) VALUES(?,?,?)&#39;</span><span class="p">,</span>
            <span class="nx">params</span><span class="o">:</span> <span class="p">[</span><span class="nx">req</span><span class="p">.</span><span class="nx">body</span><span class="p">.</span><span class="nx">username</span><span class="p">,</span> <span class="nx">id2</span><span class="p">,</span> <span class="nx">req</span><span class="p">.</span><span class="nx">body</span><span class="p">.</span><span class="nx">body</span><span class="p">]</span>
        <span class="p">}</span>
    <span class="p">];</span>
    <span class="nx">queryOptions</span> <span class="o">=</span> <span class="p">{};</span>
    <span class="nx">client</span><span class="p">.</span><span class="nx">batch</span><span class="p">(</span><span class="nx">queries</span><span class="p">,</span> <span class="nx">queryOptions</span><span class="p">,</span> <span class="kd">function</span><span class="p">(</span><span class="nx">err</span><span class="p">){</span>
        <span class="nx">console</span><span class="p">.</span><span class="nx">log</span><span class="p">(</span><span class="nx">err</span><span class="p">);</span>
        <span class="nx">res</span><span class="p">.</span><span class="nx">redirect</span><span class="p">(</span><span class="s1">&#39;/shouts&#39;</span><span class="p">);</span>
    <span class="p">});</span>
<span class="p">});</span>
</pre></div>


<p>Here we use a Cassandra batch query to insert the added shout into both the <code>shouts</code> and <code>usershouts</code> keyspaces.</p>
<p>And that covers the basic functionality of our simple shoutbox app.  In summary, we have given a brief overview of Cassandra, added our keyspaces and column families for our app as well a built a Node.js frontend to connect with our Cassandra database.</p>
<h2>References</h2>
<ul>
<li><a href="https://github.com/alexminnaar/shoutApp">Github repo</a> for the code used in this blog post.</li>
<li><a href="http://www.datastax.com/">Datastax</a> where you can download the free Cassandra community edition as well as learn more about Cassandra.</li>
<li><a href="http://nodejs.org/">nodejs.org</a> where you can download and learn more about Node.js. </li>
</ul>
                    </article>
 
<div class="paginator">
    <div class="navButton">Page 1 / 4</div>
        <div class="navButton"><a href="/index2.html" >Next</a></div>
</div>
                </div>
            </aside><!-- /#featured -->
            
        
        <section id="extras" >
       
        
        </section><!-- /#extras -->
	
        <footer id="contentinfo" >
                <address id="about" class="vcard ">
                Proudly powered by <a href="http://getpelican.com/" target="_blank">Pelican</a>, which takes
                great advantage of <a href="http://python.org" target="_blank">Python</a>. &copy; <a class="url fn" href="http://launchyard.com">LaunchYard</a>
		
                </address><!-- /#about -->
		

                
        </footer><!-- /#contentinfo -->

<script type="text/javascript">
    var disqus_shortname = 'alexminnaar';
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = 'http://' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
</body>
</html>